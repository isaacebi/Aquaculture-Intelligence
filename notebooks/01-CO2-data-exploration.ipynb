{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.metrics import classification_report, accuracy_score, f1_score, recall_score, precision_score, ConfusionMatrixDisplay, make_scorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attaching project directory\n",
    "sys.path.append(os.path.dirname(os.getcwd()))\n",
    "\n",
    "# Pathing imports\n",
    "from src import GetPath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FOLDER = GetPath().shared_data()\n",
    "ABN_B1 = os.path.join(DATA_FOLDER, 'raw', 'abnormal_b1.csv')\n",
    "ABN_B2 = os.path.join(DATA_FOLDER, 'raw', 'abnormal_b2.csv')\n",
    "ABN_B3 = os.path.join(DATA_FOLDER, 'raw', 'abnormal_b3.csv')\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "SAMPLE_SIZE = 50\n",
    "\n",
    "np.random.seed(seed=RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(ABN_B1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking abnormalities throughout experiments\n",
    "for i in range(1, 4):\n",
    "    # Experiment batch\n",
    "    experiment = f\"ABN_B{i}\"\n",
    "    # Load csv data\n",
    "    df = pd.read_csv(eval(experiment))\n",
    "\n",
    "    # Plotting countplot\n",
    "    sns.countplot(df, x='ABN')\n",
    "    plt.title(f\"Experiment {experiment}\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = pd.DataFrame()\n",
    "\n",
    "for idx in range(3):\n",
    "    # Experiment batch\n",
    "    experiment = f\"ABN_B{idx+1}\"\n",
    "    # Load csv data\n",
    "    df_all = pd.concat([df_all, pd.read_csv(eval(experiment))]).reset_index(drop=True)\n",
    "\n",
    "# Plotting countplot\n",
    "sns.countplot(df_all, x='ABN')\n",
    "plt.title(f\"Abnormal Level Count\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relationship between Hovering and Abnormal Level\n",
    "\n",
    "- Hovering BTM and Abnormal Level (down trend)\n",
    "- Hovering SUR and Abnormal Level (up trend)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# By batch of experiment\n",
    "# Position state\n",
    "positions = ['BTM', 'MID', 'SUR']\n",
    "\n",
    "for position in positions:\n",
    "    # Plotting hovering\n",
    "    fig, axes = plt.subplots(figsize=(14,6), nrows=1, ncols=3)\n",
    "    for idx, ax in enumerate(axes.flatten()):\n",
    "        # Experiment batch\n",
    "        experiment = f\"ABN_B{idx+1}\"\n",
    "        # Load csv data\n",
    "        df = pd.read_csv(eval(experiment))\n",
    "\n",
    "        sns.barplot(df, x='ABN', y=f\"Hovering {position}\", ax=ax)\n",
    "        ax.set_title(f\"Experiment {experiment}\")\n",
    "\n",
    "    fig.suptitle(f\"Relationship Between Hovering {position} and Abnormal Level\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All experiment\n",
    "# Position state\n",
    "positions = ['BTM', 'MID', 'SUR']\n",
    "\n",
    "fig, axes = plt.subplots(figsize=(14,6), nrows=1, ncols=len(positions))\n",
    "for idx, ax in enumerate(axes.flatten()):\n",
    "    sns.barplot(df_all, x='ABN', y=f\"Hovering {positions[idx]}\", ax=ax)\n",
    "\n",
    "fig.suptitle(f\"Relationship Between Hovering Position and Abnormal Level\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relationship between Fast Swim and Abnormal Level\n",
    "- Fast Swim MID and Abnormal Level (up trend)\n",
    "- Fast Swim SUR and Abnormal Level (up trend)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Position state\n",
    "positions = ['Btm', 'Mid', 'SUR']\n",
    "\n",
    "for position in positions:\n",
    "    # Plotting hovering\n",
    "    fig, axes = plt.subplots(figsize=(14,6), nrows=1, ncols=3)\n",
    "    for idx, ax in enumerate(axes.flatten()):\n",
    "        # Experiment batch\n",
    "        experiment = f\"ABN_B{idx+1}\"\n",
    "        # Load csv data\n",
    "        df = pd.read_csv(eval(experiment))\n",
    "\n",
    "        sns.barplot(df, x='ABN', y=f\"Fast Swim {position}\", ax=ax)\n",
    "        ax.set_title(f\"Experiment {experiment}\")\n",
    "\n",
    "    fig.suptitle(f\"Relationship Between Fast Swim {position} and Abnormal Level\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All experiment\n",
    "# Position state\n",
    "positions = ['Btm', 'Mid', 'SUR']\n",
    "\n",
    "fig, axes = plt.subplots(figsize=(14,6), nrows=1, ncols=len(positions))\n",
    "for idx, ax in enumerate(axes.flatten()):\n",
    "    sns.barplot(df_all, x='ABN', y=f\"Fast Swim {positions[idx]}\", ax=ax)\n",
    "\n",
    "fig.suptitle(f\"Relationship Between Fast Swim Position and Abnormal Level\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relationship beween Burst Swimming and Abnormal Level\n",
    "\n",
    "- Burst Swimming and Abnormal Level (up trend)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting Burst Swimming\n",
    "fig, axes = plt.subplots(figsize=(14,6), nrows=1, ncols=3)\n",
    "for idx, ax in enumerate(axes.flatten()):\n",
    "    # Experiment batch\n",
    "    experiment = f\"ABN_B{idx+1}\"\n",
    "    # Load csv data\n",
    "    df = pd.read_csv(eval(experiment))\n",
    "\n",
    "    sns.barplot(df, x='ABN', y='Burst Swimming', ax=ax)\n",
    "    ax.set_title(f\"Experiment {experiment}\")\n",
    "\n",
    "fig.suptitle(f\"Relationship Between Burst Swimming and Abnormal Level\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All experiment\n",
    "sns.barplot(df_all, x='ABN', y='Burst Swimming')\n",
    "plt.title(\"Relationship Between Burst Swimming and Abnormal Level\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relationship beween Acc. Ver. Position and Abnormal Level\n",
    "\n",
    "- Acc. Ver. MID and Abnormal Level (up trend)\n",
    "- Acc. Ver. SUR and Abnormal Level (up trend)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Postion state\n",
    "positions = ['MID', 'SUR']\n",
    "\n",
    "for position in positions:\n",
    "    # Plotting hovering\n",
    "    fig, axes = plt.subplots(figsize=(14,6), nrows=1, ncols=3)\n",
    "    for idx, ax in enumerate(axes.flatten()):\n",
    "        # Experiment batch\n",
    "        experiment = f\"ABN_B{idx+1}\"\n",
    "        # Load csv data\n",
    "        df = pd.read_csv(eval(experiment))\n",
    "\n",
    "        sns.barplot(df, x=\"ABN\", y=f\"Acc. Ver. {position}\", ax=ax)\n",
    "        ax.set_title(f\"Experiment {experiment}\")\n",
    "\n",
    "    fig.suptitle(f\"Relationship between Acc. Ver. {position} with Abnormal Level\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All experiment\n",
    "# Position state\n",
    "positions = ['MID', 'SUR']\n",
    "\n",
    "fig, axes = plt.subplots(figsize=(14,6), nrows=1, ncols=len(positions))\n",
    "for idx, ax in enumerate(axes.flatten()):\n",
    "    sns.barplot(df_all, x=\"ABN\", y=f\"Acc. Ver. {position}\", ax=ax)\n",
    "\n",
    "fig.suptitle(f\"Relationship Between Acc. Ver. Position and Abnormal Level\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relationship beween Turning and Abnormal Level\n",
    "- Turning and Abnormal Level (down trend)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting Burst Swimming\n",
    "fig, axes = plt.subplots(figsize=(14,6), nrows=1, ncols=3)\n",
    "for idx, ax in enumerate(axes.flatten()):\n",
    "    # Experiment batch\n",
    "    experiment = f\"ABN_B{idx+1}\"\n",
    "    # Load csv data\n",
    "    df = pd.read_csv(eval(experiment))\n",
    "\n",
    "    sns.barplot(df, x='ABN', y='Turning', ax=ax)\n",
    "    ax.set_title(f\"Experiment {experiment}\")\n",
    "\n",
    "fig.suptitle(f\"Relationship Between Turning and Abnormal Level\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All experiment\n",
    "sns.barplot(df_all, x='ABN', y='Turning')\n",
    "plt.title(\"Relationship between Turning and Abnormal Level\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relationship beween Agg. Behavior and Abnormal Level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting Burst Swimming\n",
    "fig, axes = plt.subplots(figsize=(14,6), nrows=1, ncols=3)\n",
    "for idx, ax in enumerate(axes.flatten()):\n",
    "    # Experiment batch\n",
    "    experiment = f\"ABN_B{idx+1}\"\n",
    "    # Load csv data\n",
    "    df = pd.read_csv(eval(experiment))\n",
    "\n",
    "    sns.barplot(df, x='ABN', y='Agg. Behaviour', ax=ax)\n",
    "    ax.set_title(f\"Experiment {experiment}\")\n",
    "\n",
    "fig.suptitle(f\"Relationship Between Agg. Behaviour and Abnormal Level\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All experiment \n",
    "sns.barplot(df_all, x='ABN', y='Agg. Behaviour')\n",
    "plt.title(f\"Relationship Between Agg. Behaviour and Abnormal Level\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relationship beween Resting and Abnormal Level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting Burst Swimming\n",
    "fig, axes = plt.subplots(figsize=(14,6), nrows=1, ncols=3)\n",
    "for idx, ax in enumerate(axes.flatten()):\n",
    "    # Experiment batch\n",
    "    experiment = f\"ABN_B{idx+1}\"\n",
    "    # Load csv data\n",
    "    df = pd.read_csv(eval(experiment))\n",
    "\n",
    "    sns.barplot(df, x='ABN', y='Resting', ax=ax)\n",
    "    ax.set_title(f\"Experiment {experiment}\")\n",
    "\n",
    "fig.suptitle(f\"Relationship Between Resting and Abnormal Level\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All experiment\n",
    "sns.barplot(df_all, x='ABN', y='Resting')\n",
    "plt.title(\"Relationship between Resting and Abnormal Level\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relationship beween Active and Abnormal Level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting Burst Swimming\n",
    "fig, axes = plt.subplots(figsize=(14,6), nrows=1, ncols=3)\n",
    "for idx, ax in enumerate(axes.flatten()):\n",
    "    # Experiment batch\n",
    "    experiment = f\"ABN_B{idx+1}\"\n",
    "    # Load csv data\n",
    "    df = pd.read_csv(eval(experiment))\n",
    "\n",
    "    sns.barplot(df, x='ABN', y='Active', ax=ax)\n",
    "    ax.set_title(f\"Experiment {experiment}\")\n",
    "\n",
    "fig.suptitle(f\"Relationship Between Active and Abnormal Level\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All experiment\n",
    "sns.barplot(df_all, x='ABN', y='Active')\n",
    "plt.title(\"Relationship between Active and Abnormal Level\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop nan value\n",
    "df = df.dropna()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only use relevent feature\n",
    "df = df.iloc[:, 3:]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Describe the data\n",
    "df.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on my understanding, all data are in continous\n",
    "X = df.drop(columns=['N', 'ABN', 'ABN%'])\n",
    "y = df['ABN']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection On Specific Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "for i in range(3):\n",
    "    # Experiment batch\n",
    "    experiment = f\"ABN_B{i+1}\"\n",
    "    # Load csv data\n",
    "    df = pd.read_csv(eval(experiment))\n",
    "\n",
    "    # Feature and label\n",
    "    df = df.iloc[:, 3:].dropna()\n",
    "\n",
    "    # Resample as some level only has 1\n",
    "    df = df.groupby('ABN').apply(\n",
    "        lambda x: x.sample(50, replace=True, random_state=RANDOM_STATE)\n",
    "    ).reset_index(drop=True)\n",
    "\n",
    "    # Split to feature and label\n",
    "    X = df.drop(columns=['N', 'ABN', 'ABN%'])\n",
    "    y = df['ABN']\n",
    "\n",
    "    # Split the data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=RANDOM_STATE, stratify=y)\n",
    "\n",
    "    # Iterate using from one feature to all feature\n",
    "    for i in range(len(X.columns)):\n",
    "        # Create the pipeline\n",
    "        pipeline = Pipeline([\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('feature_selection', SelectKBest(score_func=f_classif, k=i+1)),\n",
    "            ('classifier', LogisticRegression(max_iter=1000, random_state=RANDOM_STATE))\n",
    "        ])\n",
    "        \n",
    "        # Fit the pipeline\n",
    "        pipeline.fit(X_train, y_train)\n",
    "\n",
    "        # Make predictions\n",
    "        y_pred = pipeline.predict(X_test)\n",
    "\n",
    "        result = {\n",
    "            'data_batch': experiment,\n",
    "            'number_feature': i+1,\n",
    "            'accuracy': accuracy_score(y_test, y_pred),\n",
    "            'precision_weighted': precision_score(y_test, y_pred, average='weighted', zero_division=True), \n",
    "            'recall_weighted': recall_score(y_test, y_pred, average='weighted', zero_division=True),\n",
    "            'f1_weighted': f1_score(y_test, y_pred, average='weighted', zero_division=True)\n",
    "        }\n",
    "        \n",
    "        results.append(result)\n",
    "        \n",
    "        # Message\n",
    "        # print(f\"\\n--- Inspecting Experiment Batch {experiment} ---\")\n",
    "        # print(f\"--- Results --- \\n--- Using {i+1} feature ---\")\n",
    "        # print(classification_report(y_test, y_pred, target_names=np.unique(y_train.astype(str)), zero_division=True))\n",
    "\n",
    "results = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.sort_values('accuracy', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(3):\n",
    "    # Experiment batch\n",
    "    experiment = f\"ABN_B{i+1}\"\n",
    "    # Load csv data\n",
    "    df = pd.read_csv(eval(experiment))\n",
    "\n",
    "    # Feature and label\n",
    "    df = df.iloc[:, 3:].dropna()\n",
    "\n",
    "    # Resample as some level only has 1\n",
    "    df = df.groupby('ABN').apply(\n",
    "        lambda x: x.sample(50, replace=True, random_state=RANDOM_STATE)\n",
    "    ).reset_index(drop=True)\n",
    "\n",
    "    # Split to feature and label\n",
    "    X = df.drop(columns=['N', 'ABN', 'ABN%'])\n",
    "    y = df['ABN']\n",
    "\n",
    "    # Split the data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=RANDOM_STATE, stratify=y)\n",
    "\n",
    "    # Initializing and training the Random Forest Classifier\n",
    "    model = RandomForestClassifier(random_state=RANDOM_STATE)\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Getting feature importance\n",
    "    importances = model.feature_importances_\n",
    "    indices = np.argsort(importances)[::-1]\n",
    "\n",
    "    # Plotting the feature importances\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.barplot(x=importances[indices], y=np.array(X.columns)[indices], hue=np.array(X.columns)[indices], palette='viridis')\n",
    "    plt.title(f'Feature Importance {experiment}')\n",
    "    plt.xlabel('Importance Score')\n",
    "    plt.ylabel('Features')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment batch\n",
    "experiment = results.loc[results['accuracy'].idxmax()]['data_batch']\n",
    "\n",
    "# Number feature\n",
    "n_feature = results.sort_values('accuracy', ascending=False)['number_feature'][0]\n",
    "\n",
    "# Load csv data\n",
    "df = pd.read_csv(eval(experiment))\n",
    "\n",
    "# Message\n",
    "print(f\"\\n--- Inspecting Experiment Batch {experiment} ---\")\n",
    "\n",
    "# Feature and label\n",
    "df = df.iloc[:, 3:].dropna()\n",
    "\n",
    "# Resample as some level only has 1\n",
    "df = df.groupby('ABN').apply(\n",
    "    lambda x: x.sample(50, replace=True, random_state=RANDOM_STATE)\n",
    ").reset_index(drop=True)\n",
    "\n",
    "# Split to feature and label\n",
    "X = df.drop(columns=['N', 'ABN', 'ABN%'])\n",
    "y = df['ABN']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=RANDOM_STATE, stratify=y)\n",
    "\n",
    "# Create the pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('feature_selection', SelectKBest(score_func=f_classif, k=n_feature)),\n",
    "    ('classifier', LogisticRegression(max_iter=1000, random_state=RANDOM_STATE))\n",
    "])\n",
    "\n",
    "# Fit the pipeline\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "print(f\"--- Results ---\")\n",
    "print(classification_report(y_test, y_pred, target_names=np.unique(y_train.astype(str)), zero_division=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since we are training with B3, lets test with B1 and B2\n",
    "X12 = pd.DataFrame()\n",
    "y12 = pd.DataFrame()\n",
    "\n",
    "for i in range(2):\n",
    "    # Experiment batch\n",
    "    experiment = f\"ABN_B{i+1}\"\n",
    "    # Load csv data\n",
    "    df = pd.read_csv(eval(experiment))\n",
    "\n",
    "    # Feature and label\n",
    "    df = df.iloc[:, 3:].dropna()\n",
    "\n",
    "    # Split to feature and label\n",
    "    X = df.drop(columns=['N', 'ABN', 'ABN%'])\n",
    "    y = df['ABN']\n",
    "\n",
    "    X12 = pd.concat([X12, X], axis=0)\n",
    "    y12 = pd.concat([y12, y], axis=0)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = pipeline.predict(X12)\n",
    "\n",
    "print(f\"--- Results ---\")\n",
    "print(classification_report(y12, y_pred, target_names=np.unique(y_train.astype(str)), zero_division=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "for i in [2, 3]:\n",
    "    # Load batch 1\n",
    "    df = pd.read_csv(ABN_B1)\n",
    "\n",
    "    # Experiment batch\n",
    "    experiment = f\"ABN_B{i}\"\n",
    "\n",
    "    # Load csv data\n",
    "    df = pd.concat([df, pd.read_csv(eval(experiment))], axis=0)\n",
    "\n",
    "    # Feature and label\n",
    "    df = df.iloc[:, 3:].dropna()\n",
    "\n",
    "    # Resample as some level only has 1\n",
    "    df = df.groupby('ABN').apply(\n",
    "        lambda x: x.sample(50, replace=True, random_state=RANDOM_STATE)\n",
    "    ).reset_index(drop=True)\n",
    "\n",
    "    # Split to feature and label\n",
    "    X = df.drop(columns=['N', 'ABN', 'ABN%'])\n",
    "    y = df['ABN']\n",
    "\n",
    "    # Split the data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=RANDOM_STATE, stratify=y)\n",
    "\n",
    "    # Iterate using from one feature to all feature\n",
    "    for i in range(len(X.columns)):\n",
    "        # Create the pipeline\n",
    "        pipeline = Pipeline([\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('feature_selection', SelectKBest(score_func=f_classif, k=i+1)),\n",
    "            ('classifier', LogisticRegression(max_iter=1000, random_state=RANDOM_STATE))\n",
    "        ])\n",
    "        \n",
    "        # Fit the pipeline\n",
    "        pipeline.fit(X_train, y_train)\n",
    "\n",
    "        sel_feat = pipeline.named_steps['feature_selection'].get_feature_names_out(\n",
    "            input_features=X.columns\n",
    "        )\n",
    "\n",
    "        # Make predictions\n",
    "        y_pred = pipeline.predict(X_test)\n",
    "\n",
    "        result = {\n",
    "            'data_batch': experiment,\n",
    "            'number_feature': i+1,\n",
    "            'accuracy': accuracy_score(y_test, y_pred),\n",
    "            'precision_weighted': precision_score(y_test, y_pred, average='weighted', zero_division=True), \n",
    "            'recall_weighted': recall_score(y_test, y_pred, average='weighted', zero_division=True),\n",
    "            'f1_weighted': f1_score(y_test, y_pred, average='weighted', zero_division=True)\n",
    "        }\n",
    "        \n",
    "        results.append(result)\n",
    "\n",
    "        # Message\n",
    "        print(f\"\\n--- Inspecting Experiment Batch {experiment}1 ---\")\n",
    "        print(f\"Selected feature: {sel_feat}\")\n",
    "        print(f\"--- Results --- \\n--- Using {i+1} feature ---\")\n",
    "        print(classification_report(y_test, y_pred, target_names=np.unique(y_train.astype(str)), zero_division=True))\n",
    "\n",
    "results = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.sort_values('accuracy', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.concat([\n",
    "    pd.read_csv(ABN_B1).dropna(),\n",
    "    pd.read_csv(ABN_B3).dropna()\n",
    "]).reset_index(drop=True)\n",
    "\n",
    "feature_selected = ['Hovering BTM', 'Fast Swim SUR', 'Acc. Ver. SUR', 'Turning', 'Burst Swimming']\n",
    "\n",
    "X = df_train[feature_selected]\n",
    "y = df_train['ABN']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=RANDOM_STATE, stratify=y)\n",
    "\n",
    "models = [\n",
    "    LogisticRegression(max_iter=1000, random_state=RANDOM_STATE),\n",
    "    DecisionTreeClassifier(random_state=RANDOM_STATE),\n",
    "    RandomForestClassifier(random_state=RANDOM_STATE),\n",
    "    SVC(random_state=RANDOM_STATE)\n",
    "]\n",
    "\n",
    "for model in models:\n",
    "    pipeline = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('classifier', model)\n",
    "    ])\n",
    "\n",
    "    # Fit the pipeline\n",
    "    pipeline.fit(X_train, y_train)\n",
    "\n",
    "    # Make predictions\n",
    "    y_pred = pipeline.predict(X_test)\n",
    "\n",
    "    print(f\"\\n--- Results {model} ---\")\n",
    "    print(classification_report(y_test, y_pred, target_names=np.unique(y_train.astype(str)), zero_division=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.arange(2, 20, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom scoring function to prioritize recall for specific classes\n",
    "def custom_recall_score(y_true, y_pred):\n",
    "    recalls = recall_score(y_true, y_pred, average=None)\n",
    "    # return recall for class 10\n",
    "    return np.mean(recalls[2])\n",
    "\n",
    "# Create a custom scorer\n",
    "custom_scorer = make_scorer(custom_recall_score)\n",
    "\n",
    "# Define the parameter distribution for random search\n",
    "param_dist = {\n",
    "    'n_estimators': np.arange(100, 500, 100),\n",
    "    'min_samples_split': np.arange(2, 20),\n",
    "    'min_samples_leaf': np.arange(1, 20),\n",
    "    'max_features': np.random.uniform(0.1, 0.9, 20),  # Continuous uniform distribution\n",
    "    'bootstrap': [True, False],\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "}\n",
    "\n",
    "# Create a random forest classifier\n",
    "rf = RandomForestClassifier(random_state=RANDOM_STATE)\n",
    "\n",
    "# Create the random search object\n",
    "random_search = GridSearchCV(\n",
    "    estimator=rf,\n",
    "    param_grid=param_dist,\n",
    "    cv=2,\n",
    "    verbose=2,\n",
    "    n_jobs=-1,\n",
    "    scoring=custom_scorer\n",
    ")\n",
    "\n",
    "# Fit the random search object to the data\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "# Print the best parameters and score\n",
    "print(\"Best parameters:\", random_search.best_params_)\n",
    "print(\"Best cross-validation score:\", random_search.best_score_)\n",
    "\n",
    "# Get the best model\n",
    "best_rf = random_search.best_estimator_\n",
    "\n",
    "# Evaluate on the test set\n",
    "test_score = best_rf.score(X_test, y_test)\n",
    "print(\"Test set score:\", test_score)\n",
    "\n",
    "recall_10_score = custom_recall_score(y_test, best_rf.predict(X_test))\n",
    "print(f\"Recall 10 score: {recall_10_score}\")\n",
    "\n",
    "# Feature importance\n",
    "feature_importance = best_rf.feature_importances_\n",
    "sorted_idx = np.argsort(feature_importance)\n",
    "pos = np.arange(sorted_idx.shape[0]) + .5\n",
    "\n",
    "print(\"\\nTop 5 most important features:\")\n",
    "for i in sorted_idx[-5:]:\n",
    "    print(f\"Feature {i}: {feature_importance[i]:.4f}\")\n",
    "\n",
    "# Calculate mean and standard deviation of trees in the forest\n",
    "n_trees = best_rf.n_estimators\n",
    "depths = np.zeros(n_trees)\n",
    "\n",
    "for i, tree in enumerate(best_rf.estimators_):\n",
    "    depths[i] = tree.tree_.max_depth\n",
    "\n",
    "print(f\"\\nMean tree depth: {np.mean(depths):.2f}\")\n",
    "print(f\"Standard deviation of tree depths: {np.std(depths):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('classifier', best_rf)\n",
    "])\n",
    "\n",
    "# Fit the pipeline\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "print(f\"\\n--- Results {model} ---\")\n",
    "print(classification_report(y_test, y_pred, target_names=np.unique(y_train.astype(str)), zero_division=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read batch 1\n",
    "df = pd.read_csv(ABN_B1)\n",
    "\n",
    "# read combination experiment\n",
    "experiment = results.sort_values('accuracy', ascending=False)['data_batch'][0]\n",
    "dfc = pd.read_csv(eval(experiment))\n",
    "\n",
    "# combine\n",
    "df = pd.concat([df, dfc])\n",
    "\n",
    "# Feature and label\n",
    "df = df.iloc[:, 3:].dropna()\n",
    "\n",
    "# Resample as some level only has 1\n",
    "df = df.groupby('ABN').apply(\n",
    "    lambda x: x.sample(50, replace=True, random_state=RANDOM_STATE)\n",
    ").reset_index(drop=True)\n",
    "\n",
    "# Split to feature and label\n",
    "X = df.drop(columns=['N', 'ABN', 'ABN%'])\n",
    "y = df['ABN']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=RANDOM_STATE, stratify=y)\n",
    "\n",
    "# Create the pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('feature_selection', SelectKBest(score_func=f_classif, k=15)),\n",
    "    ('classifier', LogisticRegression(max_iter=1000, random_state=RANDOM_STATE))\n",
    "])\n",
    "\n",
    "# Fit the pipeline\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "print(f\"--- Results ---\")\n",
    "print(classification_report(y_test, y_pred, target_names=np.unique(y_train.astype(str)), zero_division=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection On All Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "# Feature and label\n",
    "df = df_all.iloc[:, 3:].dropna()\n",
    "\n",
    "# Split to feature and label\n",
    "X = df.drop(columns=['N', 'ABN', 'ABN%'])\n",
    "y = df['ABN']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=RANDOM_STATE, stratify=y)\n",
    "\n",
    "# Iterate using from one feature to all feature\n",
    "for i in range(len(X.columns)):\n",
    "    # Create the pipeline\n",
    "    pipeline = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('feature_selection', SelectKBest(score_func=f_classif, k=i+1)),\n",
    "        ('classifier', LogisticRegression(max_iter=1000, random_state=RANDOM_STATE))\n",
    "    ])\n",
    "    \n",
    "    # Fit the pipeline\n",
    "    pipeline.fit(X_train, y_train)\n",
    "\n",
    "    # Make predictions\n",
    "    y_pred = pipeline.predict(X_test)\n",
    "\n",
    "    result = {\n",
    "        'data_batch': \"ABN_B123\",\n",
    "        'number_feature': i+1,\n",
    "        'accuracy': accuracy_score(y_test, y_pred),\n",
    "        'precision_weighted': precision_score(y_test, y_pred, average='weighted', zero_division=True), \n",
    "        'recall_weighted': recall_score(y_test, y_pred, average='weighted', zero_division=True),\n",
    "        'f1_weighted': f1_score(y_test, y_pred, average='weighted', zero_division=True)\n",
    "    }\n",
    "    \n",
    "    results.append(result)\n",
    "    \n",
    "    # Message\n",
    "    # print(f\"\\n--- Inspecting Experiment Batch {experiment} ---\")\n",
    "    # print(f\"--- Results --- \\n--- Using {i+1} feature ---\")\n",
    "    # print(classification_report(y_test, y_pred, target_names=np.unique(y_train.astype(str)), zero_division=True))\n",
    "\n",
    "results = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.sort_values('f1_weighted', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split to feature and label\n",
    "X = df.drop(columns=['N', 'ABN', 'ABN%'])\n",
    "y = df['ABN']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=RANDOM_STATE, stratify=y)\n",
    "\n",
    "# Initializing and training the Random Forest Classifier\n",
    "model = RandomForestClassifier(random_state=RANDOM_STATE)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Getting feature importance\n",
    "importances = model.feature_importances_\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "# Plotting the feature importances\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x=importances[indices], y=np.array(X.columns)[indices], hue=np.array(X.columns)[indices], palette='viridis')\n",
    "plt.title('Feature Importance')\n",
    "plt.xlabel('Importance Score')\n",
    "plt.ylabel('Features')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "# Feature and label\n",
    "df = df_all.iloc[:, 3:].dropna()\n",
    "\n",
    "# Split to feature and label\n",
    "X = df[['Turning', 'Burst Swimming']]\n",
    "y = df['ABN']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=RANDOM_STATE, stratify=y)\n",
    "\n",
    "\n",
    "# Create the pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('classifier', LogisticRegression(random_state=RANDOM_STATE))\n",
    "])\n",
    "\n",
    "# Fit the pipeline\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "# Message\n",
    "print(f\"--- Results ---\")\n",
    "print(classification_report(y_test, y_pred, target_names=np.unique(y_train.astype(str)), zero_division=True))\n",
    "\n",
    "disp = ConfusionMatrixDisplay.from_predictions(y_test, y_pred, xticks_rotation='vertical')\n",
    "disp.figure_.suptitle(\"Confusion Matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def original_resample(y_train, y_res):\n",
    "    fig, axs = plt.subplots(ncols=2, figsize=(10, 5))\n",
    "    autopct = \"%.2f\"\n",
    "    y_train.value_counts().plot.pie(autopct=autopct, ax=axs[0])\n",
    "    axs[0].set_title(\"Original\")\n",
    "    y_res.value_counts().plot.pie(autopct=autopct, ax=axs[1])\n",
    "    axs[1].set_title(\"Resample\")\n",
    "    fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_RESAMPLE = 50\n",
    "results = []\n",
    "\n",
    "# Feature and label\n",
    "df = df_all.iloc[:, 3:].dropna()\n",
    "\n",
    "# Split to feature and label\n",
    "X = df.drop(columns=['N', 'ABN', 'ABN%'])\n",
    "y = df['ABN']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=RANDOM_STATE, stratify=y)\n",
    "\n",
    "df_res = df_all.iloc[X_train.index, :].groupby('ABN').apply(\n",
    "    lambda x: x.sample(N_RESAMPLE, replace=True, random_state=RANDOM_STATE)\n",
    ").reset_index(drop=True)\n",
    "\n",
    "df_res = df_res.iloc[:, 3:]\n",
    "\n",
    "# Split to feature and label\n",
    "X_res = df_res.drop(columns=['N', 'ABN', 'ABN%'])\n",
    "y_res = df_res['ABN']\n",
    "\n",
    "\n",
    "print(f\"Original Data Number: {len(X_train)}\")\n",
    "print(f\"Resample Data Number: {len(X_res)}\")\n",
    "original_resample(y_train, y_res)\n",
    "\n",
    "# Iterate using from one feature to all feature\n",
    "for i in range(len(X.columns)):\n",
    "    # Create the pipeline\n",
    "    pipeline = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('feature_selection', SelectKBest(score_func=f_classif, k=i+1)),\n",
    "        ('classifier', LogisticRegression(max_iter=1000, random_state=RANDOM_STATE))\n",
    "    ])\n",
    "    \n",
    "    # Fit the pipeline\n",
    "    pipeline.fit(X_res, y_res)\n",
    "\n",
    "    sel_feat = pipeline.named_steps['feature_selection'].get_feature_names_out(\n",
    "        input_features=X.columns\n",
    "    )\n",
    "\n",
    "    # Make predictions\n",
    "    y_pred = pipeline.predict(X_test)\n",
    "\n",
    "    result = {\n",
    "        'data_batch': \"ABN_B123\",\n",
    "        'number_feature': i+1,\n",
    "        'accuracy': accuracy_score(y_test, y_pred),\n",
    "        'precision_weighted': precision_score(y_test, y_pred, average='weighted', zero_division=True), \n",
    "        'recall_weighted': recall_score(y_test, y_pred, average='weighted', zero_division=True),\n",
    "        'f1_weighted': f1_score(y_test, y_pred, average='weighted', zero_division=True)\n",
    "    }\n",
    "    \n",
    "    results.append(result)\n",
    "    \n",
    "    # Message\n",
    "    print(f\"\\n--- Inspecting Experiment Batch 123 ---\")\n",
    "    print(f\"Selected feature: {sel_feat}\")\n",
    "    print(f\"--- Results --- \\n--- Using {i+1} feature ---\")\n",
    "    print(classification_report(y_test, y_pred, target_names=np.unique(y_train.astype(str)), zero_division=True))\n",
    "\n",
    "results = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.sort_values('accuracy', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.model_selection import cross_val_score, RepeatedStratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature and label\n",
    "df = df_all.iloc[:, 3:].dropna()\n",
    "\n",
    "# Split to feature and label\n",
    "X = df[['Turning', 'Burst Swimming']]\n",
    "y = df['ABN']\n",
    "\n",
    "steps = [\n",
    "    ('rus_auto', RandomUnderSampler(random_state=RANDOM_STATE)),\n",
    "    ('model', LogisticRegression(random_state=RANDOM_STATE))\n",
    "]\n",
    "\n",
    "pipeline = Pipeline(steps=steps)\n",
    "\n",
    "# evaluate pipeline\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=RANDOM_STATE)\n",
    "scores = cross_val_score(pipeline, X, y, scoring='recall_weighted', cv=cv, n_jobs=-1)\n",
    "print('Mean recall: %.3f' % np.mean(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "# Feature and label\n",
    "df = df_all.iloc[:, 3:].dropna()\n",
    "\n",
    "# Split to feature and label\n",
    "X = df.drop(columns=['N', 'ABN', 'ABN%'])\n",
    "y = df['ABN']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=RANDOM_STATE, stratify=y)\n",
    "\n",
    "rus = RandomUnderSampler(random_state=RANDOM_STATE)\n",
    "\n",
    "X_res, y_res = rus.fit_resample(X_train, y_train)\n",
    "\n",
    "print(f\"Original Data Number: {len(X_train)}\")\n",
    "print(f\"Resample Data Number: {len(X_res)}\")\n",
    "original_resample(y_train, y_res)\n",
    "\n",
    "# Iterate using from one feature to all feature\n",
    "for i in range(len(X.columns)):\n",
    "    # Create the pipeline\n",
    "    pipeline = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('feature_selection', SelectKBest(score_func=f_classif, k=i+1)),\n",
    "        ('classifier', LogisticRegression(max_iter=1000, random_state=RANDOM_STATE))\n",
    "    ])\n",
    "    \n",
    "    # Fit the pipeline\n",
    "    pipeline.fit(X_res, y_res)\n",
    "\n",
    "    sel_feat = pipeline.named_steps['feature_selection'].get_feature_names_out(\n",
    "        input_features=X.columns\n",
    "    )\n",
    "\n",
    "    # Make predictions\n",
    "    y_pred = pipeline.predict(X_test)\n",
    "\n",
    "    result = {\n",
    "        'data_batch': \"ABN_B123\",\n",
    "        'number_feature': i+1,\n",
    "        'accuracy': accuracy_score(y_test, y_pred),\n",
    "        'precision_weighted': precision_score(y_test, y_pred, average='weighted', zero_division=True), \n",
    "        'recall_weighted': recall_score(y_test, y_pred, average='weighted', zero_division=True),\n",
    "        'f1_weighted': f1_score(y_test, y_pred, average='weighted', zero_division=True)\n",
    "    }\n",
    "    \n",
    "    results.append(result)\n",
    "    \n",
    "    # Message\n",
    "    print(f\"\\n--- Inspecting Experiment Batch 123 ---\")\n",
    "    print(f\"Selected feature: {sel_feat}\")\n",
    "    print(f\"--- Results --- \\n--- Using {i+1} feature ---\")\n",
    "    print(classification_report(y_test, y_pred, target_names=np.unique(y_train.astype(str)), zero_division=True))\n",
    "\n",
    "results = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.sort_values('accuracy', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "# Feature and label\n",
    "df = df_all.iloc[:, 3:].dropna()\n",
    "\n",
    "# Split to feature and label\n",
    "X = df.drop(columns=['N', 'ABN', 'ABN%'])\n",
    "y = df['ABN']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=RANDOM_STATE, stratify=y)\n",
    "\n",
    "ros = RandomOverSampler(random_state=RANDOM_STATE)\n",
    "\n",
    "X_res, y_res = ros.fit_resample(X_train, y_train)\n",
    "\n",
    "\n",
    "print(f\"Original Data Number: {len(X_train)}\")\n",
    "print(f\"Resample Data Number: {len(X_res)}\")\n",
    "original_resample(y_train, y_res)\n",
    "\n",
    "\n",
    "# Iterate using from one feature to all feature\n",
    "for i in range(len(X.columns)):\n",
    "    # Create the pipeline\n",
    "    pipeline = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('feature_selection', SelectKBest(score_func=f_classif, k=i+1)),\n",
    "        ('classifier', LogisticRegression(max_iter=1000, random_state=RANDOM_STATE))\n",
    "    ])\n",
    "    \n",
    "    # Fit the pipeline\n",
    "    pipeline.fit(X_res, y_res)\n",
    "\n",
    "    # Make predictions\n",
    "    y_pred = pipeline.predict(X_test)\n",
    "\n",
    "    sel_feat = pipeline.named_steps['feature_selection'].get_feature_names_out(\n",
    "        input_features=X.columns\n",
    "    )\n",
    "\n",
    "    result = {\n",
    "        'data_batch': \"ABN_B123\",\n",
    "        'number_feature': i+1,\n",
    "        'accuracy': accuracy_score(y_test, y_pred),\n",
    "        'precision_weighted': precision_score(y_test, y_pred, average='weighted', zero_division=True), \n",
    "        'recall_weighted': recall_score(y_test, y_pred, average='weighted', zero_division=True),\n",
    "        'f1_weighted': f1_score(y_test, y_pred, average='weighted', zero_division=True)\n",
    "    }\n",
    "    \n",
    "    results.append(result)\n",
    "    \n",
    "    # Message\n",
    "    print(f\"\\n--- Inspecting Experiment Batch 123 ---\")\n",
    "    print(f\"Selected feature: {sel_feat}\")\n",
    "    print(f\"--- Results --- \\n--- Using {i+1} feature ---\")\n",
    "    print(classification_report(y_test, y_pred, target_names=np.unique(y_train.astype(str)), zero_division=True))\n",
    "\n",
    "results = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.sort_values('accuracy', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yolo_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
