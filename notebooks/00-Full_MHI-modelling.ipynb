{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import sys\n",
    "import uuid\n",
    "import joblib\n",
    "import random\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import concurrent.futures\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import albumentations as A\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import classification_report, ConfusionMatrixDisplay\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\n",
    "\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attaching project directory\n",
    "sys.path.append(os.path.dirname(os.getcwd()))\n",
    "\n",
    "# Pathing imports\n",
    "from src import GetPath, FullMHI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SIZE = (32, 32)\n",
    "NORMALIZE = False\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "random.seed(RANDOM_STATE)\n",
    "np.random.seed(RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = GetPath().shared_data()\n",
    "\n",
    "MHI_DATA = os.path.join(DATA_PATH, 'preprocess', 'mhi')\n",
    "\n",
    "# # Local Path\n",
    "# D_PATH = 'D:/fish_behavior'\n",
    "# MHI_DATA = os.path.join(D_PATH, 'data', 'preprocess', 'mhi_right_tail')\n",
    "# MHI_CNTRS_SAMPLE = os.path.join(MHI_DATA, 'samples', 'mhi_contour_sampling.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inspecting Data Samples for B1 batch - 15 frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform = A.Compose([\n",
    "#     # Normalize\n",
    "#     A.Normalize(mean=(0,0,0), std=(1,1,1)),\n",
    "#     A.RandomRotate90(),\n",
    "#     A.ShiftScaleRotate(shift_limit=0.0625, scale_limit=0.50, rotate_limit=45, p=.75),\n",
    "#     A.Resize(224, 224)\n",
    "# ])\n",
    "\n",
    "transform = A.Compose([\n",
    "        A.Resize(224, 224),\n",
    "        A.OneOf([\n",
    "            A.HorizontalFlip(p=0.5),\n",
    "            A.VerticalFlip(p=0.5),\n",
    "        ], p=0.5),\n",
    "        A.OneOf([\n",
    "            A.RandomRotate90(),\n",
    "            A.ShiftScaleRotate(shift_limit=0.0625, scale_limit=0.50, rotate_limit=45, p=.75),\n",
    "        ], p=0.5),\n",
    "        A.Normalize(mean=(0,0,0), std=(1,1,1),),\n",
    "    ])\n",
    "\n",
    "# transform = A.Compose([\n",
    "#     # Normalize\n",
    "#     A.Normalize(mean=(0,0,0), std=(1,1,1)),\n",
    "#     A.Resize(32, 32)\n",
    "# ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = FullMHI(root_dir=MHI_DATA, transform=transform)\n",
    "print(f\"Total experiment datasets: {len(datasets)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_track = {\n",
    "    'B1': [],\n",
    "    'B2': [],\n",
    "    'B3': []\n",
    "}\n",
    "\n",
    "labels_track = {\n",
    "    'B1': [],\n",
    "    'B2': [],\n",
    "    'B3': []\n",
    "}\n",
    "\n",
    "for idx in range(len(datasets)):\n",
    "    experiment, label, image = datasets.__getitem__(idx)\n",
    "    idx_track[experiment].append(idx)\n",
    "    labels_track[experiment].append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_train, idx_test = train_test_split(\n",
    "    idx_track['B1'] + idx_track['B2'],\n",
    "    test_size=0.2,\n",
    "    random_state=RANDOM_STATE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = []\n",
    "y_train = []\n",
    "\n",
    "for i in range(10):\n",
    "    for idx in idx_train:\n",
    "        experiment, label, image = datasets.__getitem__(idx)\n",
    "        X_train.append(image.flatten())\n",
    "        y_train.append(label)\n",
    "\n",
    "X_test = []\n",
    "y_test = []\n",
    "\n",
    "for idx in idx_test:\n",
    "    experiment, label, image = datasets.__getitem__(idx)\n",
    "    X_test.append(image.flatten())\n",
    "    y_test.append(label)\n",
    "\n",
    "X_val = []\n",
    "y_val = []\n",
    "\n",
    "for idx in idx_track['B3']:\n",
    "    experiment, label, image = datasets.__getitem__(idx)\n",
    "    X_val.append(image.flatten())\n",
    "    y_val.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassificationModel:\n",
    "    def __init__(self, model: BaseEstimator, model_name: str):\n",
    "        self.model = model\n",
    "        self.model_name = model_name\n",
    "        self.is_trained = False\n",
    "\n",
    "    def train(self, X_train, y_train):\n",
    "        self.model.fit(X_train, y_train)\n",
    "        train_accuracy = self.model.score(X_train, y_train)\n",
    "        print(f\"{self.model_name} Training accuracy: {train_accuracy:.4f}\")\n",
    "\n",
    "    def evaluate(self, X_test, y_test):\n",
    "        test_accuracy = self.model.score(X_test, y_test)\n",
    "        y_pred = self.model.predict(X_test)\n",
    "\n",
    "        print(f\"{self.model_name} Test accuracy: {test_accuracy:.4f}\")\n",
    "        print(\"\\n-----CLASSIFICATION REPORT-----\\n\")\n",
    "        print(classification_report(y_test, y_pred))\n",
    "\n",
    "        return y_test, y_pred\n",
    "\n",
    "    def plot_confusion_matrix(self, y_true, y_pred):\n",
    "        disp = ConfusionMatrixDisplay.from_predictions(y_true, y_pred, xticks_rotation='vertical')\n",
    "        disp.figure_.suptitle(f\"{self.model_name} Confusion Matrix\")\n",
    "        plt.show()\n",
    "\n",
    "    def run_full_analysis(self, X_train, y_train, X_test, y_test):\n",
    "        self.train(X_train, y_train)\n",
    "        y_true, y_pred = self.evaluate(X_test, y_test)\n",
    "        self.plot_confusion_matrix(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the performance of each base model\n",
    "def train_and_evaluate(name, model, images_train, labels_train, images_test, labels_test):\n",
    "    le = LabelEncoder()\n",
    "    labels_train = le.fit_transform((labels_train))\n",
    "    model.fit(images_train, labels_train)\n",
    "    label_predict = model.predict(images_test)\n",
    "    label_predict = le.inverse_transform(label_predict)\n",
    "    accuracy = accuracy_score(labels_test, label_predict)\n",
    "    f1 = f1_score(labels_test, label_predict, average='weighted', zero_division=0.0)\n",
    "    precision = precision_score(labels_test, label_predict, average='weighted', zero_division=0.0)\n",
    "    recall = recall_score(labels_test, label_predict, average='weighted', zero_division=0.0)\n",
    "    return [name, accuracy, f1, precision, recall]\n",
    "\n",
    "def model_selection(models, X_train, y_train, X_test, y_test):\n",
    "    results = []\n",
    "    with tqdm(total=len(models), desc='Model Training', unit='model') as progress:\n",
    "        with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:\n",
    "            futures = []\n",
    "            for name, model in models.items():\n",
    "                future = executor.submit(\n",
    "                    train_and_evaluate, \n",
    "                    name, \n",
    "                    model, \n",
    "                    X_train,\n",
    "                    y_train,\n",
    "                    X_test,\n",
    "                    y_test\n",
    "                )\n",
    "                futures.append(future)\n",
    "\n",
    "            for future in concurrent.futures.as_completed(futures):\n",
    "                results.append(future.result())\n",
    "                future.add_done_callback(lambda p: progress.update())\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the base models\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(random_state=RANDOM_STATE, max_iter=1000),\n",
    "    'SGD Classifier': SGDClassifier(random_state=RANDOM_STATE),\n",
    "    'Decision Tree': DecisionTreeClassifier(random_state=RANDOM_STATE),\n",
    "    'Random Forest': RandomForestClassifier(random_state=RANDOM_STATE),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(random_state=RANDOM_STATE),\n",
    "    'AdaBoost': AdaBoostClassifier(random_state=RANDOM_STATE, algorithm='SAMME'),\n",
    "    'K-Nearest Neighbors': KNeighborsClassifier(),\n",
    "    'Support Vector Machine': SVC(random_state=RANDOM_STATE),\n",
    "    'Gaussian Naive Bayes': GaussianNB(),\n",
    "    'Multi-layer Perceptron': MLPClassifier(random_state=RANDOM_STATE),\n",
    "    'XGBoost': xgb.XGBClassifier()\n",
    "}\n",
    "\n",
    "# run baseline selector\n",
    "results = model_selection(models, X_train, y_train, X_test, y_test)\n",
    "\n",
    "# Sort the results by F1-score in descending order\n",
    "results = sorted(results, key=lambda x: x[2], reverse=True)\n",
    "\n",
    "# Print the results\n",
    "print(\"Model Performance:\")\n",
    "print(\"+-----------------------+----------+----------+----------+----------+\")\n",
    "print(\"| Model                 | Accuracy | F1-score | Precision | Recall   |\")\n",
    "print(\"+-----------------------+----------+----------+----------+----------+\")\n",
    "for result in results:\n",
    "    print(\"| {:<20} | {:.4f}   | {:.4f}   | {:.4f}   | {:.4f}   |\".format(*result))\n",
    "print(\"+-----------------------+----------+----------+----------+----------+\")\n",
    "\n",
    "# Choose the best model based on the results\n",
    "best_model = models[results[0][0]]\n",
    "print(f\"The best model is {results[0][0]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
