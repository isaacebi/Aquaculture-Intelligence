{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import sys\n",
    "import uuid\n",
    "import joblib\n",
    "import random\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import concurrent.futures\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import classification_report, ConfusionMatrixDisplay\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\n",
    "\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SIZE = (32,32)\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "random.seed(RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CURR_PATH = os.getcwd()\n",
    "PROJECT_PATH = os.path.dirname(CURR_PATH)\n",
    "DATA_PATH = os.path.join(PROJECT_PATH, 'data')\n",
    "MODELS_PATH = os.path.join(DATA_PATH, 'models')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D_PATH = 'D:/fish_behavior'\n",
    "MHI_DATA = os.path.join(D_PATH, 'data', 'preprocess', 'mhi_right_tail')\n",
    "MHI_CNTRS_SAMPLE = os.path.join(MHI_DATA, 'samples', 'mhi_contour_sampling.csv')\n",
    "\n",
    "DVIDEO_PATH = \"D:/david's experiment/manuel observation video\"\n",
    "ABN_B1 = os.path.join(DVIDEO_PATH, 'abnormal', 'B1', '2023-10-23 14-18-13.mp4')\n",
    "ABN_B2 = os.path.join(DVIDEO_PATH, 'abnormal', 'B2', '2023-10-25 13-02-22.mp4')\n",
    "ABN_B3 = os.path.join(DVIDEO_PATH, 'abnormal', 'B3', '2023-10-26 12-48-44.mp4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(PROJECT_PATH)\n",
    "from src import tools\n",
    "from DatasetsModule import HistoryMotionImage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inspecting Data Samples for B1 batch - 15 frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = HistoryMotionImage.Fish_Dataset()\n",
    "print(f\"Total experiment datasets: {len(datasets)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading image path\n",
    "try:\n",
    "    folder = \"D:/fish_behavior/data/preprocess/mhi_binary/POC/B1A_1620\"\n",
    "    file = os.listdir(folder)\n",
    "    images = [cv2.imread(os.path.join(folder, image)) for image in file]\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing Approach\n",
    "\n",
    "\n",
    "### What Machine Learning Model Can See\n",
    "The data sample are sampled from B1 batch video both normal and abnormal condition. To mimic the real human inspection, a window of 60 seconds from the start time will be sample across video.\n",
    "\n",
    "\n",
    "### Computer Vision Approach: Motion History Image (MHI)\n",
    "Below are the picture taken based on the activity of hybrid grouper. This technique is also known as Motion History Image or Holistic Approach. In addition, to show the history, I have apply an order to the intensity of light picture. In simpler word, if the traces are whitter or more visible, it indicates that that is the latest motion by hybrid grouper\n",
    "\n",
    "\n",
    "### Advantages\n",
    "One major advantages in this technique is the approach compress the data from 1920x1080x60x60 [(size video) x (frame per second) x (time windows)] to simply 640x480 [(size image)].\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################\n",
    "### Plotting ###\n",
    "################\n",
    "\n",
    "# Calculate the number of rows and columns for the grid\n",
    "num_images = len(images)\n",
    "rows = 2 # int(np.sqrt(num_images))  # Assuming a square grid (adjust if needed)\n",
    "cols = 5 # int(np.ceil(num_images / rows))  # Round up for incomplete rows\n",
    "images_seq = [i for i in range(0, len(images), int(len(images)/(2*5)))]\n",
    "\n",
    "# Create the figure and subplots\n",
    "fig, axes = plt.subplots(rows, cols, figsize=(18, 12))  # Adjust figure size as needed\n",
    "\n",
    "for idx, ax in enumerate(axes.flatten()):\n",
    "    ax.imshow(images[images_seq[idx]])\n",
    "    ax.set_title(f\"Picture Sequece {idx}\")\n",
    "\n",
    "# Adjust layout (optional)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment: Abnormal by level 0 ~ 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Quality\n",
    "\n",
    "Each of the start frame for the windows 60 seconds are randomized in batch video of experiment B1 for both normal and abnormal activity. This is to increase the quality and also to mimic the human interaction for the data in use for machine learning. For example, image if something happen in a lab and you were called to inspect the situation. The first thing you do would be inspect the fish for a certain length of time. Here, we defined the inspection time for machine learning as 60 seconds as to determine the situation as fast as possible with the maximum time as possible. In addition, the randomized start frame also mimic the situation where expert can be called anytime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert time to seconds\n",
    "def time_to_seconds(time_str):\n",
    "    if not isinstance(time_str, str):\n",
    "        time_str = str(time_str)\n",
    "\n",
    "    h, m, s = map(int, time_str.split(':'))\n",
    "    return h * 3600 + m * 60 + s\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspecting MHI start time generated to end time\n",
    "# For example, if the column \"start_time_sample\" starts from 00:03:19 and ends at 00:04:00, this means the model loss initial 19 seconds of earlier data\n",
    "# For column \"time_frame\", this indicate either it is experiment conducted specifically for abnormal experiment (lack oxygen)\n",
    "start_frame = pd.read_csv(MHI_CNTRS_SAMPLE)\n",
    "start_frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below shows the normal distribution of start seconds for each of the condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################\n",
    "### Plotting: Start Frame Distribution (Histogram) ###\n",
    "######################################################\n",
    "\n",
    "fig, axes = plt.subplots(nrows=3, ncols=5, figsize=(20, 8))\n",
    "\n",
    "for level, ax in zip(sorted(start_frame['abn_level'].unique()), axes.flatten()):\n",
    "    sns.histplot(\n",
    "        start_frame[start_frame['abn_level']==level]['start_time_sample'].apply(time_to_seconds), ax=ax\n",
    "    )\n",
    "    ax.set_title(f\"Level {level} Abnormal\")\n",
    "    ax.set_xlabel('Start Seconds Value')\n",
    "    ax.set_xlim(0, start_frame[start_frame['abn_level']==level]['start_time_sample'].apply(time_to_seconds).max())\n",
    "\n",
    "# Adjust layout (optional)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking number of data for each batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading batch 1 experiment data\n",
    "b1_dataset = HistoryMotionImage.Fish_Dataset(experiment=\"B1\")\n",
    "print(f\"Data for experiment B1: {len(b1_dataset)}\")\n",
    "\n",
    "# Loading batch 2 experiment data\n",
    "b2_dataset = HistoryMotionImage.Fish_Dataset(experiment=\"B2\")\n",
    "print(f\"Data for experiment B2: {len(b2_dataset)}\")\n",
    "\n",
    "# Loading batch 2 experiment data\n",
    "b3_dataset = HistoryMotionImage.Fish_Dataset(experiment=\"B3\")\n",
    "print(f\"Data for experiment B3: {len(b3_dataset)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_level(input_string):\n",
    "    try:\n",
    "        # Extract the number after \"level_\"\n",
    "        level_number = int(input_string.split('_')[1])\n",
    "        \n",
    "        # Convert to two-digit string\n",
    "        return int(f\"{level_number:02d}\")\n",
    "    except (IndexError, ValueError):\n",
    "        return \"Invalid input\"\n",
    "    \n",
    "results = convert_level(\"level_10\")\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_stores = {}\n",
    "for batch, experiment in enumerate([b1_dataset, b2_dataset, b3_dataset]):\n",
    "    data_stores[f\"B{batch+1}\"] = {\n",
    "        'images': [],\n",
    "        'images_train': [],\n",
    "        'images_test': [],\n",
    "        'labels': [],\n",
    "        'labels_train': [],\n",
    "        'labels_test': []\n",
    "    }\n",
    "\n",
    "    for idx in range(len(experiment)):\n",
    "        image, label = experiment.__getitem__(idx, size=SIZE, normalize=False)\n",
    "        data_stores[f\"B{batch+1}\"]['images'].append(image.flatten())\n",
    "        data_stores[f\"B{batch+1}\"]['labels'].append(convert_level(label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training: Data Preparation On Batch 1 Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting data to train, test and validation\n",
    "train, test = train_test_split(np.arange(len(b1_dataset)), test_size=0.2, random_state=RANDOM_STATE, stratify=b1_dataset.labels)\n",
    "print(f\"The data in train: {len(train)} \\nThe data in test: {len(test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot 1: Start Frame Distribution (Histogram)\n",
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(18, 6))\n",
    "\n",
    "sns.histplot(train, ax=axes[0])\n",
    "axes[0].set_title(\"Distribution for train sets\")\n",
    "sns.histplot(test, ax=axes[1])\n",
    "axes[1].set_title(\"Distribution for test sets\")\n",
    "\n",
    "# Adjust layout (optional)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, balanced_accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Model: Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression as Base Model\n",
    "clf = LogisticRegression(random_state=RANDOM_STATE, max_iter=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in train:\n",
    "    image, label = b1_dataset.__getitem__(idx, size=SIZE, normalize=False)\n",
    "    data_stores[\"B1\"]['images_train'].append(image.flatten())\n",
    "    data_stores[\"B1\"]['labels_train'].append(convert_level(label))\n",
    "\n",
    "clf.fit(data_stores['B1']['images_train'], data_stores['B1']['labels_train'])\n",
    "\n",
    "print(f\"Model accuracy: {clf.score(data_stores['B1']['images_train'], data_stores['B1']['labels_train'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in test:\n",
    "    image, label = b1_dataset.__getitem__(idx, size=SIZE, normalize=False)\n",
    "    data_stores['B1']['images_test'].append(image.flatten())\n",
    "    data_stores['B1']['labels_test'].append(convert_level(label))\n",
    "\n",
    "y_pred = clf.predict(data_stores['B1']['images_test'])\n",
    "\n",
    "print(f\"Model accuracy: {clf.score(data_stores['B1']['images_test'], data_stores['B1']['labels_test'])}\")\n",
    "\n",
    "print(\"\\n-----CLASSIFICATION REPORT-----\\n\", classification_report(data_stores['B1']['labels_test'], y_pred))\n",
    "\n",
    "disp = ConfusionMatrixDisplay.from_predictions(data_stores['B1']['labels_test'], y_pred, xticks_rotation='vertical')\n",
    "# print(f\"Confusion matrix:\\n{disp.confusion_matrix}\")\n",
    "disp.figure_.suptitle(\"Confusion Matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Issues\n",
    "Some of the \"level abnormal\" are not present in batch 1 experiment, therefore it is not suitable to be used as training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting Countplot\n",
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(18, 6))\n",
    "\n",
    "sns.countplot(x=data_stores[\"B1\"]['labels_test'], ax=axes[0])\n",
    "axes[0].set_title(\"Distribution for train sets\")\n",
    "sns.countplot(x=data_stores[\"B1\"]['labels_test'], ax=axes[1])\n",
    "axes[1].set_title(\"Distribution for test sets\")\n",
    "\n",
    "# Adjust layout (optional)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training: Data Preparation On Batch 2 Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting data to train, test and validation\n",
    "train, test = train_test_split(np.arange(len(b2_dataset)), test_size=0.2, random_state=RANDOM_STATE, stratify=b2_dataset.labels)\n",
    "print(f\"The data in train: {len(train)} \\nThe data in test: {len(test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot 1: Start Frame Distribution (Histogram)\n",
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(18, 6))\n",
    "\n",
    "sns.histplot(train, ax=axes[0])\n",
    "axes[0].set_title(\"Distribution for train sets\")\n",
    "sns.histplot(test, ax=axes[1])\n",
    "axes[1].set_title(\"Distribution for test sets\")\n",
    "\n",
    "# Adjust layout (optional)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Model: Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression as Base Model\n",
    "clf = LogisticRegression(random_state=RANDOM_STATE, max_iter=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in train:\n",
    "    image, label = b2_dataset.__getitem__(idx, size=SIZE, normalize=False)\n",
    "    data_stores['B2']['images_train'].append(image.flatten())\n",
    "    data_stores['B2']['labels_train'].append(convert_level(label))\n",
    "\n",
    "clf.fit(data_stores['B2']['images_train'], data_stores['B2']['labels_train'])\n",
    "\n",
    "print(f\"Model accuracy: {clf.score(data_stores['B2']['images_train'], data_stores['B2']['labels_train'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in test:\n",
    "    image, label = b2_dataset.__getitem__(idx, size=SIZE, normalize=False)\n",
    "    data_stores['B2']['images_test'].append(image.flatten())\n",
    "    data_stores['B2']['labels_test'].append(convert_level(label))\n",
    "\n",
    "y_pred = clf.predict(data_stores['B2']['images_test'])\n",
    "\n",
    "print(f\"Model accuracy: {clf.score(data_stores['B2']['images_test'], data_stores['B2']['labels_test'])}\")\n",
    "\n",
    "print(\"\\n-----CLASSIFICATION REPORT-----\\n\", classification_report(data_stores['B2']['labels_test'], y_pred))\n",
    "\n",
    "disp = ConfusionMatrixDisplay.from_predictions(data_stores['B2']['labels_test'], y_pred, xticks_rotation='vertical')\n",
    "# print(f\"Confusion matrix:\\n{disp.confusion_matrix}\")\n",
    "disp.figure_.suptitle(\"Confusion Matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting Countplot\n",
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(18, 6))\n",
    "\n",
    "sns.countplot(x=data_stores['B2']['labels_train'], ax=axes[0])\n",
    "axes[0].set_title(\"Distribution for train sets\")\n",
    "sns.countplot(x=data_stores['B2']['labels_test'], ax=axes[1])\n",
    "axes[1].set_title(\"Distribution for test sets\")\n",
    "\n",
    "# Adjust layout (optional)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Selection\n",
    "\n",
    "### Training Using Batch 1 and Validation with Batch 1,2 & 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the base models\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(random_state=RANDOM_STATE),\n",
    "    'SGD Classifier': SGDClassifier(random_state=RANDOM_STATE),\n",
    "    'Decision Tree': DecisionTreeClassifier(random_state=RANDOM_STATE),\n",
    "    'Random Forest': RandomForestClassifier(random_state=RANDOM_STATE),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(random_state=RANDOM_STATE),\n",
    "    'AdaBoost': AdaBoostClassifier(random_state=RANDOM_STATE, algorithm='SAMME'),\n",
    "    'K-Nearest Neighbors': KNeighborsClassifier(),\n",
    "    'Support Vector Machine': SVC(random_state=RANDOM_STATE),\n",
    "    'Gaussian Naive Bayes': GaussianNB(),\n",
    "    'XGBoost': xgb.XGBClassifier()\n",
    "}\n",
    "\n",
    "# Evaluate the performance of each base model\n",
    "def train_and_evaluate(name, model, images_train, labels_train, images_test, labels_test):\n",
    "    le = LabelEncoder()\n",
    "    labels_train = le.fit_transform((labels_train))\n",
    "    model.fit(images_train, labels_train)\n",
    "    label_predict = model.predict(images_test)\n",
    "    label_predict = le.inverse_transform(label_predict)\n",
    "    accuracy = accuracy_score(labels_test, label_predict)\n",
    "    f1 = f1_score(labels_test, label_predict, average='weighted', zero_division=0.0)\n",
    "    precision = precision_score(labels_test, label_predict, average='weighted', zero_division=0.0)\n",
    "    recall = recall_score(labels_test, label_predict, average='weighted', zero_division=0.0)\n",
    "    return [name, accuracy, f1, precision, recall]\n",
    "\n",
    "def model_selection(experiment='B1'):\n",
    "    results = []\n",
    "    with tqdm(total=len(models), desc='Model Training', unit='model') as progress:\n",
    "        with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:\n",
    "            futures = []\n",
    "            for name, model in models.items():\n",
    "                future = executor.submit(\n",
    "                    train_and_evaluate, \n",
    "                    name, \n",
    "                    model, \n",
    "                    data_stores[experiment]['images_train'],\n",
    "                    data_stores[experiment]['labels_train'],\n",
    "                    data_stores[experiment]['images_test'],\n",
    "                    data_stores[experiment]['labels_test']\n",
    "                )\n",
    "                futures.append(future)\n",
    "\n",
    "            for future in concurrent.futures.as_completed(futures):\n",
    "                results.append(future.result())\n",
    "                future.add_done_callback(lambda p: progress.update())\n",
    "    \n",
    "    return results\n",
    "\n",
    "# run baseline selector\n",
    "results = model_selection(experiment='B1')\n",
    "\n",
    "# Sort the results by F1-score in descending order\n",
    "results = sorted(results, key=lambda x: x[2], reverse=True)\n",
    "\n",
    "# Print the results\n",
    "print(\"Model Performance:\")\n",
    "print(\"+-----------------------+----------+----------+----------+----------+\")\n",
    "print(\"| Model                 | Accuracy | F1-score | Precision | Recall   |\")\n",
    "print(\"+-----------------------+----------+----------+----------+----------+\")\n",
    "for result in results:\n",
    "    print(\"| {:<20} | {:.4f}   | {:.4f}   | {:.4f}   | {:.4f}   |\".format(*result))\n",
    "print(\"+-----------------------+----------+----------+----------+----------+\")\n",
    "\n",
    "# Choose the best model based on the results\n",
    "best_model = models[results[0][0]]\n",
    "print(f\"The best model is {results[0][0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(data_stores['B1']['labels_test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(data_stores['B1']['labels_train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ValidationAllExperiment(model, data_stores, experiment=['B1', 'B2', 'B3']):\n",
    "    for batch in experiment:\n",
    "        y_pred = model.predict(data_stores[batch]['images'])\n",
    "        print(f\"Running Validation for Batch {batch}\")\n",
    "        print(f\"Model accuracy: {model.score(data_stores[batch]['images'], data_stores[batch]['labels'])}\")\n",
    "        print(f\"\\n-----CLASSIFICATION REPORT BATCH {batch}-----\\n\", classification_report(data_stores[batch]['labels'], y_pred, zero_division=0.0))\n",
    "        disp = ConfusionMatrixDisplay.from_predictions(data_stores[batch]['labels'], y_pred, xticks_rotation='vertical')\n",
    "        # print(f\"Confusion matrix:\\n{disp.confusion_matrix}\")\n",
    "        disp.figure_.suptitle(f\"Confusion Matrix Batch {batch}\")\n",
    "        plt.show()\n",
    "\n",
    "ValidationAllExperiment(model=best_model, data_stores=data_stores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Using Batch 2 and Validation with Batch 1,2 & 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run baseline selector\n",
    "results = model_selection(experiment='B2')\n",
    "\n",
    "# Sort the results by F1-score in descending order\n",
    "results = sorted(results, key=lambda x: x[2], reverse=True)\n",
    "\n",
    "# Print the results\n",
    "print(\"Model Performance:\")\n",
    "print(\"+-----------------------+----------+----------+----------+----------+\")\n",
    "print(\"| Model                 | Accuracy | F1-score | Precision | Recall   |\")\n",
    "print(\"+-----------------------+----------+----------+----------+----------+\")\n",
    "for result in results:\n",
    "    print(\"| {:<20} | {:.4f}   | {:.4f}   | {:.4f}   | {:.4f}   |\".format(*result))\n",
    "print(\"+-----------------------+----------+----------+----------+----------+\")\n",
    "\n",
    "# Choose the best model based on the results\n",
    "best_model = models[results[0][0]]\n",
    "print(f\"The best model is {results[0][0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ValidationAllExperiment(model=best_model, data_stores=data_stores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Using Batch 3 and Validation with Batch 1,2 & 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run baseline selector\n",
    "results = model_selection(experiment='B3')\n",
    "\n",
    "# Sort the results by F1-score in descending order\n",
    "results = sorted(results, key=lambda x: x[2], reverse=True)\n",
    "\n",
    "# Print the results\n",
    "print(\"Model Performance:\")\n",
    "print(\"+-----------------------+----------+----------+----------+----------+\")\n",
    "print(\"| Model                 | Accuracy | F1-score | Precision | Recall   |\")\n",
    "print(\"+-----------------------+----------+----------+----------+----------+\")\n",
    "for result in results:\n",
    "    print(\"| {:<20} | {:.4f}   | {:.4f}   | {:.4f}   | {:.4f}   |\".format(*result))\n",
    "print(\"+-----------------------+----------+----------+----------+----------+\")\n",
    "\n",
    "# Choose the best model based on the results\n",
    "best_model = models[results[0][0]]\n",
    "print(f\"The best model is {results[0][0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ValidationAllExperiment(model=best_model, data_stores=data_stores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skopt import BayesSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'Logistic Regression': {\n",
    "        'C': [0.0001, 0.001, 0.01, 0.1, 1.0],\n",
    "        'penalty': ['l2', 'l1', 'elasticnet'],\n",
    "        'l1_ratio': [0.1, 0.3, 0.5, 0.7, 0.9]\n",
    "    },\n",
    "    'Support Vector Machine': {\n",
    "        'C': [0.0001, 0.001, 0.01, 0.1, 1.0],\n",
    "        'kernel': ['rbf', 'poly', 'sigmoid'],\n",
    "        'gamma': ['scale', 'auto', 0.1, 1]\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuned & Trained On Batch 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tuning_model(model, param_grid, experiment='B1', cv=5, scoring='accuracy', n_jobs=16):\n",
    "    # Create the grid search object\n",
    "    grid_search = BayesSearchCV(model, param_grid, cv=cv, scoring=scoring, n_jobs=n_jobs)\n",
    "\n",
    "    # Fit the grid search model\n",
    "    grid_search.fit(data_stores[experiment]['images_train'], data_stores[experiment]['labels_train'])\n",
    "\n",
    "    # Print the best hyperparameters and the corresponding score\n",
    "    print('Best hyperparameters: ', grid_search.best_params_)\n",
    "    print('Best score: ', grid_search.best_score_)\n",
    "\n",
    "    # Evaluate the model on the test set\n",
    "    test_score = grid_search.best_estimator_.score(data_stores[experiment]['images_test'], data_stores[experiment]['labels_test'])\n",
    "    print('Test set accuracy: ', test_score)\n",
    "    return grid_search.best_estimator_\n",
    "\n",
    "tuned_model = tuning_model(\n",
    "    model=best_model,\n",
    "    param_grid=param_grid['Support Vector Machine'],\n",
    "    experiment='B1',\n",
    "    cv=2,\n",
    "    n_jobs=-1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ValidationAllExperiment(model=tuned_model, data_stores=data_stores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuned & Train On Batch 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuned_model = tuning_model(\n",
    "    model=best_model,\n",
    "    param_grid=param_grid['Support Vector Machine'],\n",
    "    experiment='B2',\n",
    "    cv=2,\n",
    "    n_jobs=50\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ValidationAllExperiment(model=tuned_model, data_stores=data_stores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuned & Train On Batch 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuned_model = tuning_model(\n",
    "    model=best_model,\n",
    "    param_grid=param_grid['Support Vector Machine'],\n",
    "    experiment='B3',\n",
    "    cv=2,\n",
    "    n_jobs=-1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ValidationAllExperiment(model=tuned_model, data_stores=data_stores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "The simple data preprocessing of Human Motion Image (HMI) are sufficient to differentiate between normal and abnormal condition solely due to the reaction of hybrid grouper to the condition. In addition, the model can be further improve by fine tuning model. In addition, we can also increase the validation test.\n",
    "\n",
    "# Future Improvement\n",
    "\n",
    "While HMI shown to be successfuly, we can adept advanced preprocessing in the same domain by using optiflow approach.\n",
    "\n",
    "\n",
    "# Limitation\n",
    "\n",
    "This approach is solely to differentiate between normal and abnormal condition. However, the video possesed a lot more of potential data to be extract such as fish behaviour of fast swim, hovering, ascending vertical and etc.. This approach may be used to extract the behaviour, however, more time is needed to exactly take the frame time of such condition. We also need to consider the number of behaviour done by the number of hybrid grouper."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improvement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "import xgboost as xgb\n",
    "import concurrent.futures\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "\n",
    "#\n",
    "_, labels_train = np.unique(labels_train, return_inverse=True)\n",
    "_, labels_test = np.unique(labels_test, return_inverse=True)\n",
    "\n",
    "# Define the base models\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(),\n",
    "    'SGD Classifier': SGDClassifier(),\n",
    "    'Decision Tree': DecisionTreeClassifier(),\n",
    "    'Random Forest': RandomForestClassifier(),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(),\n",
    "    'AdaBoost': AdaBoostClassifier(),\n",
    "    'K-Nearest Neighbors': KNeighborsClassifier(),\n",
    "    'Support Vector Machine': SVC(),\n",
    "    'Gaussian Naive Bayes': GaussianNB(),\n",
    "    'XGBoost': xgb.XGBClassifier()\n",
    "}\n",
    "\n",
    "# Evaluate the performance of each base model\n",
    "results = []\n",
    "# for name, model in models.items():\n",
    "#     model.fit(images_train, labels_train)\n",
    "#     label_predict = model.predict(images_test)\n",
    "#     accuracy = accuracy_score(labels_test, label_predict)\n",
    "#     f1 = f1_score(labels_test, label_predict, average='micro')\n",
    "#     precision = precision_score(labels_test, label_predict, average='micro')\n",
    "#     recall = recall_score(labels_test, label_predict, average='micro')\n",
    "#     results.append([name, accuracy, f1, precision, recall])\n",
    "\n",
    "def train_and_evaluate(name, model, images_train, labels_train, images_test, labels_test):\n",
    "    model.fit(images_train, labels_train)\n",
    "    label_predict = model.predict(images_test)\n",
    "    accuracy = accuracy_score(labels_test, label_predict)\n",
    "    f1 = f1_score(labels_test, label_predict, average='micro')\n",
    "    precision = precision_score(labels_test, label_predict, average='micro')\n",
    "    recall = recall_score(labels_test, label_predict, average='micro')\n",
    "    return [name, accuracy, f1, precision, recall]\n",
    "\n",
    "with tqdm(total=len(models), desc='Model Training', unit='model') as progress:\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:\n",
    "        futures = []\n",
    "        for name, model in models.items():\n",
    "            future = executor.submit(train_and_evaluate, name, model, images_train, labels_train, images_test, labels_test)\n",
    "            futures.append(future)\n",
    "\n",
    "        for future in concurrent.futures.as_completed(futures):\n",
    "            results.append(future.result())\n",
    "            future.add_done_callback(lambda p: progress.update())\n",
    "\n",
    "\n",
    "\n",
    "# Sort the results by F1-score in descending order\n",
    "results = sorted(results, key=lambda x: x[2], reverse=True)\n",
    "\n",
    "# Print the results\n",
    "print(\"Model Performance:\")\n",
    "print(\"+-----------------------+----------+----------+----------+----------+\")\n",
    "print(\"| Model                 | Accuracy | F1-score | Precision | Recall   |\")\n",
    "print(\"+-----------------------+----------+----------+----------+----------+\")\n",
    "for result in results:\n",
    "    print(\"| {:<20} | {:.4f}   | {:.4f}   | {:.4f}   | {:.4f}   |\".format(*result))\n",
    "print(\"+-----------------------+----------+----------+----------+----------+\")\n",
    "\n",
    "# Choose the best model based on the results\n",
    "best_model = models[results[0][0]]\n",
    "print(f\"The best model is {results[0][0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'C': [0.0001, 0.001, 0.01, 0.1, 1.0],\n",
    "    'penalty': ['l2', 'l1', 'elasticnet'],\n",
    "    'l1_ratio': [0.1, 0.3, 0.5, 0.7, 0.9]\n",
    "}\n",
    "\n",
    "# Create the Logistic Regression model\n",
    "model = LogisticRegression()\n",
    "\n",
    "# Create the grid search object\n",
    "grid_search = GridSearchCV(model, param_grid=param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "\n",
    "# Fit the grid search model\n",
    "grid_search.fit(images_train, labels_train)\n",
    "\n",
    "# Print the best hyperparameters and the corresponding score\n",
    "print('Best hyperparameters: ', grid_search.best_params_)\n",
    "print('Best score: ', grid_search.best_score_)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_score = grid_search.best_estimator_.score(images_test, labels_test)\n",
    "print('Test set accuracy: ', test_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stratified Fold evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "y = []\n",
    "for idx in range(len(datasets)):\n",
    "    image, label = datasets.__getitem__(idx)\n",
    "    X.append(image.flatten())\n",
    "    y.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# Create a stratified k-fold cross-validation iterator\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Initialize a list to store the accuracy scores\n",
    "accuracy_scores = []\n",
    "\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "\n",
    "# Perform stratified k-fold cross-validation\n",
    "for train_index, test_index in skf.split(X, y):\n",
    "    # Split the data into training and test sets\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    # Train a logistic regression model\n",
    "    model = LogisticRegression(C=0.1, l1_ratio=0.5, penalty='l2')\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Evaluate the model on the test set\n",
    "    y_pred = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    accuracy_scores.append(accuracy)\n",
    "    \n",
    "    print(f\"Fold accuracy: {accuracy:.3f}\")\n",
    "\n",
    "# Calculate the mean accuracy score\n",
    "mean_accuracy = sum(accuracy_scores) / len(accuracy_scores)\n",
    "print(f\"Mean accuracy: {mean_accuracy:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_path = os.path.join(D_PATH, 'data', 'models', 'classification', f\"MHI_SGD_{str(uuid.uuid4())}.joblib\")\n",
    "# joblib.dump(grid_search.best_estimator_, model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = os.path.join(D_PATH, 'data', 'models', 'classification', f\"LR_{str(uuid.uuid4())}.joblib\")\n",
    "joblib.dump(grid_search.best_estimator_, model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = []\n",
    "\n",
    "for model in os.listdir(os.path.join(D_PATH, 'data', 'models', 'classification')):\n",
    "    models.append(os.path.join(D_PATH, 'data', 'models', 'classification', model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo Version 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "videos = ABN_B1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_to_seconds(time_str):\n",
    "    h, m, s = map(int, time_str.split(':'))\n",
    "    return h * 3600 + m * 60 + s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clf = SGDClassifier(alpha=0.1, l1_ratio=0.5, penalty='l2', loss='log_loss')\n",
    "clf = LogisticRegression(C=0.1, l1_ratio=0.5, penalty='l2')\n",
    "clf.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_frames = [\n",
    "    '15'\n",
    "]\n",
    "\n",
    "# clf = joblib.load(models[-1])\n",
    "\n",
    "seconds_length = 60\n",
    "scaler = 0.001\n",
    "\n",
    "video_path = ABN_B1\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "backSub = cv2.createBackgroundSubtractorMOG2(\n",
    "    varThreshold=200\n",
    ")\n",
    "kernel = (5,5)\n",
    "\n",
    "width = 720\n",
    "height = 480\n",
    "text = \"Need more time inspection\"\n",
    "\n",
    "# Get the frame rate of the video\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "# Convert start_time to seconds\n",
    "start_seconds = time_to_seconds(\"00:00:00\")\n",
    "\n",
    "# Calculate the start frame based on start time\n",
    "start_frame = int(start_seconds * fps)\n",
    "\n",
    "# Set the capture time in frames\n",
    "cap.set(cv2.CAP_PROP_POS_FRAMES, start_frame)\n",
    "\n",
    "hist_frames = {}\n",
    "\n",
    "ret, old_frame = cap.read()\n",
    "\n",
    "for fps in list_frames:\n",
    "    hist_frames[fps] = np.zeros(old_frame.shape[:2], dtype=np.uint8)\n",
    "\n",
    "total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "gif_frame = []\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    if not ret:\n",
    "        break\n",
    "    \n",
    "    # get backsubtraction method\n",
    "    bs_mask = tools.get_bs(frame, backSub, kernel)\n",
    "\n",
    "    # get contors\n",
    "    cntrs = tools.get_contours(old_frame, frame, kernel)\n",
    "\n",
    "    # current number of frame\n",
    "    curr_num_frame = cap.get(cv2.CAP_PROP_POS_FRAMES)\n",
    "\n",
    "    for fps in hist_frames.keys():\n",
    "        if curr_num_frame % int(fps) == 0:\n",
    "            # gets holistic background subtraction\n",
    "            # hist_frames[fps] = cv2.add(hist_frames[fps], bs_mask)\n",
    "\n",
    "            # get holistic contours\n",
    "            color = 255 * scaler\n",
    "            cv2.drawContours(hist_frames[fps], cntrs, -1, (color, color, color), 2)\n",
    "            gif_frame.append(hist_frames['15'])\n",
    "\n",
    "        if curr_num_frame % (60*seconds_length) == 0:\n",
    "            image_input = cv2.resize(hist_frames[fps], (224, 224))\n",
    "            image_input = cv2.normalize(image_input, None, -1, 1, cv2.NORM_MINMAX)\n",
    "            image_input = image_input.flatten()\n",
    "            condition = clf.predict_proba([image_input])\n",
    "\n",
    "            if condition.size != 0:\n",
    "                text = \"\".join([f\"ABL{i}:{round(condition[0][i]/1, 2)}\" for i in range(len(condition[0]))])\n",
    "\n",
    "                # text = f\"Condition : Abnormal: {round(condition[0], 5)}, Normal: {round(condition[1], 5)}\"\n",
    "\n",
    "            hist_frames[fps] = np.zeros(old_frame.shape[:2], dtype=np.uint8)\n",
    "            scaler = 0.001\n",
    "\n",
    "    # handle output\n",
    "    cv2.putText(frame, text, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "\n",
    "    win1 = cv2.resize(hist_frames['15'], (width, height))\n",
    "    # win2 = cv2.resize(hist_frames['30'], (width, height))\n",
    "    # win3 = cv2.resize(hist_frames['60'], (width, height))\n",
    "\n",
    "    # display = np.hstack((win1, win2, win3))\n",
    "\n",
    "    scaler += 0.0001\n",
    "\n",
    "    cv2.imshow(\"Original\", cv2.resize(frame, (width, height)))\n",
    "    cv2.imshow(\"Frames Holistic\", win1)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abnormal vs Normal - Old Scenario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = HistoryMotionImage.Fish_Dataset_Binary()\n",
    "len(datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data preparation\n",
    "start_frame = {\n",
    "    'normal':[time_to_seconds(frame) for frame in datasets.get_frame('normal')], \n",
    "    'abnormal':[time_to_seconds(frame) for frame in datasets.get_frame('abnormal')]\n",
    "}\n",
    "\n",
    "# TODO: Dataframe size and dataset size is not the same, need to inspect why software cannot extract all of the time defined, however, the existing data is sufficient for training\n",
    "# start frame for both normal and abnormal video\n",
    "start_frame = pd.DataFrame(start_frame)\n",
    "start_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Below shows the normal distribution of start seconds for each of the condition\n",
    "\n",
    "# Plot 1: Start Frame Distribution (Histogram)\n",
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(12, 6))\n",
    "\n",
    "for col, ax in zip(start_frame.columns, axes.flatten()):\n",
    "    sns.histplot(start_frame[col], ax=ax)\n",
    "    ax.set_title(f\"Distribution of {col} Start Frames\")\n",
    "    ax.set_xlabel('Start Second Value')\n",
    "\n",
    "# Adjust layout (optional)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(np.arange(len(datasets)), test_size=0.3, random_state=RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start Frame Distribution - Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot 1: Start Frame Distribution (Histogram)\n",
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(18, 6))\n",
    "\n",
    "sns.histplot(train, ax=axes[0])\n",
    "axes[0].set_title(\"Distribution for train sets\")\n",
    "sns.histplot(test, ax=axes[1])\n",
    "axes[1].set_title(\"Distribution for test sets\")\n",
    "\n",
    "# Adjust layout (optional)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_train = []\n",
    "labels_train = []\n",
    "for idx in train:\n",
    "    image, label = datasets.__getitem__(idx)\n",
    "    images_train.append(image.flatten())\n",
    "    labels_train.append(label)\n",
    "\n",
    "images_test = []\n",
    "labels_test = []\n",
    "for idx in test:\n",
    "    image, label = datasets.__getitem__(idx)\n",
    "    images_test.append(image.flatten())\n",
    "    labels_test.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(labels_train, return_inverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, labels_train = np.unique(labels_train, return_inverse=True)\n",
    "_, labels_test = np.unique(labels_test, return_inverse=True)\n",
    "\n",
    "results = []\n",
    "\n",
    "# Define the base models\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(),\n",
    "    'SGD Classifier': SGDClassifier(),\n",
    "    'Decision Tree': DecisionTreeClassifier(),\n",
    "    'Random Forest': RandomForestClassifier(),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(),\n",
    "    'AdaBoost': AdaBoostClassifier(),\n",
    "    'K-Nearest Neighbors': KNeighborsClassifier(),\n",
    "    'Support Vector Machine': SVC(),\n",
    "    'Gaussian Naive Bayes': GaussianNB(),\n",
    "    'XGBoost': xgb.XGBClassifier()\n",
    "}\n",
    "\n",
    "with tqdm(total=len(models), desc='Model Training', unit='model') as progress:\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:\n",
    "        futures = []\n",
    "        for name, model in models.items():\n",
    "            future = executor.submit(train_and_evaluate, name, model, images_train, labels_train, images_test, labels_test)\n",
    "            futures.append(future)\n",
    "\n",
    "        for future in concurrent.futures.as_completed(futures):\n",
    "            results.append(future.result())\n",
    "            future.add_done_callback(lambda p: progress.update())\n",
    "\n",
    "# Sort the results by F1-score in descending order\n",
    "results = sorted(results, key=lambda x: x[2], reverse=True)\n",
    "\n",
    "# Print the results\n",
    "print(\"Model Performance:\")\n",
    "print(\"+-----------------------+----------+----------+----------+----------+\")\n",
    "print(\"| Model                 | Accuracy | F1-score | Precision | Recall   |\")\n",
    "print(\"+-----------------------+----------+----------+----------+----------+\")\n",
    "for result in results:\n",
    "    print(\"| {:<20} | {:.4f}   | {:.4f}   | {:.4f}   | {:.4f}   |\".format(*result))\n",
    "print(\"+-----------------------+----------+----------+----------+----------+\")\n",
    "\n",
    "# Choose the best model based on the results\n",
    "best_model = models[results[0][0]]\n",
    "print(f\"The best model is {results[0][0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'C': [0.1, 1, 10, 100],\n",
    "    'gamma': [0.001, 0.01, 0.1, 1],\n",
    "    'kernel': ['linear', 'rbf', 'poly']\n",
    "}\n",
    "\n",
    "# Create the SVM model\n",
    "model = SVC()\n",
    "\n",
    "# Create the grid search object\n",
    "grid_search = GridSearchCV(model, param_grid=param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "\n",
    "# Fit the grid search model\n",
    "grid_search.fit(images_train, labels_train)\n",
    "\n",
    "# Print the best hyperparameters and the corresponding score\n",
    "print('Best hyperparameters: ', grid_search.best_params_)\n",
    "print('Best score: ', grid_search.best_score_)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_score = grid_search.best_estimator_.score(images_test, labels_test)\n",
    "print('Test set accuracy: ', test_score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
