{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import sys\n",
    "import uuid\n",
    "import joblib\n",
    "import random\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import concurrent.futures\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import classification_report, ConfusionMatrixDisplay\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attaching project directory\n",
    "sys.path.append(os.path.dirname(os.getcwd()))\n",
    "\n",
    "# Pathing imports\n",
    "from src import GetPath, MotionHistoryImage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SIZE = (64, 64)\n",
    "NORMALIZE = False\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "random.seed(RANDOM_STATE)\n",
    "np.random.seed(RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = GetPath().data()\n",
    "\n",
    "MHI_DATA = os.path.join(DATA_PATH, 'preprocess', 'mhi_right_tail')\n",
    "MHI_CNTRS_SAMPLE = os.path.join(MHI_DATA, 'samples', 'mhi_contour_sampling.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inspecting Data Samples for B1 batch - 15 frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = MotionHistoryImage()\n",
    "print(f\"Total experiment datasets: {len(datasets)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing Approach\n",
    "\n",
    "\n",
    "### What Machine Learning Model Can See\n",
    "The data sample are sampled from B1 batch video both normal and abnormal condition. To mimic the real human inspection, a window of 60 seconds from the start time will be sample across video.\n",
    "\n",
    "\n",
    "### Computer Vision Approach: Motion History Image (MHI)\n",
    "Below are the picture taken based on the activity of hybrid grouper. This technique is also known as Motion History Image or Holistic Approach. In addition, to show the history, I have apply an order to the intensity of light picture. In simpler word, if the traces are whitter or more visible, it indicates that that is the latest motion by hybrid grouper\n",
    "\n",
    "\n",
    "### Advantages\n",
    "One major advantages in this technique is the approach compress the data from 1920x1080x60x60 [(size video) x (frame per second) x (time windows)] to simply 640x480 [(size image)].\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################\n",
    "### Plotting ###\n",
    "################\n",
    "# Loading image path\n",
    "try:\n",
    "    folder = \"D:/fish_behavior/data/preprocess/mhi_binary/POC/B1A_1620\"\n",
    "    file = os.listdir(folder)\n",
    "    images = [cv2.imread(os.path.join(folder, image)) for image in file]\n",
    "\n",
    "    # Calculate the number of rows and columns for the grid\n",
    "    num_images = len(images)\n",
    "    rows = 2 # int(np.sqrt(num_images))  # Assuming a square grid (adjust if needed)\n",
    "    cols = 5 # int(np.ceil(num_images / rows))  # Round up for incomplete rows\n",
    "    images_seq = [i for i in range(0, len(images), int(len(images)/(2*5)))]\n",
    "\n",
    "    # Create the figure and subplots\n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(18, 12))  # Adjust figure size as needed\n",
    "\n",
    "    for idx, ax in enumerate(axes.flatten()):\n",
    "        ax.imshow(images[images_seq[idx]])\n",
    "        ax.set_title(f\"Picture Sequece {idx}\")\n",
    "\n",
    "    # Adjust layout (optional)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Display the plot\n",
    "    plt.show()\n",
    "\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment: Abnormal by level 0 ~ 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Quality\n",
    "\n",
    "Each of the start frame for the windows 60 seconds are randomized in batch video of experiment B1 for both normal and abnormal activity. This is to increase the quality and also to mimic the human interaction for the data in use for machine learning. For example, image if something happen in a lab and you were called to inspect the situation. The first thing you do would be inspect the fish for a certain length of time. Here, we defined the inspection time for machine learning as 60 seconds as to determine the situation as fast as possible with the maximum time as possible. In addition, the randomized start frame also mimic the situation where expert can be called anytime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert time to seconds\n",
    "def time_to_seconds(time_str):\n",
    "    if not isinstance(time_str, str):\n",
    "        time_str = str(time_str)\n",
    "\n",
    "    h, m, s = map(int, time_str.split(':'))\n",
    "    return h * 3600 + m * 60 + s\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspecting MHI start time generated to end time\n",
    "# For example, if the column \"start_time_sample\" starts from 00:03:19 and ends at 00:04:00, this means the model loss initial 19 seconds of earlier data\n",
    "# For column \"time_frame\", this indicate either it is experiment conducted specifically for abnormal experiment (lack oxygen)\n",
    "start_frame = pd.read_csv(MHI_CNTRS_SAMPLE)\n",
    "start_frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below shows the normal distribution of start seconds for each of the condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################\n",
    "### Plotting: Start Frame Distribution (Histogram) ###\n",
    "######################################################\n",
    "\n",
    "fig, axes = plt.subplots(nrows=3, ncols=5, figsize=(20, 8))\n",
    "\n",
    "for level, ax in zip(sorted(start_frame['abn_level'].unique()), axes.flatten()):\n",
    "    sns.histplot(\n",
    "        start_frame[start_frame['abn_level']==level]['start_time_sample'].apply(time_to_seconds), ax=ax\n",
    "    )\n",
    "    ax.set_title(f\"Level {level} Abnormal\")\n",
    "    ax.set_xlabel('Start Seconds Value')\n",
    "    ax.set_xlim(0, start_frame[start_frame['abn_level']==level]['start_time_sample'].apply(time_to_seconds).max())\n",
    "\n",
    "# Adjust layout (optional)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking number of data for each batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading batch 1 experiment data\n",
    "b1_dataset = MotionHistoryImage(experiment=\"B1\")\n",
    "print(f\"Data for experiment B1: {len(b1_dataset)}\")\n",
    "\n",
    "# Loading batch 2 experiment data\n",
    "b2_dataset = MotionHistoryImage(experiment=\"B2\")\n",
    "print(f\"Data for experiment B2: {len(b2_dataset)}\")\n",
    "\n",
    "# Loading batch 2 experiment data\n",
    "b3_dataset = MotionHistoryImage(experiment=\"B3\")\n",
    "print(f\"Data for experiment B3: {len(b3_dataset)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# images = []\n",
    "# labels = []\n",
    "# for idx in np.random.randint(1, len(b1_dataset), 100):\n",
    "#     image, label = b1_dataset.__getitem__(idx, size=SIZE, normalize=False)\n",
    "#     images.append(image)\n",
    "#     labels.append(label)\n",
    "\n",
    "\n",
    "def distance_image(im, im2):\n",
    "    d1 = cv2.matchShapes(im,im2,cv2.CONTOURS_MATCH_I1,0) \n",
    "    d2 = cv2.matchShapes(im,im2,cv2.CONTOURS_MATCH_I2,0) \n",
    "    d3 = cv2.matchShapes(im,im2,cv2.CONTOURS_MATCH_I3,0)\n",
    "    return d1, d2, d3\n",
    "\n",
    "def calculate_huMoments(image):\n",
    "    # Calculate Moments\n",
    "    moments = cv2.moments(image)\n",
    "\n",
    "    # Calculate Hu Moments\n",
    "    huMoments = cv2.HuMoments(moments)\n",
    "    huMoments = huMoments[:, 0]\n",
    "\n",
    "    import math\n",
    "    for i in range(len(huMoments)):\n",
    "        huMoments[i] = -1* math.copysign(1.0, huMoments[i]) * math.log10(abs(huMoments[i]))\n",
    "\n",
    "    return huMoments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image, label = b1_dataset.__getitem__(random.randint(0, len(b1_dataset)), size=SIZE, normalize=NORMALIZE)\n",
    "plt.imshow(image, cmap='gray')\n",
    "label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = []\n",
    "labels = []\n",
    "\n",
    "# Add experiment B1\n",
    "for idx in range(len(b1_dataset)):\n",
    "    image, label = b1_dataset.__getitem__(idx, size=SIZE, normalize=NORMALIZE)\n",
    "    images.append(image.flatten())\n",
    "    labels.append(label)\n",
    "\n",
    "# Add experiment B2\n",
    "for idx in range(len(b1_dataset)):\n",
    "    image, label = b1_dataset.__getitem__(idx, size=SIZE, normalize=NORMALIZE)\n",
    "    images.append(image.flatten())\n",
    "    labels.append(label)\n",
    "\n",
    "images = np.array(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KNN\n",
    "knn = KNeighborsClassifier(n_neighbors=3)\n",
    "knn.fit(images, labels)\n",
    "knn.score(images, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LDA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "lda = LinearDiscriminantAnalysis(n_components=3)\n",
    "lda.fit(images, labels)\n",
    "lda.score(images, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Create the pipeline\n",
    "SPRF = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('pca', PCA(n_components=3)),\n",
    "    ('classifier', RandomForestClassifier(random_state=RANDOM_STATE))\n",
    "])\n",
    "\n",
    "MPRF = Pipeline([\n",
    "    ('scaler', MinMaxScaler()),\n",
    "    ('pca', PCA(n_components=3)),\n",
    "    ('classifier', RandomForestClassifier(random_state=RANDOM_STATE))\n",
    "])\n",
    "\n",
    "SLRF = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('pca', LinearDiscriminantAnalysis(n_components=3)),\n",
    "    ('classifier', RandomForestClassifier(random_state=RANDOM_STATE))\n",
    "])\n",
    "\n",
    "MLRF = Pipeline([\n",
    "    ('scaler', MinMaxScaler()),\n",
    "    ('pca', LinearDiscriminantAnalysis(n_components=3)),\n",
    "    ('classifier', RandomForestClassifier(random_state=RANDOM_STATE))\n",
    "])\n",
    "\n",
    "# Fit the pipeline\n",
    "SPRF.fit(images, labels)\n",
    "MPRF.fit(images, labels)\n",
    "SLRF.fit(images, labels)\n",
    "MLRF.fit(images, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_val = []\n",
    "labels_val = []\n",
    "\n",
    "# Add experiment B1\n",
    "for idx in range(len(b3_dataset)):\n",
    "    image, label = b3_dataset.__getitem__(idx, size=SIZE, normalize=NORMALIZE)\n",
    "    images_val.append(image.flatten())\n",
    "    labels_val.append(label)\n",
    "\n",
    "models = ['knn', 'lda', 'SPRF', 'MPRF', 'SLRF', 'MLRF']\n",
    "for model in models:\n",
    "    print(f\"{model} validation accuracy: {eval(model).score(images_val, labels_val)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA, IncrementalPCA\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "pca = PCA()\n",
    "pca.fit(images)\n",
    "\n",
    "# Getting the cumulative variance\n",
    "\n",
    "var_cumu = np.cumsum(pca.explained_variance_ratio_)*100\n",
    "\n",
    "# How many PCs explain 95% of the variance?\n",
    "k = np.argmax(var_cumu>95)\n",
    "print(\"Number of components explaining 95% variance: \"+ str(k))\n",
    "#print(\"\\n\")\n",
    "\n",
    "plt.figure(figsize=[10,5])\n",
    "plt.title('Cumulative Explained Variance explained by the components')\n",
    "plt.ylabel('Cumulative Explained variance')\n",
    "plt.xlabel('Principal components')\n",
    "plt.axvline(x=k, color=\"k\", linestyle=\"--\")\n",
    "plt.axhline(y=95, color=\"r\", linestyle=\"--\")\n",
    "ax = plt.plot(var_cumu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = PCA(n_components=k).fit_transform(images)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(images, labels, test_size=0.2, stratify=labels, random_state=RANDOM_STATE)\n",
    "\n",
    "clf = LogisticRegression(random_state=RANDOM_STATE)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "print(f\"Model Training Accuracy: {clf.score(X_train, y_train)}\")\n",
    "print(f\"Model Test Accuracy: {clf.score(X_test, y_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = []\n",
    "labels = []\n",
    "\n",
    "# Add experiment B1\n",
    "for idx in range(len(b3_dataset)):\n",
    "    image, label = b3_dataset.__getitem__(idx, size=SIZE, normalize=NORMALIZE)\n",
    "    images.append(image.flatten())\n",
    "    labels.append(label)\n",
    "\n",
    "images = PCA(n_components=k).fit_transform(images)\n",
    "print(f\"Model Validation Accuracy: {clf.score(images, labels)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_stores = {}\n",
    "# for batch, experiment in enumerate([b1_dataset, b2_dataset, b3_dataset]):\n",
    "#     data_stores[f\"B{batch+1}\"] = {\n",
    "#         'images': [],\n",
    "#         'images_train': [],\n",
    "#         'images_test': [],\n",
    "#         'labels': [],\n",
    "#         'labels_train': [],\n",
    "#         'labels_test': []\n",
    "#     }\n",
    "\n",
    "#     for idx in range(len(experiment)):\n",
    "#         image, label = experiment.__getitem__(idx, size=SIZE, normalize=False)\n",
    "#         data_stores[f\"B{batch+1}\"]['images'].append(image.flatten())\n",
    "#         data_stores[f\"B{batch+1}\"]['labels'].append(convert_level(label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Store Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataStores:\n",
    "    def __init__(self, batchs=[\"B1\", \"B2\", \"B3\"], pca_model=PCA(), n_components=100, normalize=False):\n",
    "        self.batchs = batchs\n",
    "        self.experiments = {}\n",
    "        self.pca_model = pca_model\n",
    "        self.n_components = n_components\n",
    "        self.normalize = normalize\n",
    "\n",
    "        #\n",
    "        self._load_data_()\n",
    "\n",
    "    def add_experiment(self, batch: str):\n",
    "        self.experiments[batch] = {\n",
    "            'images': [],\n",
    "            'images_train': [],\n",
    "            'images_test': [],\n",
    "            'labels': [],\n",
    "            'labels_train': [],\n",
    "            'labels_test': []\n",
    "        }\n",
    "\n",
    "    def process_image_hu_moments(self, image: np.ndarray):\n",
    "        if not isinstance(image, np.ndarray) or image.ndim != 2:\n",
    "            raise ValueError(\"Input must be a 2D numpy array (grayscale image).\")\n",
    "        \n",
    "        # Calculate Moments\n",
    "        moments = cv2.moments(image)\n",
    "        \n",
    "        # Calculate Hu Moments\n",
    "        hu_moments = cv2.HuMoments(moments).flatten()\n",
    "        \n",
    "        # Process Hu Moments using vectorized operations\n",
    "        hu_moments = -1 * np.sign(hu_moments) * np.log10(np.abs(hu_moments) + 1e-10)\n",
    "        \n",
    "        return hu_moments\n",
    "\n",
    "    def process_image(self, image: np.ndarray):\n",
    "        features = np.array([])\n",
    "        features = np.append(features, image.flatten())\n",
    "\n",
    "        # Feature reduction - PCA\n",
    "        features = np.append(features, self.pca_model.transform([image.flatten()]))\n",
    "\n",
    "        # Calculate Moments\n",
    "        features = np.append(features, self.process_image_hu_moments(image))\n",
    "\n",
    "        features = np.append(features, np.mean(image))\n",
    "        features = np.append(features, np.max(image))\n",
    "        features = np.append(features, np.median(image))\n",
    "        features = np.append(features, np.sum(image))\n",
    "        return features\n",
    "    \n",
    "    def process_label(self, label: str):\n",
    "        try:\n",
    "            # Extract the number after \"level_\"\n",
    "            level_number = int(label.split('_')[1])\n",
    "            \n",
    "            # Convert to two-digit string\n",
    "            return int(f\"{level_number:02d}\")\n",
    "        except (IndexError, ValueError):\n",
    "            return \"Invalid input\"\n",
    "\n",
    "    def add_data(self, experiment_name: str, image: np.ndarray, label: str, case: str = \"\"):\n",
    "        if experiment_name not in self.experiments:\n",
    "            self.add_experiment(experiment_name)\n",
    "\n",
    "        images_store = \"images\"\n",
    "        labels_store = \"labels\"\n",
    "\n",
    "        if case:\n",
    "            images_store = f\"images_{case}\"\n",
    "            labels_store = f\"labels_{case}\"\n",
    "\n",
    "        self.experiments[experiment_name][images_store].append(self.process_image(image))\n",
    "        self.experiments[experiment_name][labels_store].append(self.process_label(label))\n",
    "\n",
    "    def combine_data(self, experiment1: str, experiment2: str):\n",
    "        if experiment1 not in self.experiments or experiment2 not in self.experiments:\n",
    "            return \"Not valid combination\"\n",
    "        \n",
    "        \n",
    "        experiment_name = f\"{experiment1}_{experiment2}\"\n",
    "        # Initialize experiment template\n",
    "        if experiment_name not in self.experiments:\n",
    "            self.add_experiment(experiment_name)\n",
    "\n",
    "        for key in self.experiments[experiment_name].keys():\n",
    "            self.experiments[experiment_name][key] = self.experiments[experiment1][key] +  self.experiments[experiment2][key]\n",
    "\n",
    "    def _load_data_(self):\n",
    "        for batch in self.batchs:\n",
    "            experiment_data = MotionHistoryImage(experiment=batch)\n",
    "\n",
    "            # Load all data\n",
    "            for idx in range(len(experiment_data)):\n",
    "                image, label = experiment_data.__getitem__(idx, size=SIZE, normalize=self.normalize)\n",
    "                self.add_data(experiment_name=batch, image=image, label=label)\n",
    "\n",
    "            # Splitting data\n",
    "            data_size = len(experiment_data)\n",
    "            train, test = train_test_split(\n",
    "                np.arange(data_size), \n",
    "                test_size=0.2, \n",
    "                random_state=RANDOM_STATE, \n",
    "                stratify=experiment_data.labels\n",
    "            )\n",
    "\n",
    "            # Train data\n",
    "            for idx in train:\n",
    "                image, label = experiment_data.__getitem__(idx, size=SIZE, normalize=self.normalize)\n",
    "                self.add_data(experiment_name=batch, image=image, label=label, case='train')\n",
    "\n",
    "            # Test data\n",
    "            for idx in test:\n",
    "                image, label = experiment_data.__getitem__(idx, size=SIZE, normalize=self.normalize)\n",
    "                self.add_data(experiment_name=batch, image=image, label=label, case='test')\n",
    "\n",
    "\n",
    "    def get_experiment(self, experiment_name: str):\n",
    "        return self.experiments.get(experiment_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_label(label: str):\n",
    "    try:\n",
    "        # Extract the number after \"level_\"\n",
    "        level_number = int(label.split('_')[1])\n",
    "        \n",
    "        # Convert to two-digit string\n",
    "        return int(f\"{level_number:02d}\")\n",
    "    except (IndexError, ValueError):\n",
    "        return \"Invalid input\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = []\n",
    "labels = {}\n",
    "\n",
    "# Add experiment B1\n",
    "for idx in range(len(b2_dataset)):\n",
    "    image, label = b2_dataset.__getitem__(idx, size=SIZE, normalize=NORMALIZE)\n",
    "    images.append(image.flatten())\n",
    "    labels[idx] = convert_label(label)\n",
    "\n",
    "labels = pd.DataFrame.from_dict(labels, orient='index', columns=['label'])\n",
    "labels.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # under sample\n",
    "# df_minority = labels[labels['label']==8]\n",
    "\n",
    "# new_df = pd.concat([df_minority, df_minority, df_minority])\n",
    "# for i in range(11):\n",
    "#     if i != 8:\n",
    "#         undersample = labels[labels['label']==i].sample(n=len(df_minority)*3, replace=True, random_state=RANDOM_STATE)\n",
    "#         new_df = pd.concat([new_df, undersample])\n",
    "\n",
    "for i in range(11):\n",
    "    if i==8 or i==7 or i==9 or i==6:\n",
    "        undersample = labels[labels['label']==i].sample(n=100, replace=True, random_state=RANDOM_STATE)\n",
    "        new_df = pd.concat([labels, undersample])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_train = []\n",
    "labels_train = []\n",
    "for idx in new_df.index:\n",
    "    image, label = b2_dataset.__getitem__(idx, size=SIZE, normalize=NORMALIZE)\n",
    "    images_train.append(image.flatten())\n",
    "    labels_train.append(label)\n",
    "\n",
    "# Fit the pipeline\n",
    "SPRF.fit(images_train, labels_train)\n",
    "MPRF.fit(images_train, labels_train)\n",
    "SLRF.fit(images_train, labels_train)\n",
    "MLRF.fit(images_train, labels_train)\n",
    "\n",
    "images_test = []\n",
    "labels_test = []\n",
    "\n",
    "# Add experiment B1\n",
    "for idx in range(len(b2_dataset)):\n",
    "    image, label = b2_dataset.__getitem__(idx, size=SIZE, normalize=NORMALIZE)\n",
    "    images_test.append(image.flatten())\n",
    "    labels_test.append(label)\n",
    "\n",
    "models = ['knn', 'lda', 'SPRF', 'MPRF', 'SLRF', 'MLRF']\n",
    "for model in models:\n",
    "    print(f\"{model} training accuracy: {eval(model).score(images_train, labels_train)}\")\n",
    "    print(f\"{model} test accuracy: {eval(model).score(images_test, labels_test)}\")\n",
    "    print(f\"{model} validation accuracy: {eval(model).score(images_val, labels_val)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = []\n",
    "labels = []\n",
    "\n",
    "# Add experiment B1\n",
    "for idx in range(len(b1_dataset)):\n",
    "    image, label = b1_dataset.__getitem__(idx, size=SIZE, normalize=NORMALIZE)\n",
    "    images.append(image.flatten())\n",
    "    labels.append(label)\n",
    "\n",
    "# Add experiment B2\n",
    "for idx in range(len(b1_dataset)):\n",
    "    image, label = b1_dataset.__getitem__(idx, size=SIZE, normalize=NORMALIZE)\n",
    "    images.append(image.flatten())\n",
    "    labels.append(label)\n",
    "\n",
    "images = np.array(images)\n",
    "pca = PCA(n_components=2)\n",
    "pca.fit(images)\n",
    "\n",
    "# release memory\n",
    "images = []\n",
    "labels = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datas = DataStores(pca_model=pca, n_components=2, normalize=NORMALIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training: Data Preparation On Batch 1 Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting data to train, test and validation\n",
    "train, test = train_test_split(np.arange(len(b1_dataset)), test_size=0.2, random_state=RANDOM_STATE, stratify=b1_dataset.labels)\n",
    "print(f\"The data in train: {len(train)} \\nThe data in test: {len(test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot 1: Start Frame Distribution (Histogram)\n",
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(18, 6))\n",
    "\n",
    "sns.histplot(train, ax=axes[0])\n",
    "axes[0].set_title(\"Distribution for train sets\")\n",
    "sns.histplot(test, ax=axes[1])\n",
    "axes[1].set_title(\"Distribution for test sets\")\n",
    "\n",
    "# Adjust layout (optional)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import accuracy_score, balanced_accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.metrics import classification_report, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class ClassificationModel:\n",
    "    def __init__(self, model: BaseEstimator, model_name: str):\n",
    "        self.model = model\n",
    "        self.model_name = model_name\n",
    "        self.is_trained = False\n",
    "\n",
    "    def train(self, data_store, experiment_name):\n",
    "        experiment = data_store.get_experiment(experiment_name)\n",
    "        if experiment is None:\n",
    "            raise ValueError(f\"Experiment '{experiment_name}' not found in the data store.\")\n",
    "\n",
    "        X_train = experiment['images_train']\n",
    "        y_train = experiment['labels_train']\n",
    "\n",
    "        self.model.fit(X_train, y_train)\n",
    "        self.is_trained = True\n",
    "        self.last_trained_experiment = experiment_name\n",
    "\n",
    "        train_accuracy = self.model.score(X_train, y_train)\n",
    "        print(f\"{self.model_name} Training accuracy: {train_accuracy:.4f}\")\n",
    "\n",
    "    def evaluate(self, data_store, experiment_name):\n",
    "        if not self.is_trained:\n",
    "            raise ValueError(\"Model hasn't been trained yet. Call train() first.\")\n",
    "\n",
    "        experiment = data_store.get_experiment(experiment_name)\n",
    "        if experiment is None:\n",
    "            raise ValueError(f\"Experiment '{experiment_name}' not found in the data store.\")\n",
    "\n",
    "        X_test = experiment['images_test']\n",
    "        y_test = experiment['labels_test']\n",
    "\n",
    "        test_accuracy = self.model.score(X_test, y_test)\n",
    "        y_pred = self.model.predict(X_test)\n",
    "\n",
    "        print(f\"{self.model_name} Test accuracy: {test_accuracy:.4f}\")\n",
    "        print(\"\\n-----CLASSIFICATION REPORT-----\\n\")\n",
    "        print(classification_report(y_test, y_pred))\n",
    "\n",
    "        return y_test, y_pred\n",
    "\n",
    "    def plot_confusion_matrix(self, y_true, y_pred):\n",
    "        disp = ConfusionMatrixDisplay.from_predictions(y_true, y_pred, xticks_rotation='vertical')\n",
    "        disp.figure_.suptitle(f\"{self.model_name} Confusion Matrix\")\n",
    "        plt.show()\n",
    "\n",
    "    def run_full_analysis(self, data_store, experiment_name):\n",
    "        self.train(data_store, experiment_name)\n",
    "        y_true, y_pred = self.evaluate(data_store, experiment_name)\n",
    "        self.plot_confusion_matrix(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression\n",
    "lr_model = ClassificationModel(\n",
    "    LogisticRegression(random_state=RANDOM_STATE, max_iter=10000),\n",
    "    \"Logistic Regression\"\n",
    ")\n",
    "lr_model.run_full_analysis(datas, 'B1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Issues\n",
    "Some of the \"level abnormal\" are not present in batch 1 experiment, therefore it is not suitable to be used as training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting Countplot\n",
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(18, 6))\n",
    "\n",
    "sns.countplot(x=datas.get_experiment('B1')['labels_train'], ax=axes[0])\n",
    "axes[0].set_title(\"Distribution for train sets\")\n",
    "sns.countplot(x=datas.get_experiment('B1')['labels_test'], ax=axes[1])\n",
    "axes[1].set_title(\"Distribution for test sets\")\n",
    "\n",
    "# Adjust layout (optional)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training: Data Preparation On Batch 2 Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting data to train, test and validation\n",
    "train, test = train_test_split(np.arange(len(b2_dataset)), test_size=0.2, random_state=RANDOM_STATE, stratify=b2_dataset.labels)\n",
    "print(f\"The data in train: {len(train)} \\nThe data in test: {len(test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot 1: Start Frame Distribution (Histogram)\n",
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(18, 6))\n",
    "\n",
    "sns.histplot(train, ax=axes[0])\n",
    "axes[0].set_title(\"Distribution for train sets\")\n",
    "sns.histplot(test, ax=axes[1])\n",
    "axes[1].set_title(\"Distribution for test sets\")\n",
    "\n",
    "# Adjust layout (optional)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression\n",
    "lr_model.run_full_analysis(datas, 'B2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting Countplot\n",
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(18, 6))\n",
    "\n",
    "sns.countplot(x=datas.get_experiment('B2')['labels_train'], ax=axes[0])\n",
    "axes[0].set_title(\"Distribution for train sets\")\n",
    "sns.countplot(x=datas.get_experiment('B2')['labels_test'], ax=axes[1])\n",
    "axes[1].set_title(\"Distribution for test sets\")\n",
    "\n",
    "# Adjust layout (optional)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training: Data Preparation On Batch 3 Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting data to train, test and validation\n",
    "train, test = train_test_split(np.arange(len(b3_dataset)), test_size=0.2, random_state=RANDOM_STATE, stratify=b3_dataset.labels)\n",
    "print(f\"The data in train: {len(train)} \\nThe data in test: {len(test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot 1: Start Frame Distribution (Histogram)\n",
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(18, 6))\n",
    "\n",
    "sns.histplot(train, ax=axes[0])\n",
    "axes[0].set_title(\"Distribution for train sets\")\n",
    "sns.histplot(test, ax=axes[1])\n",
    "axes[1].set_title(\"Distribution for test sets\")\n",
    "\n",
    "# Adjust layout (optional)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression\n",
    "lr_model.run_full_analysis(datas, 'B3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting Countplot\n",
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(18, 6))\n",
    "\n",
    "sns.countplot(x=datas.get_experiment('B3')['labels_train'], ax=axes[0])\n",
    "axes[0].set_title(\"Distribution for train sets\")\n",
    "sns.countplot(x=datas.get_experiment('B3')['labels_test'], ax=axes[1])\n",
    "axes[1].set_title(\"Distribution for test sets\")\n",
    "\n",
    "# Adjust layout (optional)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Selection\n",
    "\n",
    "### Training Using Batch 1 and Validation with Batch 1,2 & 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the base models\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(random_state=RANDOM_STATE, max_iter=1000),\n",
    "    'SGD Classifier': SGDClassifier(random_state=RANDOM_STATE),\n",
    "    'Decision Tree': DecisionTreeClassifier(random_state=RANDOM_STATE),\n",
    "    'Random Forest': RandomForestClassifier(random_state=RANDOM_STATE),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(random_state=RANDOM_STATE),\n",
    "    'AdaBoost': AdaBoostClassifier(random_state=RANDOM_STATE, algorithm='SAMME'),\n",
    "    'K-Nearest Neighbors': KNeighborsClassifier(),\n",
    "    'Support Vector Machine': SVC(random_state=RANDOM_STATE),\n",
    "    'Gaussian Naive Bayes': GaussianNB(),\n",
    "    'Multi-layer Perceptron': MLPClassifier(random_state=RANDOM_STATE)\n",
    "    # 'XGBoost': xgb.XGBClassifier()\n",
    "}\n",
    "\n",
    "# Evaluate the performance of each base model\n",
    "def train_and_evaluate(name, model, images_train, labels_train, images_test, labels_test):\n",
    "    le = LabelEncoder()\n",
    "    labels_train = le.fit_transform((labels_train))\n",
    "    model.fit(images_train, labels_train)\n",
    "    label_predict = model.predict(images_test)\n",
    "    label_predict = le.inverse_transform(label_predict)\n",
    "    accuracy = accuracy_score(labels_test, label_predict)\n",
    "    f1 = f1_score(labels_test, label_predict, average='weighted', zero_division=0.0)\n",
    "    precision = precision_score(labels_test, label_predict, average='weighted', zero_division=0.0)\n",
    "    recall = recall_score(labels_test, label_predict, average='weighted', zero_division=0.0)\n",
    "    return [name, accuracy, f1, precision, recall]\n",
    "\n",
    "def model_selection(experiment='B1'):\n",
    "    results = []\n",
    "    with tqdm(total=len(models), desc='Model Training', unit='model') as progress:\n",
    "        with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:\n",
    "            futures = []\n",
    "            for name, model in models.items():\n",
    "                future = executor.submit(\n",
    "                    train_and_evaluate, \n",
    "                    name, \n",
    "                    model, \n",
    "                    datas.get_experiment(experiment)['images_train'],\n",
    "                    datas.get_experiment(experiment)['labels_train'],\n",
    "                    datas.get_experiment(experiment)['images_test'],\n",
    "                    datas.get_experiment(experiment)['labels_test']\n",
    "                )\n",
    "                futures.append(future)\n",
    "\n",
    "            for future in concurrent.futures.as_completed(futures):\n",
    "                results.append(future.result())\n",
    "                future.add_done_callback(lambda p: progress.update())\n",
    "    \n",
    "    return results\n",
    "\n",
    "# run baseline selector\n",
    "results = model_selection(experiment='B1')\n",
    "\n",
    "# Sort the results by F1-score in descending order\n",
    "results = sorted(results, key=lambda x: x[2], reverse=True)\n",
    "\n",
    "# Print the results\n",
    "print(\"Model Performance:\")\n",
    "print(\"+-----------------------+----------+----------+----------+----------+\")\n",
    "print(\"| Model                 | Accuracy | F1-score | Precision | Recall   |\")\n",
    "print(\"+-----------------------+----------+----------+----------+----------+\")\n",
    "for result in results:\n",
    "    print(\"| {:<20} | {:.4f}   | {:.4f}   | {:.4f}   | {:.4f}   |\".format(*result))\n",
    "print(\"+-----------------------+----------+----------+----------+----------+\")\n",
    "\n",
    "# Choose the best model based on the results\n",
    "best_model = models[results[0][0]]\n",
    "print(f\"The best model is {results[0][0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ValidationAllExperiment(model, data_stores, experiment=['B1', 'B2', 'B3']):\n",
    "    for batch in experiment:\n",
    "        y_pred = model.predict(data_stores.get_experiment(batch)['images'])\n",
    "        print(f\"Running Validation for Batch {batch}\")\n",
    "        print(f\"Model accuracy: {model.score(data_stores.get_experiment(batch)['images'], data_stores.get_experiment(batch)['labels'])}\")\n",
    "        print(f\"\\n-----CLASSIFICATION REPORT BATCH {batch}-----\\n\", classification_report(data_stores.get_experiment(batch)['labels'], y_pred, zero_division=0.0))\n",
    "        disp = ConfusionMatrixDisplay.from_predictions(data_stores.get_experiment(batch)['labels'], y_pred, xticks_rotation='vertical')\n",
    "        # print(f\"Confusion matrix:\\n{disp.confusion_matrix}\")\n",
    "        disp.figure_.suptitle(f\"Confusion Matrix Batch {batch}\")\n",
    "        plt.show()\n",
    "\n",
    "ValidationAllExperiment(model=best_model, data_stores=datas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Using Batch 2 and Validation with Batch 1,2 & 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run baseline selector\n",
    "results = model_selection(experiment='B2')\n",
    "\n",
    "# Sort the results by F1-score in descending order\n",
    "results = sorted(results, key=lambda x: x[2], reverse=True)\n",
    "\n",
    "# Print the results\n",
    "print(\"Model Performance:\")\n",
    "print(\"+-----------------------+----------+----------+----------+----------+\")\n",
    "print(\"| Model                 | Accuracy | F1-score | Precision | Recall   |\")\n",
    "print(\"+-----------------------+----------+----------+----------+----------+\")\n",
    "for result in results:\n",
    "    print(\"| {:<20} | {:.4f}   | {:.4f}   | {:.4f}   | {:.4f}   |\".format(*result))\n",
    "print(\"+-----------------------+----------+----------+----------+----------+\")\n",
    "\n",
    "# Choose the best model based on the results\n",
    "best_model = models[results[0][0]]\n",
    "print(f\"The best model is {results[0][0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ValidationAllExperiment(model=best_model, data_stores=datas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Using Batch 3 and Validation with Batch 1,2 & 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run baseline selector\n",
    "results = model_selection(experiment='B3')\n",
    "\n",
    "# Sort the results by F1-score in descending order\n",
    "results = sorted(results, key=lambda x: x[2], reverse=True)\n",
    "\n",
    "# Print the results\n",
    "print(\"Model Performance:\")\n",
    "print(\"+-----------------------+----------+----------+----------+----------+\")\n",
    "print(\"| Model                 | Accuracy | F1-score | Precision | Recall   |\")\n",
    "print(\"+-----------------------+----------+----------+----------+----------+\")\n",
    "for result in results:\n",
    "    print(\"| {:<20} | {:.4f}   | {:.4f}   | {:.4f}   | {:.4f}   |\".format(*result))\n",
    "print(\"+-----------------------+----------+----------+----------+----------+\")\n",
    "\n",
    "# Choose the best model based on the results\n",
    "best_model = models[results[0][0]]\n",
    "print(f\"The best model is {results[0][0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ValidationAllExperiment(model=best_model, data_stores=datas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Using Batch 1&2 and Validation with Batch 1,2 & 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datas.combine_data('B1', 'B2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run baseline selector\n",
    "results = model_selection(experiment='B1_B2')\n",
    "\n",
    "# Sort the results by F1-score in descending order\n",
    "results = sorted(results, key=lambda x: x[2], reverse=True)\n",
    "\n",
    "# Print the results\n",
    "print(\"Model Performance:\")\n",
    "print(\"+-----------------------+----------+----------+----------+----------+\")\n",
    "print(\"| Model                 | Accuracy | F1-score | Precision | Recall   |\")\n",
    "print(\"+-----------------------+----------+----------+----------+----------+\")\n",
    "for result in results:\n",
    "    print(\"| {:<20} | {:.4f}   | {:.4f}   | {:.4f}   | {:.4f}   |\".format(*result))\n",
    "print(\"+-----------------------+----------+----------+----------+----------+\")\n",
    "\n",
    "# Choose the best model based on the results\n",
    "best_model = models[results[0][0]]\n",
    "print(f\"The best model is {results[0][0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ValidationAllExperiment(model=best_model, data_stores=datas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.feature_selection import SelectKBest, f_classif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in range(1, 10):\n",
    "\n",
    "    # Create the pipeline\n",
    "    pipeline = Pipeline([\n",
    "        # ('scaler', StandardScaler()),\n",
    "        ('feature_selection', SelectKBest(f_classif, k=k)),  # Select top 10 features\n",
    "        ('classifier', VotingClassifier(\n",
    "            estimators=[\n",
    "                # Create base classifier\n",
    "                ('lr', LogisticRegression(random_state=RANDOM_STATE, max_iter=10000)),\n",
    "                ('svm', SVC(random_state=RANDOM_STATE)),\n",
    "                ('sgd', SGDClassifier(random_state=RANDOM_STATE)),\n",
    "                ('rf', RandomForestClassifier(random_state=RANDOM_STATE)),\n",
    "                ('dt', DecisionTreeClassifier(random_state=RANDOM_STATE)),\n",
    "                ('gb', GradientBoostingClassifier(random_state=RANDOM_STATE))\n",
    "            ],\n",
    "            voting='hard'\n",
    "        ))\n",
    "    ])\n",
    "\n",
    "    # Fit the pipeline\n",
    "    pipeline.fit(datas.experiments['B1_B2']['images_train'], datas.experiments['B1_B2']['labels_train'])\n",
    "\n",
    "    train_score = pipeline.score(datas.experiments['B1_B2']['images_train'], datas.experiments['B1_B2']['labels_train'])\n",
    "    test_score = pipeline.score(datas.experiments['B1_B2']['images_test'], datas.experiments['B1_B2']['labels_test'])\n",
    "    val_score = pipeline.score(datas.experiments['B3']['images'], datas.experiments['B3']['labels'])\n",
    "\n",
    "    print(f\"---- Features: {k}\")\n",
    "    print(f\"Train score: {train_score}\")\n",
    "    print(f\"Test score: {test_score}\")\n",
    "    print(f\"Validation score: {val_score}\")\n",
    "    print(f\"---- End of evaluation {k} features \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ValidationAllExperiment(model=pipeline, data_stores=datas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skopt import BayesSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'Logistic Regression': {\n",
    "        'C': [0.0001, 0.001, 0.01, 0.1, 1.0],\n",
    "        'penalty': ['l2', 'l1', 'elasticnet'],\n",
    "        'l1_ratio': [0.1, 0.3, 0.5, 0.7, 0.9]\n",
    "    },\n",
    "    'Support Vector Machine': {\n",
    "        'C': [0.0001, 0.001, 0.01, 0.1, 1.0],\n",
    "        'kernel': ['rbf', 'poly', 'sigmoid'],\n",
    "        'gamma': ['scale', 'auto', 0.1, 1]\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuned & Trained On Batch 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tuning_model(model, param_grid, experiment='B1', cv=5, scoring='accuracy', n_jobs=16):\n",
    "    # Create the grid search object\n",
    "    grid_search = BayesSearchCV(model, param_grid, cv=cv, scoring=scoring, n_jobs=n_jobs)\n",
    "\n",
    "    # Fit the grid search model\n",
    "    grid_search.fit(data_stores[experiment]['images_train'], data_stores[experiment]['labels_train'])\n",
    "\n",
    "    # Print the best hyperparameters and the corresponding score\n",
    "    print('Best hyperparameters: ', grid_search.best_params_)\n",
    "    print('Best score: ', grid_search.best_score_)\n",
    "\n",
    "    # Evaluate the model on the test set\n",
    "    test_score = grid_search.best_estimator_.score(data_stores[experiment]['images_test'], data_stores[experiment]['labels_test'])\n",
    "    print('Test set accuracy: ', test_score)\n",
    "    return grid_search.best_estimator_\n",
    "\n",
    "tuned_model = tuning_model(\n",
    "    model=best_model,\n",
    "    param_grid=param_grid['Support Vector Machine'],\n",
    "    experiment='B1',\n",
    "    cv=2,\n",
    "    n_jobs=-1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ValidationAllExperiment(model=tuned_model, data_stores=data_stores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuned & Train On Batch 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuned_model = tuning_model(\n",
    "    model=best_model,\n",
    "    param_grid=param_grid['Support Vector Machine'],\n",
    "    experiment='B2',\n",
    "    cv=2,\n",
    "    n_jobs=50\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ValidationAllExperiment(model=tuned_model, data_stores=data_stores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuned & Train On Batch 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuned_model = tuning_model(\n",
    "    model=best_model,\n",
    "    param_grid=param_grid['Support Vector Machine'],\n",
    "    experiment='B3',\n",
    "    cv=2,\n",
    "    n_jobs=-1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ValidationAllExperiment(model=tuned_model, data_stores=data_stores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "The simple data preprocessing of Human Motion Image (HMI) are sufficient to differentiate between normal and abnormal condition solely due to the reaction of hybrid grouper to the condition. In addition, the model can be further improve by fine tuning model. In addition, we can also increase the validation test.\n",
    "\n",
    "# Future Improvement\n",
    "\n",
    "While HMI shown to be successfuly, we can adept advanced preprocessing in the same domain by using optiflow approach.\n",
    "\n",
    "\n",
    "# Limitation\n",
    "\n",
    "This approach is solely to differentiate between normal and abnormal condition. However, the video possesed a lot more of potential data to be extract such as fish behaviour of fast swim, hovering, ascending vertical and etc.. This approach may be used to extract the behaviour, however, more time is needed to exactly take the frame time of such condition. We also need to consider the number of behaviour done by the number of hybrid grouper."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improvement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "import xgboost as xgb\n",
    "import concurrent.futures\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "\n",
    "#\n",
    "_, labels_train = np.unique(labels_train, return_inverse=True)\n",
    "_, labels_test = np.unique(labels_test, return_inverse=True)\n",
    "\n",
    "# Define the base models\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(),\n",
    "    'SGD Classifier': SGDClassifier(),\n",
    "    'Decision Tree': DecisionTreeClassifier(),\n",
    "    'Random Forest': RandomForestClassifier(),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(),\n",
    "    'AdaBoost': AdaBoostClassifier(),\n",
    "    'K-Nearest Neighbors': KNeighborsClassifier(),\n",
    "    'Support Vector Machine': SVC(),\n",
    "    'Gaussian Naive Bayes': GaussianNB(),\n",
    "    'XGBoost': xgb.XGBClassifier()\n",
    "}\n",
    "\n",
    "# Evaluate the performance of each base model\n",
    "results = []\n",
    "# for name, model in models.items():\n",
    "#     model.fit(images_train, labels_train)\n",
    "#     label_predict = model.predict(images_test)\n",
    "#     accuracy = accuracy_score(labels_test, label_predict)\n",
    "#     f1 = f1_score(labels_test, label_predict, average='micro')\n",
    "#     precision = precision_score(labels_test, label_predict, average='micro')\n",
    "#     recall = recall_score(labels_test, label_predict, average='micro')\n",
    "#     results.append([name, accuracy, f1, precision, recall])\n",
    "\n",
    "def train_and_evaluate(name, model, images_train, labels_train, images_test, labels_test):\n",
    "    model.fit(images_train, labels_train)\n",
    "    label_predict = model.predict(images_test)\n",
    "    accuracy = accuracy_score(labels_test, label_predict)\n",
    "    f1 = f1_score(labels_test, label_predict, average='micro')\n",
    "    precision = precision_score(labels_test, label_predict, average='micro')\n",
    "    recall = recall_score(labels_test, label_predict, average='micro')\n",
    "    return [name, accuracy, f1, precision, recall]\n",
    "\n",
    "with tqdm(total=len(models), desc='Model Training', unit='model') as progress:\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:\n",
    "        futures = []\n",
    "        for name, model in models.items():\n",
    "            future = executor.submit(train_and_evaluate, name, model, images_train, labels_train, images_test, labels_test)\n",
    "            futures.append(future)\n",
    "\n",
    "        for future in concurrent.futures.as_completed(futures):\n",
    "            results.append(future.result())\n",
    "            future.add_done_callback(lambda p: progress.update())\n",
    "\n",
    "\n",
    "\n",
    "# Sort the results by F1-score in descending order\n",
    "results = sorted(results, key=lambda x: x[2], reverse=True)\n",
    "\n",
    "# Print the results\n",
    "print(\"Model Performance:\")\n",
    "print(\"+-----------------------+----------+----------+----------+----------+\")\n",
    "print(\"| Model                 | Accuracy | F1-score | Precision | Recall   |\")\n",
    "print(\"+-----------------------+----------+----------+----------+----------+\")\n",
    "for result in results:\n",
    "    print(\"| {:<20} | {:.4f}   | {:.4f}   | {:.4f}   | {:.4f}   |\".format(*result))\n",
    "print(\"+-----------------------+----------+----------+----------+----------+\")\n",
    "\n",
    "# Choose the best model based on the results\n",
    "best_model = models[results[0][0]]\n",
    "print(f\"The best model is {results[0][0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'C': [0.0001, 0.001, 0.01, 0.1, 1.0],\n",
    "    'penalty': ['l2', 'l1', 'elasticnet'],\n",
    "    'l1_ratio': [0.1, 0.3, 0.5, 0.7, 0.9]\n",
    "}\n",
    "\n",
    "# Create the Logistic Regression model\n",
    "model = LogisticRegression()\n",
    "\n",
    "# Create the grid search object\n",
    "grid_search = GridSearchCV(model, param_grid=param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "\n",
    "# Fit the grid search model\n",
    "grid_search.fit(images_train, labels_train)\n",
    "\n",
    "# Print the best hyperparameters and the corresponding score\n",
    "print('Best hyperparameters: ', grid_search.best_params_)\n",
    "print('Best score: ', grid_search.best_score_)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_score = grid_search.best_estimator_.score(images_test, labels_test)\n",
    "print('Test set accuracy: ', test_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stratified Fold evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "y = []\n",
    "for idx in range(len(datasets)):\n",
    "    image, label = datasets.__getitem__(idx)\n",
    "    X.append(image.flatten())\n",
    "    y.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# Create a stratified k-fold cross-validation iterator\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Initialize a list to store the accuracy scores\n",
    "accuracy_scores = []\n",
    "\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "\n",
    "# Perform stratified k-fold cross-validation\n",
    "for train_index, test_index in skf.split(X, y):\n",
    "    # Split the data into training and test sets\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    # Train a logistic regression model\n",
    "    model = LogisticRegression(C=0.1, l1_ratio=0.5, penalty='l2')\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Evaluate the model on the test set\n",
    "    y_pred = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    accuracy_scores.append(accuracy)\n",
    "    \n",
    "    print(f\"Fold accuracy: {accuracy:.3f}\")\n",
    "\n",
    "# Calculate the mean accuracy score\n",
    "mean_accuracy = sum(accuracy_scores) / len(accuracy_scores)\n",
    "print(f\"Mean accuracy: {mean_accuracy:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_path = os.path.join(D_PATH, 'data', 'models', 'classification', f\"MHI_SGD_{str(uuid.uuid4())}.joblib\")\n",
    "# joblib.dump(grid_search.best_estimator_, model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = os.path.join(D_PATH, 'data', 'models', 'classification', f\"LR_{str(uuid.uuid4())}.joblib\")\n",
    "joblib.dump(grid_search.best_estimator_, model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = []\n",
    "\n",
    "for model in os.listdir(os.path.join(D_PATH, 'data', 'models', 'classification')):\n",
    "    models.append(os.path.join(D_PATH, 'data', 'models', 'classification', model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo Version 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "videos = ABN_B1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_to_seconds(time_str):\n",
    "    h, m, s = map(int, time_str.split(':'))\n",
    "    return h * 3600 + m * 60 + s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clf = SGDClassifier(alpha=0.1, l1_ratio=0.5, penalty='l2', loss='log_loss')\n",
    "clf = LogisticRegression(C=0.1, l1_ratio=0.5, penalty='l2')\n",
    "clf.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_frames = [\n",
    "    '15'\n",
    "]\n",
    "\n",
    "# clf = joblib.load(models[-1])\n",
    "\n",
    "seconds_length = 60\n",
    "scaler = 0.001\n",
    "\n",
    "video_path = ABN_B1\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "backSub = cv2.createBackgroundSubtractorMOG2(\n",
    "    varThreshold=200\n",
    ")\n",
    "kernel = (5,5)\n",
    "\n",
    "width = 720\n",
    "height = 480\n",
    "text = \"Need more time inspection\"\n",
    "\n",
    "# Get the frame rate of the video\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "# Convert start_time to seconds\n",
    "start_seconds = time_to_seconds(\"00:00:00\")\n",
    "\n",
    "# Calculate the start frame based on start time\n",
    "start_frame = int(start_seconds * fps)\n",
    "\n",
    "# Set the capture time in frames\n",
    "cap.set(cv2.CAP_PROP_POS_FRAMES, start_frame)\n",
    "\n",
    "hist_frames = {}\n",
    "\n",
    "ret, old_frame = cap.read()\n",
    "\n",
    "for fps in list_frames:\n",
    "    hist_frames[fps] = np.zeros(old_frame.shape[:2], dtype=np.uint8)\n",
    "\n",
    "total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "gif_frame = []\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    if not ret:\n",
    "        break\n",
    "    \n",
    "    # get backsubtraction method\n",
    "    bs_mask = tools.get_bs(frame, backSub, kernel)\n",
    "\n",
    "    # get contors\n",
    "    cntrs = tools.get_contours(old_frame, frame, kernel)\n",
    "\n",
    "    # current number of frame\n",
    "    curr_num_frame = cap.get(cv2.CAP_PROP_POS_FRAMES)\n",
    "\n",
    "    for fps in hist_frames.keys():\n",
    "        if curr_num_frame % int(fps) == 0:\n",
    "            # gets holistic background subtraction\n",
    "            # hist_frames[fps] = cv2.add(hist_frames[fps], bs_mask)\n",
    "\n",
    "            # get holistic contours\n",
    "            color = 255 * scaler\n",
    "            cv2.drawContours(hist_frames[fps], cntrs, -1, (color, color, color), 2)\n",
    "            gif_frame.append(hist_frames['15'])\n",
    "\n",
    "        if curr_num_frame % (60*seconds_length) == 0:\n",
    "            image_input = cv2.resize(hist_frames[fps], (224, 224))\n",
    "            image_input = cv2.normalize(image_input, None, -1, 1, cv2.NORM_MINMAX)\n",
    "            image_input = image_input.flatten()\n",
    "            condition = clf.predict_proba([image_input])\n",
    "\n",
    "            if condition.size != 0:\n",
    "                text = \"\".join([f\"ABL{i}:{round(condition[0][i]/1, 2)}\" for i in range(len(condition[0]))])\n",
    "\n",
    "                # text = f\"Condition : Abnormal: {round(condition[0], 5)}, Normal: {round(condition[1], 5)}\"\n",
    "\n",
    "            hist_frames[fps] = np.zeros(old_frame.shape[:2], dtype=np.uint8)\n",
    "            scaler = 0.001\n",
    "\n",
    "    # handle output\n",
    "    cv2.putText(frame, text, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "\n",
    "    win1 = cv2.resize(hist_frames['15'], (width, height))\n",
    "    # win2 = cv2.resize(hist_frames['30'], (width, height))\n",
    "    # win3 = cv2.resize(hist_frames['60'], (width, height))\n",
    "\n",
    "    # display = np.hstack((win1, win2, win3))\n",
    "\n",
    "    scaler += 0.0001\n",
    "\n",
    "    cv2.imshow(\"Original\", cv2.resize(frame, (width, height)))\n",
    "    cv2.imshow(\"Frames Holistic\", win1)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abnormal vs Normal - Old Scenario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = HistoryMotionImage.Fish_Dataset_Binary()\n",
    "len(datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data preparation\n",
    "start_frame = {\n",
    "    'normal':[time_to_seconds(frame) for frame in datasets.get_frame('normal')], \n",
    "    'abnormal':[time_to_seconds(frame) for frame in datasets.get_frame('abnormal')]\n",
    "}\n",
    "\n",
    "# TODO: Dataframe size and dataset size is not the same, need to inspect why software cannot extract all of the time defined, however, the existing data is sufficient for training\n",
    "# start frame for both normal and abnormal video\n",
    "start_frame = pd.DataFrame(start_frame)\n",
    "start_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Below shows the normal distribution of start seconds for each of the condition\n",
    "\n",
    "# Plot 1: Start Frame Distribution (Histogram)\n",
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(12, 6))\n",
    "\n",
    "for col, ax in zip(start_frame.columns, axes.flatten()):\n",
    "    sns.histplot(start_frame[col], ax=ax)\n",
    "    ax.set_title(f\"Distribution of {col} Start Frames\")\n",
    "    ax.set_xlabel('Start Second Value')\n",
    "\n",
    "# Adjust layout (optional)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(np.arange(len(datasets)), test_size=0.3, random_state=RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start Frame Distribution - Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot 1: Start Frame Distribution (Histogram)\n",
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(18, 6))\n",
    "\n",
    "sns.histplot(train, ax=axes[0])\n",
    "axes[0].set_title(\"Distribution for train sets\")\n",
    "sns.histplot(test, ax=axes[1])\n",
    "axes[1].set_title(\"Distribution for test sets\")\n",
    "\n",
    "# Adjust layout (optional)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_train = []\n",
    "labels_train = []\n",
    "for idx in train:\n",
    "    image, label = datasets.__getitem__(idx)\n",
    "    images_train.append(image.flatten())\n",
    "    labels_train.append(label)\n",
    "\n",
    "images_test = []\n",
    "labels_test = []\n",
    "for idx in test:\n",
    "    image, label = datasets.__getitem__(idx)\n",
    "    images_test.append(image.flatten())\n",
    "    labels_test.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(labels_train, return_inverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, labels_train = np.unique(labels_train, return_inverse=True)\n",
    "_, labels_test = np.unique(labels_test, return_inverse=True)\n",
    "\n",
    "results = []\n",
    "\n",
    "# Define the base models\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(),\n",
    "    'SGD Classifier': SGDClassifier(),\n",
    "    'Decision Tree': DecisionTreeClassifier(),\n",
    "    'Random Forest': RandomForestClassifier(),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(),\n",
    "    'AdaBoost': AdaBoostClassifier(),\n",
    "    'K-Nearest Neighbors': KNeighborsClassifier(),\n",
    "    'Support Vector Machine': SVC(),\n",
    "    'Gaussian Naive Bayes': GaussianNB(),\n",
    "    'XGBoost': xgb.XGBClassifier()\n",
    "}\n",
    "\n",
    "with tqdm(total=len(models), desc='Model Training', unit='model') as progress:\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:\n",
    "        futures = []\n",
    "        for name, model in models.items():\n",
    "            future = executor.submit(train_and_evaluate, name, model, images_train, labels_train, images_test, labels_test)\n",
    "            futures.append(future)\n",
    "\n",
    "        for future in concurrent.futures.as_completed(futures):\n",
    "            results.append(future.result())\n",
    "            future.add_done_callback(lambda p: progress.update())\n",
    "\n",
    "# Sort the results by F1-score in descending order\n",
    "results = sorted(results, key=lambda x: x[2], reverse=True)\n",
    "\n",
    "# Print the results\n",
    "print(\"Model Performance:\")\n",
    "print(\"+-----------------------+----------+----------+----------+----------+\")\n",
    "print(\"| Model                 | Accuracy | F1-score | Precision | Recall   |\")\n",
    "print(\"+-----------------------+----------+----------+----------+----------+\")\n",
    "for result in results:\n",
    "    print(\"| {:<20} | {:.4f}   | {:.4f}   | {:.4f}   | {:.4f}   |\".format(*result))\n",
    "print(\"+-----------------------+----------+----------+----------+----------+\")\n",
    "\n",
    "# Choose the best model based on the results\n",
    "best_model = models[results[0][0]]\n",
    "print(f\"The best model is {results[0][0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'C': [0.1, 1, 10, 100],\n",
    "    'gamma': [0.001, 0.01, 0.1, 1],\n",
    "    'kernel': ['linear', 'rbf', 'poly']\n",
    "}\n",
    "\n",
    "# Create the SVM model\n",
    "model = SVC()\n",
    "\n",
    "# Create the grid search object\n",
    "grid_search = GridSearchCV(model, param_grid=param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "\n",
    "# Fit the grid search model\n",
    "grid_search.fit(images_train, labels_train)\n",
    "\n",
    "# Print the best hyperparameters and the corresponding score\n",
    "print('Best hyperparameters: ', grid_search.best_params_)\n",
    "print('Best score: ', grid_search.best_score_)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_score = grid_search.best_estimator_.score(images_test, labels_test)\n",
    "print('Test set accuracy: ', test_score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
