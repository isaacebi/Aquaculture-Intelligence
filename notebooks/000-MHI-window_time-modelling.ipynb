{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import sys\n",
    "import uuid\n",
    "import joblib\n",
    "import random\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import concurrent.futures\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import classification_report, ConfusionMatrixDisplay\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attaching project directory\n",
    "sys.path.append(os.path.dirname(os.getcwd()))\n",
    "\n",
    "# Pathing imports\n",
    "from src import GetPath, FullMHI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize images\n",
    "NORMALIZE = False\n",
    "\n",
    "# Fixed randomness\n",
    "RANDOM_STATE = 42\n",
    "random.seed(RANDOM_STATE)\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "# Model max iteration\n",
    "MAX_ITERM = 1000\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = GetPath().shared_data()\n",
    "\n",
    "MHI_DATA = os.path.join(DATA_PATH, 'preprocess', 'mhi')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassificationModel:\n",
    "    def __init__(self, model: BaseEstimator, model_name: str):\n",
    "        self.model = model\n",
    "        self.model_name = model_name\n",
    "        self.is_trained = False\n",
    "\n",
    "    def train(self, X_train, y_train):\n",
    "        self.model.fit(X_train, y_train)\n",
    "        train_accuracy = self.model.score(X_train, y_train)\n",
    "        print(f\"{self.model_name} Training accuracy: {train_accuracy:.4f}\")\n",
    "\n",
    "    def evaluate(self, X_test, y_test):\n",
    "        test_accuracy = self.model.score(X_test, y_test)\n",
    "        y_pred = self.model.predict(X_test)\n",
    "\n",
    "        print(f\"{self.model_name} Test accuracy: {test_accuracy:.4f}\")\n",
    "        print(\"\\n-----CLASSIFICATION REPORT-----\\n\")\n",
    "        print(classification_report(y_test, y_pred))\n",
    "\n",
    "        return y_test, y_pred\n",
    "\n",
    "    def plot_confusion_matrix(self, y_true, y_pred):\n",
    "        disp = ConfusionMatrixDisplay.from_predictions(y_true, y_pred, xticks_rotation='vertical')\n",
    "        disp.figure_.suptitle(f\"{self.model_name} Confusion Matrix\")\n",
    "        plt.show()\n",
    "\n",
    "    def run_full_analysis(self, X_train, y_train, X_test, y_test):\n",
    "        self.train(X_train, y_train)\n",
    "        y_true, y_pred = self.evaluate(X_test, y_test)\n",
    "        self.plot_confusion_matrix(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the performance of each base model\n",
    "def train_and_evaluate(name, model, images_train, labels_train, images_test, labels_test):\n",
    "    le = LabelEncoder()\n",
    "    labels_train = le.fit_transform((labels_train))\n",
    "    model.fit(images_train, labels_train)\n",
    "    label_predict = model.predict(images_test)\n",
    "    label_predict = le.inverse_transform(label_predict)\n",
    "    accuracy = accuracy_score(labels_test, label_predict)\n",
    "    f1 = f1_score(labels_test, label_predict, average='weighted', zero_division=0.0)\n",
    "    precision = precision_score(labels_test, label_predict, average='weighted', zero_division=0.0)\n",
    "    recall = recall_score(labels_test, label_predict, average='weighted', zero_division=0.0)\n",
    "    return [name, accuracy, f1, precision, recall]\n",
    "\n",
    "def model_selection(models, X_train, y_train, X_test, y_test):\n",
    "    results = []\n",
    "    with tqdm(total=len(models), desc='Model Training', unit='model') as progress:\n",
    "        with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:\n",
    "            futures = []\n",
    "            for name, model in models.items():\n",
    "                future = executor.submit(\n",
    "                    train_and_evaluate, \n",
    "                    name, \n",
    "                    model, \n",
    "                    X_train,\n",
    "                    y_train,\n",
    "                    X_test,\n",
    "                    y_test\n",
    "                )\n",
    "                futures.append(future)\n",
    "\n",
    "            for future in concurrent.futures.as_completed(futures):\n",
    "                results.append(future.result())\n",
    "                future.add_done_callback(lambda p: progress.update())\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training with different time windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting different time windows\n",
    "roots_dir = []\n",
    "for file in os.listdir(MHI_DATA):\n",
    "    file = os.path.join(MHI_DATA, file)\n",
    "    if os.path.isdir(file):\n",
    "        roots_dir.append(os.path.join(MHI_DATA, file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the base models\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(random_state=RANDOM_STATE, max_iter=MAX_ITERM),\n",
    "    'SGD Classifier': SGDClassifier(random_state=RANDOM_STATE),\n",
    "    'Decision Tree': DecisionTreeClassifier(random_state=RANDOM_STATE),\n",
    "    'Random Forest': RandomForestClassifier(random_state=RANDOM_STATE),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(random_state=RANDOM_STATE),\n",
    "    'AdaBoost': AdaBoostClassifier(random_state=RANDOM_STATE, algorithm='SAMME'),\n",
    "    'K-Nearest Neighbors': KNeighborsClassifier(),\n",
    "    'Support Vector Machine': SVC(random_state=RANDOM_STATE),\n",
    "    'Gaussian Naive Bayes': GaussianNB(),\n",
    "    'Multi-layer Perceptron': MLPClassifier(random_state=RANDOM_STATE, max_iter=MAX_ITERM)\n",
    "    # 'XGBoost': xgb.XGBClassifier()\n",
    "}\n",
    "\n",
    "# Image transformation\n",
    "train_transform = A.Compose([\n",
    "        # Image resize\n",
    "        A.Resize(32, 32),\n",
    "        # Image normalize\n",
    "        A.Normalize(mean=(0,0,0), std=(1,1,1),),\n",
    "        # Random flip\n",
    "        A.OneOf([\n",
    "            A.HorizontalFlip(p=0.5),\n",
    "            A.VerticalFlip(p=0.5),\n",
    "        ], p=0.5),\n",
    "        # Random rotate\n",
    "        A.OneOf([\n",
    "            A.RandomRotate90(),\n",
    "            A.ShiftScaleRotate(shift_limit=0.0625, scale_limit=0.50, rotate_limit=45, p=.75),\n",
    "        ], p=0.5),\n",
    "    ])\n",
    "\n",
    "# Image transformation\n",
    "test_transform = A.Compose([\n",
    "        # Image resize\n",
    "        A.Resize(32, 32),\n",
    "        # Image normalize\n",
    "        A.Normalize(mean=(0,0,0), std=(1,1,1),),\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Time_Windows_Selector:\n",
    "    def __init__(self, \n",
    "                 model: BaseEstimator,\n",
    "                 model_name: str, \n",
    "                 dataset_original: list, \n",
    "                 dataset_augmentated: list, \n",
    "                 n_augmentation: int, \n",
    "                 random_state: int):\n",
    "        \n",
    "        self.model = model\n",
    "        self.model_name = model_name\n",
    "        self.dataset_original = dataset_original\n",
    "        self.dataset_augmentated = dataset_augmentated\n",
    "        self.n_augmentation = n_augmentation\n",
    "        self.random_state = random_state\n",
    "\n",
    "        # Template\n",
    "        self.idx_track = {\n",
    "            'B1': [],\n",
    "            'B2': [],\n",
    "            'B3': []\n",
    "        }\n",
    "\n",
    "        self.labels_track = {\n",
    "            'B1': [],\n",
    "            'B2': [],\n",
    "            'B3': []\n",
    "        }\n",
    "\n",
    "        # Assign tracker by experiments\n",
    "        self.__get_tracks__()\n",
    "\n",
    "        # Total dataset\n",
    "        print(f\"Total experiment dataset: {len(dataset_original)}\")\n",
    "\n",
    "    def __get_tracks__(self):\n",
    "        # Assigning index tracker by batch experiment - training\n",
    "        for idx in range(len(self.dataset_original)):\n",
    "            experiment, label, image = self.dataset_original.__getitem__(idx)\n",
    "            self.idx_track[experiment].append(idx)\n",
    "            self.labels_track[experiment].append(label)\n",
    "\n",
    "    def get_sets(self, idx_track: list, labels_track: list, validation_experiment: list):\n",
    "\n",
    "        # Template\n",
    "        X_train = []\n",
    "        y_train = []\n",
    "        X_test = []\n",
    "        y_test = []\n",
    "        X_val = []\n",
    "        y_val = []\n",
    "\n",
    "        # Train and test splitting between experiment B1 & B2, B1 does not have all abnormal level\n",
    "        idx_train, idx_test = train_test_split(\n",
    "            idx_track,\n",
    "            test_size=0.2,\n",
    "            random_state=self.random_state,\n",
    "            stratify=labels_track\n",
    "        )\n",
    "            \n",
    "        # Generate n times sample with different augmentation\n",
    "        for i in range(self.n_augmentation):\n",
    "            for idx in idx_train:\n",
    "                _, label, image = self.dataset_augmentated.__getitem__(idx)\n",
    "                X_train.append(image.flatten())\n",
    "                y_train.append(label)\n",
    "\n",
    "        # Message\n",
    "        print(f\"Total augmentated data for training: {len(X_train)}\")\n",
    "\n",
    "        for idx in idx_test:\n",
    "            _, label, image = self.dataset_original.__getitem__(idx)\n",
    "            X_test.append(image.flatten())\n",
    "            y_test.append(label)\n",
    "\n",
    "        for idx in validation_experiment:\n",
    "            _, label, image = self.dataset_original.__getitem__(idx)\n",
    "            X_val.append(image.flatten())\n",
    "            y_val.append(label)\n",
    "\n",
    "        return X_train, y_train, X_test, y_test, X_val, y_val\n",
    "\n",
    "\n",
    "    def train(self, X_train, y_train):\n",
    "        self.model.fit(X_train, y_train)\n",
    "        train_accuracy = self.model.score(X_train, y_train)\n",
    "        print(f\"{self.model_name} Training accuracy: {train_accuracy:.4f}\")\n",
    "\n",
    "    def evaluate(self, X_test, y_test):\n",
    "        test_accuracy = self.model.score(X_test, y_test)\n",
    "        y_pred = self.model.predict(X_test)\n",
    "\n",
    "        print(f\"{self.model_name} Test accuracy: {test_accuracy:.4f}\")\n",
    "        print(\"\\n-----CLASSIFICATION REPORT-----\\n\")\n",
    "        print(classification_report(y_test, y_pred))\n",
    "\n",
    "        return y_test, y_pred\n",
    "\n",
    "    def plot_confusion_matrix(self, y_true, y_pred):\n",
    "        disp = ConfusionMatrixDisplay.from_predictions(y_true, y_pred, xticks_rotation='vertical')\n",
    "        disp.figure_.suptitle(f\"{self.model_name} Confusion Matrix\")\n",
    "        plt.show()\n",
    "\n",
    "    def run_full_analysis(self):\n",
    "        for batch in ['B2', 'B3']:\n",
    "            if batch == 'B2':\n",
    "                validation_experiment = self.idx_track['B3']\n",
    "            else:\n",
    "                validation_experiment = self.idx_track['B2']\n",
    "\n",
    "            X_train, y_train, X_test, y_test, X_val, y_val = self.get_sets(\n",
    "                idx_track = self.idx_track['B1'] + self.idx_track[batch],\n",
    "                labels_track = self.labels_track['B1'] + self.labels_track[batch],\n",
    "                validation_experiment=validation_experiment\n",
    "            )\n",
    "\n",
    "            print(f\"Conducting training with experiment B1 and {batch}\")\n",
    "            self.train(X_train, y_train)\n",
    "\n",
    "            # Test\n",
    "            print(f\"Evaluation on test sets\")\n",
    "            y_true, y_pred = self.evaluate(X_test, y_test)\n",
    "            self.plot_confusion_matrix(y_true, y_pred)\n",
    "\n",
    "            # Validation\n",
    "            print(f\"Evaluation on validation sets\")\n",
    "            y_true, y_pred = self.evaluate(X_val, y_val)\n",
    "            self.plot_confusion_matrix(y_true, y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selector = Time_Windows_Selector(\n",
    "    model=RandomForestClassifier(random_state=RANDOM_STATE),\n",
    "    model_name=\"Random Forest\",\n",
    "    dataset_original=datasets,\n",
    "    dataset_augmentated=datasetsWithTransform,\n",
    "    n_augmentation=20,\n",
    "    random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "selector.run_full_analysis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate through windows time (seconds)\n",
    "for file in roots_dir:\n",
    "    # Message\n",
    "    print(f\"Experimenting data on folder {file}\")\n",
    "    # Loading datasets with augmentation\n",
    "    datasetsWithTransform = FullMHI(root_dir=file, transform=train_transform)\n",
    "    datasets = FullMHI(root_dir=file, transform=test_transform)\n",
    "    # Message\n",
    "    print(f\"Total experiment datasets: {len(datasets)}\")\n",
    "\n",
    "    # Assigning index tracker for each batch experiment\n",
    "    idx_track = {\n",
    "        'B1': [],\n",
    "        'B2': [],\n",
    "        'B3': []\n",
    "    }\n",
    "\n",
    "    labels_track = {\n",
    "        'B1': [],\n",
    "        'B2': [],\n",
    "        'B3': []\n",
    "    }\n",
    "\n",
    "    # Reasign data\n",
    "    for idx in range(len(datasets)):\n",
    "        experiment, label, image = datasetsWithTransform.__getitem__(idx)\n",
    "        idx_track[experiment].append(idx)\n",
    "        labels_track[experiment].append(label)\n",
    "\n",
    "    # Train and test splitting between experiment B1 & B2, B1 does not have all abnormal level\n",
    "    idx_train, idx_test = train_test_split(\n",
    "        idx_track['B1'] + idx_track['B2'],\n",
    "        test_size=0.2,\n",
    "        random_state=RANDOM_STATE,\n",
    "        stratify=labels_track['B1'] + labels_track['B2']\n",
    "    )\n",
    "\n",
    "    # Training variable\n",
    "    X_train = []\n",
    "    y_train = []\n",
    "\n",
    "    # Generate n times sample with different augmentation\n",
    "    for i in range(3):\n",
    "        for idx in idx_train:\n",
    "            experiment, label, image = datasetsWithTransform.__getitem__(idx)\n",
    "            X_train.append(image.flatten())\n",
    "            y_train.append(label)\n",
    "\n",
    "    # Message\n",
    "    print(f\"Total augmentated data for training: {len(idx_train)}\")\n",
    "\n",
    "    # Test variable\n",
    "    X_test = []\n",
    "    y_test = []\n",
    "\n",
    "    for idx in idx_test:\n",
    "        experiment, label, image = datasets.__getitem__(idx)\n",
    "        X_test.append(image.flatten())\n",
    "        y_test.append(label)\n",
    "\n",
    "    # Validation variable with experiment B3\n",
    "    X_val = []\n",
    "    y_val = []\n",
    "\n",
    "    for idx in idx_track['B3']:\n",
    "        experiment, label, image = datasets.__getitem__(idx)\n",
    "        X_val.append(image.flatten())\n",
    "        y_val.append(label)\n",
    "\n",
    "    # run baseline selector\n",
    "    results = model_selection(models, X_train, y_train, X_test, y_test)\n",
    "\n",
    "    # Sort the results by F1-score in descending order\n",
    "    results = sorted(results, key=lambda x: x[2], reverse=True)\n",
    "\n",
    "    # Print the results\n",
    "    print(\"Model Performance:\")\n",
    "    print(\"+-----------------------+----------+----------+----------+----------+\")\n",
    "    print(\"| Model                 | Accuracy | F1-score | Precision | Recall   |\")\n",
    "    print(\"+-----------------------+----------+----------+----------+----------+\")\n",
    "    for result in results:\n",
    "        print(\"| {:<20} | {:.4f}   | {:.4f}   | {:.4f}   | {:.4f}   |\".format(*result))\n",
    "    print(\"+-----------------------+----------+----------+----------+----------+\")\n",
    "\n",
    "    # Choose the best model based on the results\n",
    "    best_model = models[results[0][0]]\n",
    "    print(f\"The best model is {results[0][0]}\")\n",
    "\n",
    "    # Evaluation\n",
    "    model = ClassificationModel(\n",
    "        best_model,\n",
    "        result[0][0]\n",
    "    )\n",
    "\n",
    "    X_train = []\n",
    "    y_train = []\n",
    "    for i in range(5):\n",
    "        for idx in idx_train + idx_test:\n",
    "            experiment, label, image = datasetsWithTransform.__getitem__(idx)\n",
    "            X_train.append(image.flatten())\n",
    "            y_train.append(label)\n",
    "\n",
    "    model.run_full_analysis(\n",
    "        X_train=X_train,\n",
    "        X_test=X_val,\n",
    "        y_train=y_train,\n",
    "        y_test=y_val\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the autoencoder architecture\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.input_size = input_size\n",
    "\n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(self.input_size, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 16)\n",
    "        )\n",
    "\n",
    "        # Decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(16, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, self.input_size),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Autoencoder\n",
    "# Initialize the autoencoder\n",
    "autoencoder_model = Autoencoder(input_size=32*32).to(device)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(autoencoder_model.parameters())\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "\n",
    "# Image transformation\n",
    "autoencoder_transform = A.Compose([\n",
    "        # Image resize\n",
    "        A.Resize(32, 32),\n",
    "        # Image normalize\n",
    "        A.Normalize(mean=(0,0,0), std=(1,1,1),),\n",
    "        ToTensorV2()\n",
    "    ])\n",
    "\n",
    "# Iterate through windows time (seconds)\n",
    "for file in roots_dir:\n",
    "    # Message\n",
    "    print(f\"Experimenting data on folder {file}\")\n",
    "    datasets = FullMHI(root_dir=file, transform=autoencoder_transform)\n",
    "    train_loader = DataLoader(datasets, batch_size=128, shuffle=True)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        for data in train_loader:\n",
    "            _, _, img = data\n",
    "            img = img.view(img.size(0), -1).to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            output = autoencoder_model(img)\n",
    "            loss = criterion(output, img)\n",
    "\n",
    "            # Backward pass and optimization\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "print(\"Training finished!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder_image(image, input_size, model):\n",
    "    model.eval()\n",
    "    \n",
    "    # Image to GPU\n",
    "    image = torch.from_numpy(image)\n",
    "    image = image.view(-1, input_size).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        encoder_output = model.encoder(image)\n",
    "\n",
    "    return encoder_output.cpu().numpy().flatten()\n",
    "\n",
    "input_size = 32*32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate through windows time (seconds)\n",
    "for file in roots_dir:\n",
    "    # Message\n",
    "    print(f\"Experimenting data on folder {file}\")\n",
    "    # Loading datasets with augmentation\n",
    "    datasetsWithTransform = FullMHI(root_dir=file, transform=train_transform)\n",
    "    datasets = FullMHI(root_dir=file, transform=test_transform)\n",
    "    # Message\n",
    "    print(f\"Total experiment datasets: {len(datasets)}\")\n",
    "\n",
    "    # Assigning index tracker for each batch experiment\n",
    "    idx_track = {\n",
    "        'B1': [],\n",
    "        'B2': [],\n",
    "        'B3': []\n",
    "    }\n",
    "\n",
    "    labels_track = {\n",
    "        'B1': [],\n",
    "        'B2': [],\n",
    "        'B3': []\n",
    "    }\n",
    "\n",
    "    # Reasign data\n",
    "    for idx in range(len(datasets)):\n",
    "        experiment, label, image = datasetsWithTransform.__getitem__(idx)\n",
    "        idx_track[experiment].append(idx)\n",
    "        labels_track[experiment].append(label)\n",
    "\n",
    "    # Train and test splitting between experiment B1 & B2, B1 does not have all abnormal level\n",
    "    idx_train, idx_test = train_test_split(\n",
    "        idx_track['B1'] + idx_track['B2'],\n",
    "        test_size=0.2,\n",
    "        random_state=RANDOM_STATE,\n",
    "        stratify=labels_track['B1'] + labels_track['B2']\n",
    "    )\n",
    "\n",
    "    # Training variable\n",
    "    X_train = []\n",
    "    y_train = []\n",
    "\n",
    "    # Generate n times sample with different augmentation\n",
    "    for i in range(3):\n",
    "        for idx in idx_train:\n",
    "            experiment, label, image = datasetsWithTransform.__getitem__(idx)\n",
    "\n",
    "            # feature extraction\n",
    "            new_feature = encoder_image(image, input_size, autoencoder_model)\n",
    "\n",
    "            # Extracted feature for training\n",
    "            X_train.append(new_feature)\n",
    "            y_train.append(label)\n",
    "\n",
    "    # Message\n",
    "    print(f\"Total augmentated data for training: {len(idx_train)}\")\n",
    "\n",
    "    # Test variable\n",
    "    X_test = []\n",
    "    y_test = []\n",
    "\n",
    "    for idx in idx_test:\n",
    "        experiment, label, image = datasets.__getitem__(idx)\n",
    "\n",
    "        # feature extraction\n",
    "        new_feature = encoder_image(image, input_size, autoencoder_model)\n",
    "\n",
    "        # Extracted feature for testing\n",
    "        X_test.append(new_feature)\n",
    "        y_test.append(label)\n",
    "\n",
    "    # Validation variable with experiment B3\n",
    "    X_val = []\n",
    "    y_val = []\n",
    "\n",
    "    for idx in idx_track['B3']:\n",
    "        experiment, label, image = datasets.__getitem__(idx)\n",
    "\n",
    "        # feature extraction\n",
    "        new_feature = encoder_image(image, input_size, autoencoder_model)\n",
    "\n",
    "        # Extracted feature for validation\n",
    "        X_val.append(new_feature)\n",
    "        y_val.append(label)\n",
    "\n",
    "    # run baseline selector\n",
    "    results = model_selection(models, X_train, y_train, X_test, y_test)\n",
    "\n",
    "    # Sort the results by F1-score in descending order\n",
    "    results = sorted(results, key=lambda x: x[2], reverse=True)\n",
    "\n",
    "    # Print the results\n",
    "    print(\"Model Performance:\")\n",
    "    print(\"+-----------------------+----------+----------+----------+----------+\")\n",
    "    print(\"| Model                 | Accuracy | F1-score | Precision | Recall   |\")\n",
    "    print(\"+-----------------------+----------+----------+----------+----------+\")\n",
    "    for result in results:\n",
    "        print(\"| {:<20} | {:.4f}   | {:.4f}   | {:.4f}   | {:.4f}   |\".format(*result))\n",
    "    print(\"+-----------------------+----------+----------+----------+----------+\")\n",
    "\n",
    "    # Choose the best model based on the results\n",
    "    best_model = models[results[0][0]]\n",
    "    print(f\"The best model is {results[0][0]}\")\n",
    "\n",
    "    # Evaluation\n",
    "    model = ClassificationModel(\n",
    "        best_model,\n",
    "        result[0][0]\n",
    "    )\n",
    "    model.run_full_analysis(\n",
    "        X_train=X_train+X_test,\n",
    "        X_test=X_val,\n",
    "        y_train=y_train+y_test,\n",
    "        y_test=y_val\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create feature names\n",
    "feature_names = [f'feature_{i}' for i in range(X_train[0].shape[0])]\n",
    "\n",
    "# Create a Random Forest model\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Get feature importances\n",
    "importances = rf_model.feature_importances_\n",
    "\n",
    "# Sort feature importances in descending order\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "# Create a DataFrame of feature importances\n",
    "feature_importance_df = pd.DataFrame({\n",
    "    'feature': [feature_names[i] for i in indices],\n",
    "    'importance': importances[indices]\n",
    "})\n",
    "\n",
    "# Plot feature importances\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.title(\"Feature Importances\")\n",
    "plt.bar(range(X_train[0].shape[0]), importances[indices])\n",
    "plt.xticks(range(X_train[0].shape[0]), [feature_names[i] for i in indices], rotation=90)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print feature importances\n",
    "print(feature_importance_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Autoencoder\n",
    "# Initialize the autoencoder\n",
    "autoencoder_model = Autoencoder(input_size=32*32).to(device)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(autoencoder_model.parameters())\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "\n",
    "# Image transformation\n",
    "autoencoder_transform = A.Compose([\n",
    "        # Image resize\n",
    "        A.Resize(32, 32),\n",
    "        # Image normalize\n",
    "        A.Normalize(mean=(0,0,0), std=(1,1,1),),\n",
    "        ToTensorV2()\n",
    "    ])\n",
    "\n",
    "# Iterate through windows time (seconds)\n",
    "for file in roots_dir:\n",
    "    # Message\n",
    "    print(f\"Experimenting data on folder {file}\")\n",
    "    datasets = FullMHI(root_dir=file, transform=autoencoder_transform)\n",
    "    train_loader = DataLoader(datasets, batch_size=128, shuffle=True)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        for data in train_loader:\n",
    "            _, _, img = data\n",
    "            img = img.view(img.size(0), -1).to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            output = autoencoder_model(img)\n",
    "            loss = criterion(output, img)\n",
    "\n",
    "            # Backward pass and optimization\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "    # Loading datasets with augmentation\n",
    "    datasetsWithTransform = FullMHI(root_dir=file, transform=train_transform)\n",
    "    datasets = FullMHI(root_dir=file, transform=test_transform)\n",
    "    # Message\n",
    "    print(f\"Total experiment datasets: {len(datasets)}\")\n",
    "\n",
    "\n",
    "    # Assigning index tracker for each batch experiment\n",
    "    idx_track = {\n",
    "        'B1': [],\n",
    "        'B2': [],\n",
    "        'B3': []\n",
    "    }\n",
    "\n",
    "    labels_track = {\n",
    "        'B1': [],\n",
    "        'B2': [],\n",
    "        'B3': []\n",
    "    }\n",
    "\n",
    "    # Reasign data\n",
    "    for idx in range(len(datasets)):\n",
    "        experiment, label, image = datasetsWithTransform.__getitem__(idx)\n",
    "        idx_track[experiment].append(idx)\n",
    "        labels_track[experiment].append(label)\n",
    "\n",
    "    # Train and test splitting between experiment B1 & B2, B1 does not have all abnormal level\n",
    "    idx_train, idx_test = train_test_split(\n",
    "        idx_track['B1'] + idx_track['B2'],\n",
    "        test_size=0.2,\n",
    "        random_state=RANDOM_STATE,\n",
    "        stratify=labels_track['B1'] + labels_track['B2']\n",
    "    )\n",
    "\n",
    "    # Training variable\n",
    "    X_train = []\n",
    "    y_train = []\n",
    "\n",
    "    # Generate n times sample with different augmentation\n",
    "    for i in range(3):\n",
    "        for idx in idx_train:\n",
    "            experiment, label, image = datasetsWithTransform.__getitem__(idx)\n",
    "\n",
    "            # feature extraction\n",
    "            new_feature = encoder_image(image, input_size, autoencoder_model)\n",
    "\n",
    "            # Extracted feature for training\n",
    "            X_train.append(new_feature)\n",
    "            y_train.append(label)\n",
    "\n",
    "    # Message\n",
    "    print(f\"Total augmentated data for training: {len(idx_train)}\")\n",
    "\n",
    "    # Test variable\n",
    "    X_test = []\n",
    "    y_test = []\n",
    "\n",
    "    for idx in idx_test:\n",
    "        experiment, label, image = datasets.__getitem__(idx)\n",
    "\n",
    "        # feature extraction\n",
    "        new_feature = encoder_image(image, input_size, autoencoder_model)\n",
    "\n",
    "        # Extracted feature for testing\n",
    "        X_test.append(new_feature)\n",
    "        y_test.append(label)\n",
    "\n",
    "    # Validation variable with experiment B3\n",
    "    X_val = []\n",
    "    y_val = []\n",
    "\n",
    "    for idx in idx_track['B3']:\n",
    "        experiment, label, image = datasets.__getitem__(idx)\n",
    "\n",
    "        # feature extraction\n",
    "        new_feature = encoder_image(image, input_size, autoencoder_model)\n",
    "\n",
    "        # Extracted feature for validation\n",
    "        X_val.append(new_feature)\n",
    "        y_val.append(label)\n",
    "\n",
    "    # run baseline selector\n",
    "    results = model_selection(models, X_train, y_train, X_test, y_test)\n",
    "\n",
    "    # Sort the results by F1-score in descending order\n",
    "    results = sorted(results, key=lambda x: x[2], reverse=True)\n",
    "\n",
    "    # Print the results\n",
    "    print(\"Model Performance:\")\n",
    "    print(\"+-----------------------+----------+----------+----------+----------+\")\n",
    "    print(\"| Model                 | Accuracy | F1-score | Precision | Recall   |\")\n",
    "    print(\"+-----------------------+----------+----------+----------+----------+\")\n",
    "    for result in results:\n",
    "        print(\"| {:<20} | {:.4f}   | {:.4f}   | {:.4f}   | {:.4f}   |\".format(*result))\n",
    "    print(\"+-----------------------+----------+----------+----------+----------+\")\n",
    "\n",
    "    # Choose the best model based on the results\n",
    "    best_model = models[results[0][0]]\n",
    "    print(f\"The best model is {results[0][0]}\")\n",
    "\n",
    "    # Evaluation\n",
    "    model = ClassificationModel(\n",
    "        best_model,\n",
    "        result[0][0]\n",
    "    )\n",
    "    model.run_full_analysis(\n",
    "        X_train=X_train+X_test,\n",
    "        X_test=X_val,\n",
    "        y_train=y_train+y_test,\n",
    "        y_test=y_val\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create feature names\n",
    "feature_names = [f'feature_{i}' for i in range(X_val[0].shape[0])]\n",
    "\n",
    "df_ae = pd.concat([\n",
    "    pd.DataFrame(X_val, columns=feature_names), \n",
    "    pd.DataFrame(y_val, columns=['label'])],\n",
    "    axis=1)\n",
    "df_ae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def pca_compress(data, explained_variance_threshold=0.95):\n",
    "    # Standardize the data\n",
    "    scaler = StandardScaler()\n",
    "    data_scaled = scaler.fit_transform(data)\n",
    "    \n",
    "    # Perform PCA with full number of components\n",
    "    pca = PCA()\n",
    "    pca.fit(data_scaled)\n",
    "    \n",
    "    # Find the optimal number of components\n",
    "    cumulative_variance_ratio = np.cumsum(pca.explained_variance_ratio_)\n",
    "    n_components = np.argmax(cumulative_variance_ratio >= explained_variance_threshold) + 1\n",
    "    \n",
    "    # Perform PCA with the optimal number of components\n",
    "    pca = PCA(n_components=n_components)\n",
    "    compressed_data = pca.fit_transform(data_scaled)\n",
    "    \n",
    "    # Calculate compression ratio\n",
    "    original_size = data.shape[1]\n",
    "    compressed_size = compressed_data.shape[1]\n",
    "    compression_ratio = 1 - (compressed_size / original_size)\n",
    "    \n",
    "    # Calculate explained variance ratio\n",
    "    explained_variance_ratio = np.sum(pca.explained_variance_ratio_)\n",
    "    \n",
    "    return compressed_data, compression_ratio, explained_variance_ratio, n_components\n",
    "\n",
    "# Compress the data\n",
    "compressed_data, compression_ratio, explained_variance_ratio, n_components = pca_compress(df_ae.iloc[:, :-1])\n",
    "\n",
    "print(f\"Original data shape: {data.shape}\")\n",
    "print(f\"Compressed data shape: {compressed_data.shape}\")\n",
    "print(f\"Optimal number of components: {n_components}\")\n",
    "print(f\"Compression ratio: {compression_ratio:.2f}\")\n",
    "print(f\"Explained variance ratio: {explained_variance_ratio:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfc = pd.concat([\n",
    "    pd.DataFrame(compressed_data, columns=['feature_1', 'feature_2']),\n",
    "    pd.DataFrame(y_val, columns=['label'])\n",
    "], axis=1)\n",
    "\n",
    "sns.scatterplot(dfc, x='feature_1', y='feature_2', hue='label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
