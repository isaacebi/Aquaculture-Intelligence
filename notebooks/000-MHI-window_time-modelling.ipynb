{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import sys\n",
    "import uuid\n",
    "import joblib\n",
    "import random\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import concurrent.futures\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_selection import f_classif, SelectKBest\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score, ShuffleSplit\n",
    "from sklearn.metrics import classification_report, ConfusionMatrixDisplay\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attaching project directory\n",
    "sys.path.append(os.path.dirname(os.getcwd()))\n",
    "\n",
    "# Pathing imports\n",
    "from src import GetPath, FullMHI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize images\n",
    "NORMALIZE = False\n",
    "\n",
    "# Fixed randomness\n",
    "RANDOM_STATE = 42\n",
    "random.seed(RANDOM_STATE)\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "# Model max iteration\n",
    "MAX_ITERM = 1000\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = GetPath().shared_data()\n",
    "\n",
    "REPO_PATH = GetPath().repo_data()\n",
    "\n",
    "MHI_DATA = os.path.join(DATA_PATH, 'preprocess', 'mhi')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassificationModel:\n",
    "    def __init__(self, model: BaseEstimator, model_name: str):\n",
    "        self.model = model\n",
    "        self.model_name = model_name\n",
    "        self.is_trained = False\n",
    "\n",
    "    def train(self, X_train, y_train):\n",
    "        self.model.fit(X_train, y_train)\n",
    "        train_accuracy = self.model.score(X_train, y_train)\n",
    "        print(f\"{self.model_name} Training accuracy: {train_accuracy:.4f}\")\n",
    "\n",
    "    def evaluate(self, X_test, y_test):\n",
    "        test_accuracy = self.model.score(X_test, y_test)\n",
    "        y_pred = self.model.predict(X_test)\n",
    "\n",
    "        print(f\"{self.model_name} Test accuracy: {test_accuracy:.4f}\")\n",
    "        print(\"\\n-----CLASSIFICATION REPORT-----\\n\")\n",
    "        print(classification_report(y_test, y_pred))\n",
    "\n",
    "        return y_test, y_pred\n",
    "\n",
    "    def plot_confusion_matrix(self, y_true, y_pred):\n",
    "        disp = ConfusionMatrixDisplay.from_predictions(y_true, y_pred, xticks_rotation='vertical')\n",
    "        disp.figure_.suptitle(f\"{self.model_name} Confusion Matrix\")\n",
    "        plt.show()\n",
    "\n",
    "    def run_full_analysis(self, X_train, y_train, X_test, y_test):\n",
    "        self.train(X_train, y_train)\n",
    "        y_true, y_pred = self.evaluate(X_test, y_test)\n",
    "        self.plot_confusion_matrix(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the performance of each base model\n",
    "def train_and_evaluate(name, model, images_train, labels_train, images_test, labels_test):\n",
    "    le = LabelEncoder()\n",
    "    labels_train = le.fit_transform((labels_train))\n",
    "    model.fit(images_train, labels_train)\n",
    "    label_predict = model.predict(images_test)\n",
    "    label_predict = le.inverse_transform(label_predict)\n",
    "    accuracy = accuracy_score(labels_test, label_predict)\n",
    "    f1 = f1_score(labels_test, label_predict, average='weighted', zero_division=0.0)\n",
    "    precision = precision_score(labels_test, label_predict, average='weighted', zero_division=0.0)\n",
    "    recall = recall_score(labels_test, label_predict, average='weighted', zero_division=0.0)\n",
    "    return [name, accuracy, f1, precision, recall]\n",
    "\n",
    "def model_selection(models, X_train, y_train, X_test, y_test):\n",
    "    results = []\n",
    "    with tqdm(total=len(models), desc='Model Training', unit='model') as progress:\n",
    "        with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:\n",
    "            futures = []\n",
    "            for name, model in models.items():\n",
    "                future = executor.submit(\n",
    "                    train_and_evaluate, \n",
    "                    name, \n",
    "                    model, \n",
    "                    X_train,\n",
    "                    y_train,\n",
    "                    X_test,\n",
    "                    y_test\n",
    "                )\n",
    "                futures.append(future)\n",
    "\n",
    "            for future in concurrent.futures.as_completed(futures):\n",
    "                results.append(future.result())\n",
    "                future.add_done_callback(lambda p: progress.update())\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training with different time windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting different time windows\n",
    "roots_dir = []\n",
    "for file in os.listdir(MHI_DATA):\n",
    "    file = os.path.join(MHI_DATA, file)\n",
    "    if os.path.isdir(file):\n",
    "        roots_dir.append(os.path.join(MHI_DATA, file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the base models\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(random_state=RANDOM_STATE, max_iter=MAX_ITERM),\n",
    "    'SGD Classifier': SGDClassifier(random_state=RANDOM_STATE),\n",
    "    'Decision Tree': DecisionTreeClassifier(random_state=RANDOM_STATE),\n",
    "    'Random Forest': RandomForestClassifier(random_state=RANDOM_STATE),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(random_state=RANDOM_STATE),\n",
    "    'AdaBoost': AdaBoostClassifier(random_state=RANDOM_STATE, algorithm='SAMME'),\n",
    "    'K-Nearest Neighbors': KNeighborsClassifier(),\n",
    "    'Support Vector Machine': SVC(random_state=RANDOM_STATE),\n",
    "    'Gaussian Naive Bayes': GaussianNB(),\n",
    "    'Multi-layer Perceptron': MLPClassifier(random_state=RANDOM_STATE, max_iter=MAX_ITERM),\n",
    "    'XGBoost': xgb.XGBClassifier()\n",
    "}\n",
    "\n",
    "# Image transformation\n",
    "train_transform = A.Compose([\n",
    "        # Image resize\n",
    "        A.Resize(32, 32),\n",
    "        # Image normalize\n",
    "        A.Normalize(mean=(0,0,0), std=(1,1,1),),\n",
    "        # Random flip\n",
    "        A.OneOf([\n",
    "            A.HorizontalFlip(p=0.5),\n",
    "            A.VerticalFlip(p=0.5),\n",
    "        ], p=0.5),\n",
    "        # Random rotate\n",
    "        A.OneOf([\n",
    "            A.RandomRotate90(),\n",
    "            A.ShiftScaleRotate(shift_limit=0.0625, scale_limit=0.50, rotate_limit=45, p=.75),\n",
    "        ], p=0.5),\n",
    "    ])\n",
    "\n",
    "# Image transformation\n",
    "test_transform = A.Compose([\n",
    "        # Image resize\n",
    "        A.Resize(32, 32),\n",
    "        # Image normalize\n",
    "        A.Normalize(mean=(0,0,0), std=(1,1,1),),\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization of different size impact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = [8, 16, 32, 64, 128, 256]\n",
    "images = []\n",
    "\n",
    "for size in image_size:\n",
    "    # Image transformation\n",
    "    size_transform = A.Compose([\n",
    "            # Image resize\n",
    "            A.Resize(size, size),\n",
    "            # Image normalize\n",
    "            A.Normalize(mean=(0,0,0), std=(1,1,1),),\n",
    "        ])\n",
    "    \n",
    "    datasets = FullMHI(root_dir=roots_dir[0], transform=size_transform)\n",
    "    _, label, image = datasets.__getitem__(0)\n",
    "    images.append(image)\n",
    "\n",
    "\n",
    "# Calculate the number of rows and columns for the subplot grid\n",
    "num_pictures = len(images)\n",
    "num_cols = 6  # You can adjust this number to change the layout\n",
    "num_rows = (num_pictures + num_cols - 1) // num_cols\n",
    "\n",
    "# Create a figure and axis objects\n",
    "fig, axes = plt.subplots(num_rows, num_cols, figsize=(18, 4 * num_rows))\n",
    "axes = axes.flatten()  # Flatten the 2D array of axes for easier indexing\n",
    "\n",
    "# Plot each picture placeholder\n",
    "for i, size in enumerate(image_size):\n",
    "    width = size\n",
    "    height = size\n",
    "    \n",
    "    ax = axes[i]\n",
    "    ax.set_xlim(0, width)\n",
    "    ax.set_ylim(0, height)\n",
    "    ax.set_aspect('equal')\n",
    "    ax.add_patch(plt.Rectangle((0, 0), width, height, fill=False))\n",
    "    ax.imshow(images[i], cmap='gray')\n",
    "    ax.set_title(f'Picture size {size}')\n",
    "    ax.axis('off')\n",
    "\n",
    "# Remove any unused subplots\n",
    "for i in range(num_pictures, len(axes)):\n",
    "    fig.delaxes(axes[i])\n",
    "\n",
    "# Adjust the layout and display the plot\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization of different time windows impact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = []\n",
    "\n",
    "# Iterate through windows time (seconds)\n",
    "for file in roots_dir:\n",
    "    datasets = FullMHI(root_dir=file)\n",
    "    _, label, image = datasets.__getitem__(0)\n",
    "    images.append(image)\n",
    "\n",
    "    \n",
    "# Calculate the number of rows and columns for the subplot grid\n",
    "num_pictures = len(images)\n",
    "num_cols = 6  # You can adjust this number to change the layout\n",
    "num_rows = (num_pictures + num_cols - 1) // num_cols\n",
    "\n",
    "# Create a figure and axis objects\n",
    "fig, axes = plt.subplots(num_rows, num_cols, figsize=(18, 4 * num_rows))\n",
    "axes = axes.flatten()  # Flatten the 2D array of axes for easier indexing\n",
    "\n",
    "# Plot each picture placeholder\n",
    "for i, time_window in enumerate(roots_dir):\n",
    "    time_window = roots_dir[0].split(\"\\\\\")[-1]\n",
    "    ax = axes[i]\n",
    "    ax.imshow(images[i], cmap='gray')\n",
    "    ax.set_title(f'Time windows {time_window}')\n",
    "    ax.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Time_Windows_Selector:\n",
    "    def __init__(self, \n",
    "                 model: BaseEstimator,\n",
    "                 model_name: str, \n",
    "                 dataset_original: list, \n",
    "                 dataset_augmentated: list, \n",
    "                 n_augmentation: int, \n",
    "                 random_state: int,\n",
    "                 result_path):\n",
    "        \n",
    "        self.model = model\n",
    "        self.model_name = model_name\n",
    "        self.dataset_original = dataset_original\n",
    "        self.dataset_augmentated = dataset_augmentated\n",
    "        self.n_augmentation = n_augmentation\n",
    "        self.random_state = random_state\n",
    "        self.result_path = result_path\n",
    "\n",
    "        # Template\n",
    "        self.idx_track = {\n",
    "            'B1': [],\n",
    "            'B2': [],\n",
    "            'B3': []\n",
    "        }\n",
    "\n",
    "        self.labels_track = {\n",
    "            'B1': [],\n",
    "            'B2': [],\n",
    "            'B3': []\n",
    "        }\n",
    "\n",
    "        # Assign tracker by experiments\n",
    "        self.__get_tracks__()\n",
    "\n",
    "        # Total dataset\n",
    "        print(f\"Total experiment dataset: {len(dataset_original)}\")\n",
    "\n",
    "    def __get_tracks__(self):\n",
    "        # Assigning index tracker by batch experiment - training\n",
    "        for idx in range(len(self.dataset_original)):\n",
    "            experiment, label, image = self.dataset_original.__getitem__(idx)\n",
    "            self.idx_track[experiment].append(idx)\n",
    "            self.labels_track[experiment].append(label)\n",
    "\n",
    "    def save_results(self, result: pd.DataFrame, fileName: str = \"abnormal_multi_selection.csv\"):\n",
    "        file = os.path.join(self.result_path, fileName)\n",
    "\n",
    "        if os.path.exists(file):\n",
    "            results = pd.read_csv(file)\n",
    "            results = pd.concat([results, result]).reset_index(drop=True)\n",
    "            results.to_csv(file, index=False)\n",
    "\n",
    "        if not os.path.exists(file):\n",
    "            if not os.path.exists(self.result_path):\n",
    "                os.makedirs(self.result_path)\n",
    "                \n",
    "            result.to_csv(file, index=False)\n",
    "            \n",
    "\n",
    "    def get_sets(self, idx_track: list, labels_track: list, validation_experiment: list):\n",
    "        def get_image_label(idx, data):\n",
    "            _, label, image = data.__getitem__(idx)\n",
    "            return image.flatten(), label\n",
    "\n",
    "        # Template\n",
    "        X_train = []\n",
    "        y_train = []\n",
    "        X_test = []\n",
    "        y_test = []\n",
    "        X_val = []\n",
    "        y_val = []\n",
    "\n",
    "        # Train and test splitting between experiment B1 & B2, B1 does not have all abnormal level\n",
    "        idx_train, idx_test = train_test_split(\n",
    "            idx_track,\n",
    "            test_size=0.3,\n",
    "            random_state=self.random_state,\n",
    "            stratify=labels_track\n",
    "        )\n",
    "            \n",
    "        # Generate n times sample with different augmentation\n",
    "        for i in range(self.n_augmentation):\n",
    "            # for idx in idx_train:\n",
    "            #     _, label, image = self.dataset_augmentated.__getitem__(idx)\n",
    "            #     X_train.append(image.flatten())\n",
    "            #     y_train.append(label)\n",
    "            with ThreadPoolExecutor(max_workers=16) as executor:\n",
    "                # Submitting get_image_label\n",
    "                futures = [executor.submit(get_image_label, idx, self.dataset_augmentated) for idx in idx_train]\n",
    "\n",
    "                # Collecting images and labels\n",
    "                for future in futures:\n",
    "                    X, y = future.result()\n",
    "                    X_train.append(X)\n",
    "                    y_train.append(y)\n",
    "            \n",
    "        # Message\n",
    "        print(f\"Total augmentated data for training: {len(X_train)}\")\n",
    "\n",
    "        # for idx in idx_test:\n",
    "        #     _, label, image = self.dataset_original.__getitem__(idx)\n",
    "        #     X_test.append(image.flatten())\n",
    "        #     y_test.append(label)\n",
    "        with ThreadPoolExecutor(max_workers=16) as executor:\n",
    "            # Submitting get_image_label\n",
    "            futures = [executor.submit(get_image_label, idx, self.dataset_original) for idx in idx_test]\n",
    "\n",
    "            # Collecting images and labels\n",
    "            for future in futures:\n",
    "                X, y = future.result()\n",
    "                X_test.append(X)\n",
    "                y_test.append(y)        \n",
    "\n",
    "        # for idx in validation_experiment:\n",
    "        #     _, label, image = self.dataset_original.__getitem__(idx)\n",
    "        #     X_val.append(image.flatten())\n",
    "        #     y_val.append(label)\n",
    "        with ThreadPoolExecutor(max_workers=16) as executor:\n",
    "            # Submitting get_image_label\n",
    "            futures = [executor.submit(get_image_label, idx, self.dataset_original) for idx in validation_experiment]\n",
    "\n",
    "            # Collecting images and labels\n",
    "            for future in futures:\n",
    "                X, y = future.result()\n",
    "                X_val.append(X)\n",
    "                y_val.append(y)   \n",
    "        \n",
    "        return X_train, y_train, X_test, y_test, X_val, y_val\n",
    "\n",
    "\n",
    "    def train(self, X_train, y_train):\n",
    "        self.model.fit(X_train, y_train)\n",
    "        train_accuracy = self.model.score(X_train, y_train)\n",
    "        print(f\"{self.model_name} Training accuracy: {train_accuracy:.4f}\")\n",
    "\n",
    "    def evaluate(self, X_test, y_test):\n",
    "        # test_accuracy = self.model.score(X_test, y_test)\n",
    "        y_pred = self.model.predict(X_test)\n",
    "\n",
    "        # print(f\"{self.model_name} Test accuracy: {test_accuracy:.4f}\")\n",
    "        print(\"\\n-----CLASSIFICATION REPORT-----\\n\")\n",
    "        print(classification_report(y_test, y_pred))\n",
    "\n",
    "        return y_pred\n",
    "\n",
    "    def plot_confusion_matrix(self, y_true, y_pred, model_name, batch, windows_time, img_size, save=False):\n",
    "        if not model_name:\n",
    "            model_name = self.model_name\n",
    "\n",
    "        disp = ConfusionMatrixDisplay.from_predictions(y_true, y_pred, xticks_rotation='vertical')\n",
    "        disp.figure_.suptitle(f\"{model_name} Confusion Matrix\")\n",
    "\n",
    "        if save:\n",
    "            fileName = f'{model_name}_{batch}_{windows_time}_{img_size}.png'\n",
    "            fileName = os.path.join(self.result_path, fileName)\n",
    "            plt.savefig(fileName)\n",
    "            \n",
    "        plt.show()\n",
    "\n",
    "\n",
    "    def macro_average_evaluation_matrix(self, y_true, y_pred, labels=None) -> pd.DataFrame:\n",
    "        if isinstance(y_true, np.ndarray):\n",
    "            y_true = np.array(y_true)\n",
    "\n",
    "        if isinstance(y_pred, np.ndarray):\n",
    "            y_pred = np.array(y_pred)\n",
    "\n",
    "        # Get the classification report as a dictionary\n",
    "        report = classification_report(y_true, y_pred, labels=labels, output_dict=True)\n",
    "        \n",
    "        # To dataframe only for macro average\n",
    "        df = pd.DataFrame(report['macro avg'], index=[0])\n",
    "        \n",
    "        return df.rename(columns={\n",
    "            'precision': 'precision_macro_avg',\n",
    "            'recall': 'recall_macro_avg',\n",
    "            'f1-score': 'f1_score_macro_avg',\n",
    "            'support': 'support_macro_avg'\n",
    "        })\n",
    "\n",
    "    def run_full_analysis(self) -> pd.DataFrame:\n",
    "        results = pd.DataFrame()\n",
    "        for batch in ['B2', 'B3']:\n",
    "            if batch == 'B2':\n",
    "                validation_experiment = self.idx_track['B3']\n",
    "            else:\n",
    "                validation_experiment = self.idx_track['B2']\n",
    "\n",
    "            X_train, y_train, X_test, y_test, X_val, y_val = self.get_sets(\n",
    "                idx_track = self.idx_track['B1'] + self.idx_track[batch],\n",
    "                labels_track = self.labels_track['B1'] + self.labels_track[batch],\n",
    "                validation_experiment=validation_experiment\n",
    "            )\n",
    "\n",
    "            print(f\"--- Conducting training with experiment B1 and {batch}\")\n",
    "\n",
    "            self.train(X_train, y_train)\n",
    "\n",
    "            # Test\n",
    "            print(f\"Evaluation on test sets\")\n",
    "            yt_pred = self.evaluate(X_test, y_test)\n",
    "            self.plot_confusion_matrix(y_test, yt_pred)\n",
    "\n",
    "            # Validation\n",
    "            print(f\"Evaluation on validation sets\")\n",
    "            yv_pred = self.evaluate(X_val, y_val)\n",
    "            self.plot_confusion_matrix(y_val, yv_pred)\n",
    "\n",
    "            # Validation results\n",
    "            result = self.macro_average_evaluation_matrix(y_val, yv_pred)\n",
    "            result['training_batch'] = f'B1{batch}'\n",
    "            results = pd.concat([results, result])\n",
    "\n",
    "    def run_model_selection(self, models: dict, img_size: int, windows_time: int) -> pd.DataFrame:\n",
    "        fileName = f'abnormal_multi_selection_{str(uuid.uuid4())}.csv'\n",
    "\n",
    "        for batch in ['B2', 'B3']:\n",
    "\n",
    "            if batch == 'B2':\n",
    "                validation_experiment = self.idx_track['B3']\n",
    "            else:\n",
    "                validation_experiment = self.idx_track['B2']\n",
    "\n",
    "            X_train, y_train, X_test, y_test, X_val, y_val = self.get_sets(\n",
    "                idx_track = self.idx_track['B1'] + self.idx_track[batch],\n",
    "                labels_track = self.labels_track['B1'] + self.labels_track[batch],\n",
    "                validation_experiment=validation_experiment\n",
    "            )\n",
    "\n",
    "            for name, model in models.items():\n",
    "                # Message\n",
    "                print(f\"--- Experimenting data on folder {file} with images size of {size}\")\n",
    "                print(f\"--- Conducting training with experiment B1 and {batch} and {name} model\")\n",
    "\n",
    "                batch_name = f'B1{batch}'\n",
    "\n",
    "                # Training\n",
    "                model.fit(X_train, y_train)\n",
    "\n",
    "                # Test\n",
    "                print(f\"Evaluation on test sets\")\n",
    "                yt_pred = model.predict(X_test)\n",
    "                print(classification_report(y_test, yt_pred))\n",
    "                self.plot_confusion_matrix(y_test, yt_pred, name, batch_name, windows_time, img_size)\n",
    "\n",
    "                # Validation\n",
    "                print(f\"Evaluation on validation sets\")\n",
    "                yv_pred = model.predict(X_val)\n",
    "                print(classification_report(y_val, yv_pred))\n",
    "                self.plot_confusion_matrix(y_val, yv_pred, name, batch_name, windows_time, img_size, save=True)\n",
    "\n",
    "                # Validation results\n",
    "                result = self.macro_average_evaluation_matrix(y_val, yv_pred)\n",
    "\n",
    "\n",
    "                result['training_batch'] = batch_name\n",
    "                result['model'] = name\n",
    "                result['img_size'] = img_size\n",
    "                result['windows_time'] = windows_time\n",
    "\n",
    "                result['accuracy'] = accuracy_score(y_val, yv_pred)\n",
    "                result['recall_weighted'] = recall_score(y_val, yv_pred, average='weighted')\n",
    "                result['precision_weighted'] = precision_score(y_val, yv_pred, average='weighted')\n",
    "                result['f1_score_weighted'] = f1_score(y_val, yv_pred, average='weighted')\n",
    "                \n",
    "                self.save_results(result, fileName=fileName)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformation Testing\n",
    "size = 52\n",
    "transform = A.Compose([\n",
    "        # Image resize\n",
    "        A.Resize(size, size),    \n",
    "        A.SafeRotate(limit=45, border_mode=cv2.BORDER_CONSTANT, value=0, p=0.5),\n",
    "        A.RandomCrop(height=size, width=size, always_apply=False, p=0.5),\n",
    "        A.PadIfNeeded(min_height=size, min_width=size, border_mode=cv2.BORDER_CONSTANT, value=0, p=1.0),\n",
    "        A.Flip(p=0.5),\n",
    "        A.Transpose(p=0.5),\n",
    "        A.OneOf([\n",
    "            A.MotionBlur(p=0.5),\n",
    "            A.MedianBlur(p=0.5),\n",
    "            A.GaussianBlur(p=0.5),\n",
    "        ], p=0.5),\n",
    "        A.OneOf([\n",
    "            A.OpticalDistortion(border_mode=cv2.BORDER_CONSTANT, value=0, p=0.5),\n",
    "            A.GridDistortion(border_mode=cv2.BORDER_CONSTANT, value=0, p=0.5),\n",
    "            A.ElasticTransform(border_mode=cv2.BORDER_CONSTANT, value=0, p=0.5),\n",
    "        ], p=0.5),\n",
    "        A.CLAHE(clip_limit=4.0, tile_grid_size=(8, 8), p=0.5),\n",
    "        A.RandomBrightnessContrast(p=0.5),\n",
    "        A.RandomGamma(p=0.5),\n",
    "    ])\n",
    "\n",
    "datasets = FullMHI(root_dir=file, transform=transform)\n",
    "_, label, image = datasets.__getitem__(0)\n",
    "\n",
    "plt.imshow(image, cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = [8, 16, 32, 52, 64, 128, 224, 256]\n",
    "results = pd.DataFrame()\n",
    "\n",
    "# Image transformation\n",
    "ToTrainTransform = A.Compose([\n",
    "        # Image resize\n",
    "        A.Resize(size, size),\n",
    "        # Image normalize\n",
    "        A.Normalize(mean=(0,0,0), std=(1,1,1),),\n",
    "        # Random flip\n",
    "        A.OneOf([\n",
    "            A.HorizontalFlip(p=0.5),\n",
    "            A.VerticalFlip(p=0.5),\n",
    "        ], p=0.5),\n",
    "        # Random rotate\n",
    "        A.OneOf([\n",
    "            A.RandomRotate90(),\n",
    "            A.ShiftScaleRotate(shift_limit=0.0625, scale_limit=0.50, rotate_limit=45, p=.75),\n",
    "        ], p=0.5),\n",
    "        # Brightness randomness\n",
    "        A.RandomBrightnessContrast(p=0.5),\n",
    "    ])\n",
    "\n",
    "# Image transformation\n",
    "ToTestTransform = A.Compose([\n",
    "        # Image resize\n",
    "        A.Resize(size, size),\n",
    "        # Image normalize\n",
    "        A.Normalize(mean=(0,0,0), std=(1,1,1),),\n",
    "    ])\n",
    "\n",
    "\n",
    "# for size in image_size:\n",
    "#     # Iterate through windows time (seconds)\n",
    "#     for file in roots_dir:\n",
    "#         # Message\n",
    "#         print(f\"--- Experimenting data on folder {file} with images size of {size}\")\n",
    "#         # Loading datasets with augmentation\n",
    "#         datasetsWithTransform = FullMHI(root_dir=file, transform=ToTrainTransform)\n",
    "#         datasets = FullMHI(root_dir=file, transform=ToTestTransform)\n",
    "\n",
    "#         selector = Time_Windows_Selector(\n",
    "#             model=RandomForestClassifier(random_state=RANDOM_STATE),\n",
    "#             model_name=\"Random Forest\",\n",
    "#             dataset_original=datasets,\n",
    "#             dataset_augmentated=datasetsWithTransform,\n",
    "#             n_augmentation=20,\n",
    "#             random_state=RANDOM_STATE\n",
    "#         )\n",
    "\n",
    "#         result = selector.run_full_analysis()\n",
    "\n",
    "#         result['img_size'] = size\n",
    "\n",
    "#         # collector\n",
    "#         results = pd.concat([results, result])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the base models\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(random_state=RANDOM_STATE, max_iter=MAX_ITERM),\n",
    "    'SGD Classifier': SGDClassifier(random_state=RANDOM_STATE),\n",
    "    'Decision Tree': DecisionTreeClassifier(random_state=RANDOM_STATE),\n",
    "    'Random Forest': RandomForestClassifier(random_state=RANDOM_STATE),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(random_state=RANDOM_STATE),\n",
    "    'AdaBoost': AdaBoostClassifier(random_state=RANDOM_STATE, algorithm='SAMME'),\n",
    "    'K-Nearest Neighbors': KNeighborsClassifier(),\n",
    "    'Support Vector Machine': SVC(random_state=RANDOM_STATE),\n",
    "    'Gaussian Naive Bayes': GaussianNB(),\n",
    "    'Multi-layer Perceptron': MLPClassifier(random_state=RANDOM_STATE, max_iter=MAX_ITERM)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = []\n",
    "X_test = []\n",
    "y_train = []\n",
    "y_test = []\n",
    "results = []\n",
    "\n",
    "cv = ShuffleSplit(n_splits=5, test_size=0.3, random_state=RANDOM_STATE)\n",
    "\n",
    "AUG_NUM = 2\n",
    "\n",
    "for size in image_size:\n",
    "    for file in roots_dir:\n",
    "        db_aug = FullMHI(root_dir=file, transform=ToTrainTransform)\n",
    "        db = FullMHI(root_dir=file, transform=ToTestTransform)\n",
    "\n",
    "        # Get train\n",
    "        for i in range(AUG_NUM):\n",
    "            for idx in range(len(db)):\n",
    "                experiment, label, image = db_aug.__getitem__(idx)\n",
    "\n",
    "                if experiment == 'B1' or experiment == 'B3':\n",
    "                    X_train.append(image.flatten())\n",
    "                    y_train.append(label)\n",
    "\n",
    "        # Get Test\n",
    "        for idx in range(len(db)):\n",
    "            experiment, label, image = db.__getitem__(idx)\n",
    "            X_test.append(image.flatten())\n",
    "            y_train.append(label)\n",
    "\n",
    "\n",
    "        model = models['Random Forest']\n",
    "\n",
    "        result = {\n",
    "            'size': size,\n",
    "            'window_size': os.path.basename(file)\n",
    "        }\n",
    "\n",
    "        for scoring in ['accuracy', 'precision', 'recall', 'f1']:\n",
    "            scores = cross_val_score(model, X_train, y_train, cv=cv, scoring=scoring)\n",
    "            result[scoring] = np.mean(scores)\n",
    "\n",
    "        model.train(X_train, y_train)\n",
    "        \n",
    "        y_pred = model.predict(X_test)\n",
    "\n",
    "        disp = ConfusionMatrixDisplay.from_predictions(y_test, y_pred, xticks_rotation='vertical')\n",
    "        titleName = f'{result['size']}_{result['window_size']}_CM'\n",
    "        disp.figure_.suptitle(titleName)\n",
    "\n",
    "        path = os.path.join(REPO_PATH, 'results', f'{titleName}.png')\n",
    "        plt.savefig(path)\n",
    "        results.append(result)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for size in image_size:\n",
    "    # Iterate through windows time (seconds)\n",
    "    for file in roots_dir:\n",
    "\n",
    "        # Loading datasets with augmentation\n",
    "        datasetsWithTransform = FullMHI(root_dir=file, transform=ToTrainTransform)\n",
    "        datasets = FullMHI(root_dir=file, transform=ToTestTransform)\n",
    "\n",
    "        # Initiate selector\n",
    "        selector = Time_Windows_Selector(\n",
    "            model=RandomForestClassifier(),\n",
    "            model_name='Random Forest',\n",
    "            dataset_original=datasets,\n",
    "            dataset_augmentated=datasetsWithTransform,\n",
    "            n_augmentation=20,\n",
    "            random_state=RANDOM_STATE,\n",
    "            result_path=os.path.join(REPO_PATH, 'results')\n",
    "        )\n",
    "\n",
    "        selector.run_model_selection(models, size, os.path.basename(file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for size in image_size:\n",
    "    # Iterate through windows time (seconds)\n",
    "    for file in roots_dir:\n",
    "\n",
    "        # Loading datasets with augmentation\n",
    "        datasetsWithTransform = FullMHI(root_dir=file, transform=ToTrainTransform)\n",
    "        datasets = FullMHI(root_dir=file, transform=ToTestTransform)\n",
    "\n",
    "        # Initiate selector\n",
    "        selector = Time_Windows_Selector(\n",
    "            model=xgb.XGBClassifier(),\n",
    "            model_name='xgboost',\n",
    "            dataset_original=datasets,\n",
    "            dataset_augmentated=datasetsWithTransform,\n",
    "            n_augmentation=20,\n",
    "            random_state=RANDOM_STATE,\n",
    "            result_path=os.path.join(REPO_PATH, 'results')\n",
    "        )\n",
    "\n",
    "        selector.run_model_selection(models, size, os.path.basename(file))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate through windows time (seconds)\n",
    "for file in roots_dir:\n",
    "    # Message\n",
    "    print(f\"Experimenting data on folder {file}\")\n",
    "    # Loading datasets with augmentation\n",
    "    datasetsWithTransform = FullMHI(root_dir=file, transform=train_transform)\n",
    "    datasets = FullMHI(root_dir=file, transform=test_transform)\n",
    "    # Message\n",
    "    print(f\"Total experiment datasets: {len(datasets)}\")\n",
    "\n",
    "    # Assigning index tracker for each batch experiment\n",
    "    idx_track = {\n",
    "        'B1': [],\n",
    "        'B2': [],\n",
    "        'B3': []\n",
    "    }\n",
    "\n",
    "    labels_track = {\n",
    "        'B1': [],\n",
    "        'B2': [],\n",
    "        'B3': []\n",
    "    }\n",
    "\n",
    "    # Reasign data\n",
    "    for idx in range(len(datasets)):\n",
    "        experiment, label, image = datasetsWithTransform.__getitem__(idx)\n",
    "        idx_track[experiment].append(idx)\n",
    "        labels_track[experiment].append(label)\n",
    "\n",
    "    # Train and test splitting between experiment B1 & B2, B1 does not have all abnormal level\n",
    "    idx_train, idx_test = train_test_split(\n",
    "        idx_track['B1'] + idx_track['B2'],\n",
    "        test_size=0.2,\n",
    "        random_state=RANDOM_STATE,\n",
    "        stratify=labels_track['B1'] + labels_track['B2']\n",
    "    )\n",
    "\n",
    "    # Training variable\n",
    "    X_train = []\n",
    "    y_train = []\n",
    "\n",
    "    # Generate n times sample with different augmentation\n",
    "    for i in range(3):\n",
    "        for idx in idx_train:\n",
    "            experiment, label, image = datasetsWithTransform.__getitem__(idx)\n",
    "            X_train.append(image.flatten())\n",
    "            y_train.append(label)\n",
    "\n",
    "    # Message\n",
    "    print(f\"Total augmentated data for training: {len(idx_train)}\")\n",
    "\n",
    "    # Test variable\n",
    "    X_test = []\n",
    "    y_test = []\n",
    "\n",
    "    for idx in idx_test:\n",
    "        experiment, label, image = datasets.__getitem__(idx)\n",
    "        X_test.append(image.flatten())\n",
    "        y_test.append(label)\n",
    "\n",
    "    # Validation variable with experiment B3\n",
    "    X_val = []\n",
    "    y_val = []\n",
    "\n",
    "    for idx in idx_track['B3']:\n",
    "        experiment, label, image = datasets.__getitem__(idx)\n",
    "        X_val.append(image.flatten())\n",
    "        y_val.append(label)\n",
    "\n",
    "    # run baseline selector\n",
    "    results = model_selection(models, X_train, y_train, X_test, y_test)\n",
    "\n",
    "    # Sort the results by F1-score in descending order\n",
    "    results = sorted(results, key=lambda x: x[2], reverse=True)\n",
    "\n",
    "    # Print the results\n",
    "    print(\"Model Performance:\")\n",
    "    print(\"+-----------------------+----------+----------+----------+----------+\")\n",
    "    print(\"| Model                 | Accuracy | F1-score | Precision | Recall   |\")\n",
    "    print(\"+-----------------------+----------+----------+----------+----------+\")\n",
    "    for result in results:\n",
    "        print(\"| {:<20} | {:.4f}   | {:.4f}   | {:.4f}   | {:.4f}   |\".format(*result))\n",
    "    print(\"+-----------------------+----------+----------+----------+----------+\")\n",
    "\n",
    "    # Choose the best model based on the results\n",
    "    best_model = models[results[0][0]]\n",
    "    print(f\"The best model is {results[0][0]}\")\n",
    "\n",
    "    # Evaluation\n",
    "    model = ClassificationModel(\n",
    "        best_model,\n",
    "        result[0][0]\n",
    "    )\n",
    "\n",
    "    X_train = []\n",
    "    y_train = []\n",
    "    for i in range(5):\n",
    "        for idx in idx_train + idx_test:\n",
    "            experiment, label, image = datasetsWithTransform.__getitem__(idx)\n",
    "            X_train.append(image.flatten())\n",
    "            y_train.append(label)\n",
    "\n",
    "    model.run_full_analysis(\n",
    "        X_train=X_train,\n",
    "        X_test=X_val,\n",
    "        y_train=y_train,\n",
    "        y_test=y_val\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the autoencoder architecture\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.input_size = input_size\n",
    "\n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(self.input_size, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 16)\n",
    "        )\n",
    "\n",
    "        # Decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(16, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, self.input_size),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Autoencoder\n",
    "# Initialize the autoencoder\n",
    "autoencoder_model = Autoencoder(input_size=32*32).to(device)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(autoencoder_model.parameters())\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "\n",
    "# Image transformation\n",
    "autoencoder_transform = A.Compose([\n",
    "        # Image resize\n",
    "        A.Resize(32, 32),\n",
    "        # Image normalize\n",
    "        A.Normalize(mean=(0,0,0), std=(1,1,1),),\n",
    "        ToTensorV2()\n",
    "    ])\n",
    "\n",
    "# Iterate through windows time (seconds)\n",
    "for file in roots_dir:\n",
    "    # Message\n",
    "    print(f\"Experimenting data on folder {file}\")\n",
    "    datasets = FullMHI(root_dir=file, transform=autoencoder_transform)\n",
    "    train_loader = DataLoader(datasets, batch_size=128, shuffle=True)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        for data in train_loader:\n",
    "            _, _, img = data\n",
    "            img = img.view(img.size(0), -1).to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            output = autoencoder_model(img)\n",
    "            loss = criterion(output, img)\n",
    "\n",
    "            # Backward pass and optimization\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "print(\"Training finished!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder_image(image, input_size, model):\n",
    "    model.eval()\n",
    "    \n",
    "    # Image to GPU\n",
    "    image = torch.from_numpy(image)\n",
    "    image = image.view(-1, input_size).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        encoder_output = model.encoder(image)\n",
    "\n",
    "    return encoder_output.cpu().numpy().flatten()\n",
    "\n",
    "input_size = 32*32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate through windows time (seconds)\n",
    "for file in roots_dir:\n",
    "    # Message\n",
    "    print(f\"Experimenting data on folder {file}\")\n",
    "    # Loading datasets with augmentation\n",
    "    datasetsWithTransform = FullMHI(root_dir=file, transform=train_transform)\n",
    "    datasets = FullMHI(root_dir=file, transform=test_transform)\n",
    "    # Message\n",
    "    print(f\"Total experiment datasets: {len(datasets)}\")\n",
    "\n",
    "    # Assigning index tracker for each batch experiment\n",
    "    idx_track = {\n",
    "        'B1': [],\n",
    "        'B2': [],\n",
    "        'B3': []\n",
    "    }\n",
    "\n",
    "    labels_track = {\n",
    "        'B1': [],\n",
    "        'B2': [],\n",
    "        'B3': []\n",
    "    }\n",
    "\n",
    "    # Reasign data\n",
    "    for idx in range(len(datasets)):\n",
    "        experiment, label, image = datasetsWithTransform.__getitem__(idx)\n",
    "        idx_track[experiment].append(idx)\n",
    "        labels_track[experiment].append(label)\n",
    "\n",
    "    # Train and test splitting between experiment B1 & B2, B1 does not have all abnormal level\n",
    "    idx_train, idx_test = train_test_split(\n",
    "        idx_track['B1'] + idx_track['B2'],\n",
    "        test_size=0.2,\n",
    "        random_state=RANDOM_STATE,\n",
    "        stratify=labels_track['B1'] + labels_track['B2']\n",
    "    )\n",
    "\n",
    "    # Training variable\n",
    "    X_train = []\n",
    "    y_train = []\n",
    "\n",
    "    # Generate n times sample with different augmentation\n",
    "    for i in range(3):\n",
    "        for idx in idx_train:\n",
    "            experiment, label, image = datasetsWithTransform.__getitem__(idx)\n",
    "\n",
    "            # feature extraction\n",
    "            new_feature = encoder_image(image, input_size, autoencoder_model)\n",
    "\n",
    "            # Extracted feature for training\n",
    "            X_train.append(new_feature)\n",
    "            y_train.append(label)\n",
    "\n",
    "    # Message\n",
    "    print(f\"Total augmentated data for training: {len(idx_train)}\")\n",
    "\n",
    "    # Test variable\n",
    "    X_test = []\n",
    "    y_test = []\n",
    "\n",
    "    for idx in idx_test:\n",
    "        experiment, label, image = datasets.__getitem__(idx)\n",
    "\n",
    "        # feature extraction\n",
    "        new_feature = encoder_image(image, input_size, autoencoder_model)\n",
    "\n",
    "        # Extracted feature for testing\n",
    "        X_test.append(new_feature)\n",
    "        y_test.append(label)\n",
    "\n",
    "    # Validation variable with experiment B3\n",
    "    X_val = []\n",
    "    y_val = []\n",
    "\n",
    "    for idx in idx_track['B3']:\n",
    "        experiment, label, image = datasets.__getitem__(idx)\n",
    "\n",
    "        # feature extraction\n",
    "        new_feature = encoder_image(image, input_size, autoencoder_model)\n",
    "\n",
    "        # Extracted feature for validation\n",
    "        X_val.append(new_feature)\n",
    "        y_val.append(label)\n",
    "\n",
    "    # run baseline selector\n",
    "    results = model_selection(models, X_train, y_train, X_test, y_test)\n",
    "\n",
    "    # Sort the results by F1-score in descending order\n",
    "    results = sorted(results, key=lambda x: x[2], reverse=True)\n",
    "\n",
    "    # Print the results\n",
    "    print(\"Model Performance:\")\n",
    "    print(\"+-----------------------+----------+----------+----------+----------+\")\n",
    "    print(\"| Model                 | Accuracy | F1-score | Precision | Recall   |\")\n",
    "    print(\"+-----------------------+----------+----------+----------+----------+\")\n",
    "    for result in results:\n",
    "        print(\"| {:<20} | {:.4f}   | {:.4f}   | {:.4f}   | {:.4f}   |\".format(*result))\n",
    "    print(\"+-----------------------+----------+----------+----------+----------+\")\n",
    "\n",
    "    # Choose the best model based on the results\n",
    "    best_model = models[results[0][0]]\n",
    "    print(f\"The best model is {results[0][0]}\")\n",
    "\n",
    "    # Evaluation\n",
    "    model = ClassificationModel(\n",
    "        best_model,\n",
    "        result[0][0]\n",
    "    )\n",
    "    model.run_full_analysis(\n",
    "        X_train=X_train+X_test,\n",
    "        X_test=X_val,\n",
    "        y_train=y_train+y_test,\n",
    "        y_test=y_val\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create feature names\n",
    "feature_names = [f'feature_{i}' for i in range(X_train[0].shape[0])]\n",
    "\n",
    "# Create a Random Forest model\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Get feature importances\n",
    "importances = rf_model.feature_importances_\n",
    "\n",
    "# Sort feature importances in descending order\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "# Create a DataFrame of feature importances\n",
    "feature_importance_df = pd.DataFrame({\n",
    "    'feature': [feature_names[i] for i in indices],\n",
    "    'importance': importances[indices]\n",
    "})\n",
    "\n",
    "# Plot feature importances\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.title(\"Feature Importances\")\n",
    "plt.bar(range(X_train[0].shape[0]), importances[indices])\n",
    "plt.xticks(range(X_train[0].shape[0]), [feature_names[i] for i in indices], rotation=90)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print feature importances\n",
    "print(feature_importance_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Autoencoder\n",
    "# Initialize the autoencoder\n",
    "autoencoder_model = Autoencoder(input_size=32*32).to(device)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(autoencoder_model.parameters())\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "\n",
    "# Image transformation\n",
    "autoencoder_transform = A.Compose([\n",
    "        # Image resize\n",
    "        A.Resize(32, 32),\n",
    "        # Image normalize\n",
    "        A.Normalize(mean=(0,0,0), std=(1,1,1),),\n",
    "        ToTensorV2()\n",
    "    ])\n",
    "\n",
    "# Iterate through windows time (seconds)\n",
    "for file in roots_dir:\n",
    "    # Message\n",
    "    print(f\"Experimenting data on folder {file}\")\n",
    "    datasets = FullMHI(root_dir=file, transform=autoencoder_transform)\n",
    "    train_loader = DataLoader(datasets, batch_size=128, shuffle=True)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        for data in train_loader:\n",
    "            _, _, img = data\n",
    "            img = img.view(img.size(0), -1).to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            output = autoencoder_model(img)\n",
    "            loss = criterion(output, img)\n",
    "\n",
    "            # Backward pass and optimization\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "    # Loading datasets with augmentation\n",
    "    datasetsWithTransform = FullMHI(root_dir=file, transform=train_transform)\n",
    "    datasets = FullMHI(root_dir=file, transform=test_transform)\n",
    "    # Message\n",
    "    print(f\"Total experiment datasets: {len(datasets)}\")\n",
    "\n",
    "\n",
    "    # Assigning index tracker for each batch experiment\n",
    "    idx_track = {\n",
    "        'B1': [],\n",
    "        'B2': [],\n",
    "        'B3': []\n",
    "    }\n",
    "\n",
    "    labels_track = {\n",
    "        'B1': [],\n",
    "        'B2': [],\n",
    "        'B3': []\n",
    "    }\n",
    "\n",
    "    # Reasign data\n",
    "    for idx in range(len(datasets)):\n",
    "        experiment, label, image = datasetsWithTransform.__getitem__(idx)\n",
    "        idx_track[experiment].append(idx)\n",
    "        labels_track[experiment].append(label)\n",
    "\n",
    "    # Train and test splitting between experiment B1 & B2, B1 does not have all abnormal level\n",
    "    idx_train, idx_test = train_test_split(\n",
    "        idx_track['B1'] + idx_track['B2'],\n",
    "        test_size=0.2,\n",
    "        random_state=RANDOM_STATE,\n",
    "        stratify=labels_track['B1'] + labels_track['B2']\n",
    "    )\n",
    "\n",
    "    # Training variable\n",
    "    X_train = []\n",
    "    y_train = []\n",
    "\n",
    "    # Generate n times sample with different augmentation\n",
    "    for i in range(3):\n",
    "        for idx in idx_train:\n",
    "            experiment, label, image = datasetsWithTransform.__getitem__(idx)\n",
    "\n",
    "            # feature extraction\n",
    "            new_feature = encoder_image(image, input_size, autoencoder_model)\n",
    "\n",
    "            # Extracted feature for training\n",
    "            X_train.append(new_feature)\n",
    "            y_train.append(label)\n",
    "\n",
    "    # Message\n",
    "    print(f\"Total augmentated data for training: {len(idx_train)}\")\n",
    "\n",
    "    # Test variable\n",
    "    X_test = []\n",
    "    y_test = []\n",
    "\n",
    "    for idx in idx_test:\n",
    "        experiment, label, image = datasets.__getitem__(idx)\n",
    "\n",
    "        # feature extraction\n",
    "        new_feature = encoder_image(image, input_size, autoencoder_model)\n",
    "\n",
    "        # Extracted feature for testing\n",
    "        X_test.append(new_feature)\n",
    "        y_test.append(label)\n",
    "\n",
    "    # Validation variable with experiment B3\n",
    "    X_val = []\n",
    "    y_val = []\n",
    "\n",
    "    for idx in idx_track['B3']:\n",
    "        experiment, label, image = datasets.__getitem__(idx)\n",
    "\n",
    "        # feature extraction\n",
    "        new_feature = encoder_image(image, input_size, autoencoder_model)\n",
    "\n",
    "        # Extracted feature for validation\n",
    "        X_val.append(new_feature)\n",
    "        y_val.append(label)\n",
    "\n",
    "    # run baseline selector\n",
    "    results = model_selection(models, X_train, y_train, X_test, y_test)\n",
    "\n",
    "    # Sort the results by F1-score in descending order\n",
    "    results = sorted(results, key=lambda x: x[2], reverse=True)\n",
    "\n",
    "    # Print the results\n",
    "    print(\"Model Performance:\")\n",
    "    print(\"+-----------------------+----------+----------+----------+----------+\")\n",
    "    print(\"| Model                 | Accuracy | F1-score | Precision | Recall   |\")\n",
    "    print(\"+-----------------------+----------+----------+----------+----------+\")\n",
    "    for result in results:\n",
    "        print(\"| {:<20} | {:.4f}   | {:.4f}   | {:.4f}   | {:.4f}   |\".format(*result))\n",
    "    print(\"+-----------------------+----------+----------+----------+----------+\")\n",
    "\n",
    "    # Choose the best model based on the results\n",
    "    best_model = models[results[0][0]]\n",
    "    print(f\"The best model is {results[0][0]}\")\n",
    "\n",
    "    # Evaluation\n",
    "    model = ClassificationModel(\n",
    "        best_model,\n",
    "        result[0][0]\n",
    "    )\n",
    "    model.run_full_analysis(\n",
    "        X_train=X_train+X_test,\n",
    "        X_test=X_val,\n",
    "        y_train=y_train+y_test,\n",
    "        y_test=y_val\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create feature names\n",
    "feature_names = [f'feature_{i}' for i in range(X_val[0].shape[0])]\n",
    "\n",
    "df_ae = pd.concat([\n",
    "    pd.DataFrame(X_val, columns=feature_names), \n",
    "    pd.DataFrame(y_val, columns=['label'])],\n",
    "    axis=1)\n",
    "df_ae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def pca_compress(data, explained_variance_threshold=0.95):\n",
    "    # Standardize the data\n",
    "    scaler = StandardScaler()\n",
    "    data_scaled = scaler.fit_transform(data)\n",
    "    \n",
    "    # Perform PCA with full number of components\n",
    "    pca = PCA()\n",
    "    pca.fit(data_scaled)\n",
    "    \n",
    "    # Find the optimal number of components\n",
    "    cumulative_variance_ratio = np.cumsum(pca.explained_variance_ratio_)\n",
    "    n_components = np.argmax(cumulative_variance_ratio >= explained_variance_threshold) + 1\n",
    "    \n",
    "    # Perform PCA with the optimal number of components\n",
    "    pca = PCA(n_components=n_components)\n",
    "    compressed_data = pca.fit_transform(data_scaled)\n",
    "    \n",
    "    # Calculate compression ratio\n",
    "    original_size = data.shape[1]\n",
    "    compressed_size = compressed_data.shape[1]\n",
    "    compression_ratio = 1 - (compressed_size / original_size)\n",
    "    \n",
    "    # Calculate explained variance ratio\n",
    "    explained_variance_ratio = np.sum(pca.explained_variance_ratio_)\n",
    "    \n",
    "    return compressed_data, compression_ratio, explained_variance_ratio, n_components\n",
    "\n",
    "# Compress the data\n",
    "compressed_data, compression_ratio, explained_variance_ratio, n_components = pca_compress(df_ae.iloc[:, :-1])\n",
    "\n",
    "print(f\"Original data shape: {data.shape}\")\n",
    "print(f\"Compressed data shape: {compressed_data.shape}\")\n",
    "print(f\"Optimal number of components: {n_components}\")\n",
    "print(f\"Compression ratio: {compression_ratio:.2f}\")\n",
    "print(f\"Explained variance ratio: {explained_variance_ratio:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfc = pd.concat([\n",
    "    pd.DataFrame(compressed_data, columns=['feature_1', 'feature_2']),\n",
    "    pd.DataFrame(y_val, columns=['label'])\n",
    "], axis=1)\n",
    "\n",
    "sns.scatterplot(dfc, x='feature_1', y='feature_2', hue='label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
