{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import sys\n",
    "import uuid\n",
    "import joblib\n",
    "import random\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import concurrent.futures\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import albumentations as A\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import classification_report, ConfusionMatrixDisplay\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\n",
    "\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attaching project directory\n",
    "sys.path.append(os.path.dirname(os.getcwd()))\n",
    "\n",
    "# Pathing imports\n",
    "from src import GetPath, FullMHI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize images\n",
    "NORMALIZE = False\n",
    "\n",
    "# Fixed randomness\n",
    "RANDOM_STATE = 42\n",
    "random.seed(RANDOM_STATE)\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "# Model max iteration\n",
    "MAX_ITERM = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = GetPath().shared_data()\n",
    "\n",
    "MHI_DATA = os.path.join(DATA_PATH, 'preprocess', 'mhi')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassificationModel:\n",
    "    def __init__(self, model: BaseEstimator, model_name: str):\n",
    "        self.model = model\n",
    "        self.model_name = model_name\n",
    "        self.is_trained = False\n",
    "\n",
    "    def train(self, X_train, y_train):\n",
    "        self.model.fit(X_train, y_train)\n",
    "        train_accuracy = self.model.score(X_train, y_train)\n",
    "        print(f\"{self.model_name} Training accuracy: {train_accuracy:.4f}\")\n",
    "\n",
    "    def evaluate(self, X_test, y_test):\n",
    "        test_accuracy = self.model.score(X_test, y_test)\n",
    "        y_pred = self.model.predict(X_test)\n",
    "\n",
    "        print(f\"{self.model_name} Test accuracy: {test_accuracy:.4f}\")\n",
    "        print(\"\\n-----CLASSIFICATION REPORT-----\\n\")\n",
    "        print(classification_report(y_test, y_pred))\n",
    "\n",
    "        return y_test, y_pred\n",
    "\n",
    "    def plot_confusion_matrix(self, y_true, y_pred):\n",
    "        disp = ConfusionMatrixDisplay.from_predictions(y_true, y_pred, xticks_rotation='vertical')\n",
    "        disp.figure_.suptitle(f\"{self.model_name} Confusion Matrix\")\n",
    "        plt.show()\n",
    "\n",
    "    def run_full_analysis(self, X_train, y_train, X_test, y_test):\n",
    "        self.train(X_train, y_train)\n",
    "        y_true, y_pred = self.evaluate(X_test, y_test)\n",
    "        self.plot_confusion_matrix(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the performance of each base model\n",
    "def train_and_evaluate(name, model, images_train, labels_train, images_test, labels_test):\n",
    "    le = LabelEncoder()\n",
    "    labels_train = le.fit_transform((labels_train))\n",
    "    model.fit(images_train, labels_train)\n",
    "    label_predict = model.predict(images_test)\n",
    "    label_predict = le.inverse_transform(label_predict)\n",
    "    accuracy = accuracy_score(labels_test, label_predict)\n",
    "    f1 = f1_score(labels_test, label_predict, average='weighted', zero_division=0.0)\n",
    "    precision = precision_score(labels_test, label_predict, average='weighted', zero_division=0.0)\n",
    "    recall = recall_score(labels_test, label_predict, average='weighted', zero_division=0.0)\n",
    "    return [name, accuracy, f1, precision, recall]\n",
    "\n",
    "def model_selection(models, X_train, y_train, X_test, y_test):\n",
    "    results = []\n",
    "    with tqdm(total=len(models), desc='Model Training', unit='model') as progress:\n",
    "        with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:\n",
    "            futures = []\n",
    "            for name, model in models.items():\n",
    "                future = executor.submit(\n",
    "                    train_and_evaluate, \n",
    "                    name, \n",
    "                    model, \n",
    "                    X_train,\n",
    "                    y_train,\n",
    "                    X_test,\n",
    "                    y_test\n",
    "                )\n",
    "                futures.append(future)\n",
    "\n",
    "            for future in concurrent.futures.as_completed(futures):\n",
    "                results.append(future.result())\n",
    "                future.add_done_callback(lambda p: progress.update())\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training with different time windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting different time windows\n",
    "roots_dir = []\n",
    "for file in os.listdir(MHI_DATA):\n",
    "    file = os.path.join(MHI_DATA, file)\n",
    "    if os.path.isdir(file):\n",
    "        roots_dir.append(os.path.join(MHI_DATA, file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the base models\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(random_state=RANDOM_STATE, max_iter=MAX_ITERM),\n",
    "    'SGD Classifier': SGDClassifier(random_state=RANDOM_STATE),\n",
    "    'Decision Tree': DecisionTreeClassifier(random_state=RANDOM_STATE),\n",
    "    'Random Forest': RandomForestClassifier(random_state=RANDOM_STATE),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(random_state=RANDOM_STATE),\n",
    "    'AdaBoost': AdaBoostClassifier(random_state=RANDOM_STATE, algorithm='SAMME'),\n",
    "    'K-Nearest Neighbors': KNeighborsClassifier(),\n",
    "    'Support Vector Machine': SVC(random_state=RANDOM_STATE),\n",
    "    'Gaussian Naive Bayes': GaussianNB(),\n",
    "    'Multi-layer Perceptron': MLPClassifier(random_state=RANDOM_STATE, max_iter=MAX_ITERM)\n",
    "    # 'XGBoost': xgb.XGBClassifier()\n",
    "}\n",
    "\n",
    "# Image transformation\n",
    "train_transform = A.Compose([\n",
    "        # Image resize\n",
    "        A.Resize(32, 32),\n",
    "        # Image normalize\n",
    "        A.Normalize(mean=(0,0,0), std=(1,1,1),),\n",
    "        # Random flip\n",
    "        A.OneOf([\n",
    "            A.HorizontalFlip(p=0.5),\n",
    "            A.VerticalFlip(p=0.5),\n",
    "        ], p=0.5),\n",
    "        # Random rotate\n",
    "        A.OneOf([\n",
    "            A.RandomRotate90(),\n",
    "            A.ShiftScaleRotate(shift_limit=0.0625, scale_limit=0.50, rotate_limit=45, p=.75),\n",
    "        ], p=0.5),\n",
    "    ])\n",
    "\n",
    "# Image transformation\n",
    "test_transform = A.Compose([\n",
    "        # Image resize\n",
    "        A.Resize(32, 32),\n",
    "        # Image normalize\n",
    "        A.Normalize(mean=(0,0,0), std=(1,1,1),),\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate through windows time (seconds)\n",
    "for file in roots_dir:\n",
    "    # Message\n",
    "    print(f\"Experimenting data on folder {file}\")\n",
    "    # Loading datasets with augmentation\n",
    "    datasetsWithTransform = FullMHI(root_dir=file, transform=train_transform)\n",
    "    datasets = FullMHI(root_dir=file, transform=test_transform)\n",
    "    # Message\n",
    "    print(f\"Total experiment datasets: {len(datasets)}\")\n",
    "\n",
    "    # Assigning index tracker for each batch experiment\n",
    "    idx_track = {\n",
    "        'B1': [],\n",
    "        'B2': [],\n",
    "        'B3': []\n",
    "    }\n",
    "\n",
    "    labels_track = {\n",
    "        'B1': [],\n",
    "        'B2': [],\n",
    "        'B3': []\n",
    "    }\n",
    "\n",
    "    # Reasign data\n",
    "    for idx in range(len(datasets)):\n",
    "        experiment, label, image = datasetsWithTransform.__getitem__(idx)\n",
    "        idx_track[experiment].append(idx)\n",
    "        labels_track[experiment].append(label)\n",
    "\n",
    "    # Train and test splitting between experiment B1 & B2, B1 does not have all abnormal level\n",
    "    idx_train, idx_test = train_test_split(\n",
    "        idx_track['B1'] + idx_track['B2'],\n",
    "        test_size=0.2,\n",
    "        random_state=RANDOM_STATE,\n",
    "        stratify=labels_track['B1'] + labels_track['B2']\n",
    "    )\n",
    "\n",
    "    # Training variable\n",
    "    X_train = []\n",
    "    y_train = []\n",
    "\n",
    "    # Generate n times sample with different augmentation\n",
    "    for i in range(3):\n",
    "        for idx in idx_train:\n",
    "            experiment, label, image = datasetsWithTransform.__getitem__(idx)\n",
    "            X_train.append(image.flatten())\n",
    "            y_train.append(label)\n",
    "\n",
    "    # Message\n",
    "    print(f\"Total augmentated data for training: {len(idx_train)}\")\n",
    "\n",
    "    # Test variable\n",
    "    X_test = []\n",
    "    y_test = []\n",
    "\n",
    "    for idx in idx_test:\n",
    "        experiment, label, image = datasets.__getitem__(idx)\n",
    "        X_test.append(image.flatten())\n",
    "        y_test.append(label)\n",
    "\n",
    "    # Validation variable with experiment B3\n",
    "    X_val = []\n",
    "    y_val = []\n",
    "\n",
    "    for idx in idx_track['B3']:\n",
    "        experiment, label, image = datasets.__getitem__(idx)\n",
    "        X_val.append(image.flatten())\n",
    "        y_val.append(label)\n",
    "\n",
    "    # run baseline selector\n",
    "    results = model_selection(models, X_train, y_train, X_test, y_test)\n",
    "\n",
    "    # Sort the results by F1-score in descending order\n",
    "    results = sorted(results, key=lambda x: x[2], reverse=True)\n",
    "\n",
    "    # Print the results\n",
    "    print(\"Model Performance:\")\n",
    "    print(\"+-----------------------+----------+----------+----------+----------+\")\n",
    "    print(\"| Model                 | Accuracy | F1-score | Precision | Recall   |\")\n",
    "    print(\"+-----------------------+----------+----------+----------+----------+\")\n",
    "    for result in results:\n",
    "        print(\"| {:<20} | {:.4f}   | {:.4f}   | {:.4f}   | {:.4f}   |\".format(*result))\n",
    "    print(\"+-----------------------+----------+----------+----------+----------+\")\n",
    "\n",
    "    # Choose the best model based on the results\n",
    "    best_model = models[results[0][0]]\n",
    "    print(f\"The best model is {results[0][0]}\")\n",
    "\n",
    "    # Evaluation\n",
    "    model = ClassificationModel(\n",
    "        best_model,\n",
    "        result[0][0]\n",
    "    )\n",
    "    model.run_full_analysis(\n",
    "        X_train=X_train+X_test,\n",
    "        X_test=X_val,\n",
    "        y_train=y_train+y_test,\n",
    "        y_test=y_val\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
